## let's set up our denbi pilot machine for 2024

ssh ubuntu@129.70.51.6 -p 30476

## let's add Dimitri:
## Dimitri ##

pwgen 1 10


sudo adduser dimitri

sudo usermod -aG sudo dimitri

## made an ssh folder and authorized key file...

cd /home/dimitri

mkdir .ssh

cd .ssh
touch authorized_keys ## then paste in key

## I think these need to belong to him...
chown dimitri:dimitri .ssh
chown dimitri:dimitri authorized_keys 

## check permissions

## not run, didn't need, but maybe for student keys:
chmod 700 /home/user_name/.ssh && chmod 600 /home/user_name/.ssh/authorized_keys

## this VM is behaving really funny. 
## constantly checking the ssh connection, and kicking 
## us out when it does this.

## so we try a new machine.

ssh -i ~/.ssh ubuntu@129.70.51.6 -p 30476

## meh, that old VM is basically dead, can't connect

## seems to work ok. 

## need to mount the volume...

## following: https://simplevm.denbi.de/wiki/simple_vm/volumes/

lsblk ## vde, I think

lsblk -o NAME,SIZE,MOUNTPOINT,FSTYPE,TYPE | egrep -v "^loop"

blkid /dev/vde ## not in there?

## guess it is not yet formatted? 

#sudo mkfs.ext4 /dev/vde

sudo mkdir -p /vol/funMicStorage

sudo mount /dev/vde /vol/funMicStorage

## add it to the fstab

## get the UUID
lsblk -o NAME,SIZE,MOUNTPOINT,FSTYPE,TYPE,UUID | egrep -v "^loop"

## add line to fstab 
UUID=a29e2283-ddb8-477b-ae03-14d276642b7f       /vol/funMicStorage      auto    defaults        0       2
## check after shutting down, seems stable

## how many cores do we have?

lscpu -p ## looks like 14, one core/cpu

### we need to get conda going ###

wget https://repo.anaconda.com/archive/Anaconda3-2023.09-0-Linux-x86_64.sh

## works, but conda not being found, probably because I put it on the 
## volume, so add to bashrc path def

export PATH=/vol/funMicStorage/anaconda3/bin:$PATH

## seems to work. 

## also, since we are always on the volume, can we change this to our home
## directory?

## meh, maybe better to add an alias to get students quickly?

alias vol="cd /vol/funMicStorage"

#### preliminary data #####

## let's grab the zymogen mock community reads, as per last year:

cd /vol/funMicStorage/datasets/

nohup wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR725/009/ERR7255689/ERR7255689_1.fastq.gz &
nohup wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR725/009/ERR7255689/ERR7255689_2.fastq.gz &

## the dataset that Dmitri wants to use, kelp on beaches:
https://www.ncbi.nlm.nih.gov/bioproject/?term=PRJEB36085

## made an accession list. put in on denbi

## script:
#####################
#!/bin/bash

cd /vol/funMicStorage/datasets/kelp
for i in $(cat SraAccList.csv); do
  echo $i
  fastq-dump $i -O .
done
#####################

nohup sh sraDownload.sh &


##################

#### software installs ####

## let's start on the installs

## conda and mamba are working together now. 
## to get mamba's solver into conda,
## following this page: https://www.anaconda.com/blog/a-faster-conda-for-a-growing-community

conda update -n base conda
conda install -n base conda-libmamba-solver
conda config --set solver libmamba

## get bioconda channel
conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge

## first environment: raw read processing 
## Dimitri would like to use:
conda create -n RawReadProcessing -c bioconda trim-galore bbmap seqtk phyloflash
## I added seqtk

conda activate RawReadProcessing

## trim_galore is having python issues:

## let's try getting rid of the autodetection of python version:

cd /vol/funMicStorage/anaconda3/envs/RawReadProcessing/bin/

cp cutadapt cutadapt.bk

vim cutadapt 

## change the sheebang to directly say "#!/usr/bin/env python3"

## does that break anything? yup

## let's try a separate install for trim_galore below

## phyloFlash needs a database:

cd /vol/funMicStorage/databases/phyloFlash

phyloFlash.pl -check_env

## as per: http://hrgv.github.io/phyloFlash/

wget https://zenodo.org/record/7892522/files/138.1.tar.gz # 5.5 GB download
tar -xzf 138.1.tar.gz

phyloFlash.pl -dbhome /vol/funMicStorage/databases/phyloFlash/138.1 -lib TEST -CPUs 14 \
 -read1 ${CONDA_PREFIX}/lib/phyloFlash/test_files/test_F.fq.gz \
 -read2 ${CONDA_PREFIX}/lib/phyloFlash/test_files/test_R.fq.gz \
 -almosteverything

## trim_galore

conda create -n trim-galore -c bioconda trim-galore


## next, assembler:
conda create -n assembly -c bioconda megahit

## let's add quast for assembly quality:
## quast seems to rely on old versions of 
## python, can't install it with our assembler,
## probably would cause problems with other
## newer software. Gets its own environment.

conda create -n quast -c bioconda quast

## check to make sure that didn't break our assembler

## binning and refining

conda create -n binning -c bioconda metabat2 vamb das_tool
## ran without errors...

## we also need the following, according to the vamb github repo:
conda activate binning
conda install -c pytorch pytorch torchvision cudatoolkit=10.2
## but we don't have GPUs, so not sure why we need cuda
## that did some weird downgrades...not a good sign...
## we also need minimap2, will this install in this env?
conda install -c bioconda minimap2
## and samtools...
conda install -c bioconda samtools
## and bowtie2, oh jeez...
conda install -c bioconda bowtie2
## remember to test these. Stuff in this environment is the most likely to fail.

## let's also install concoct. Probably won't use it, but 
## JIC:

## make sure channels ordered right
conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge
conda create -n concoct python=3 concoct

conda activate concoct

## genome quality checks:

## following https://github.com/chklovski/CheckM2

conda create -n magQC -c bioconda checkm2
## get some weird typeError from tensorflow
## so do lots of other folks: https://github.com/chklovski/CheckM2/issues/82

conda remove -n magQC --all 

## according to one answer, you have to make sure python isn't too old/new
conda create -n magQC -c bioconda -c conda-forge checkm2 'python>=3.7, <3.9'

conda activate magQC
checkm2 -h

nohup checkm2 database --download --path /vol/funMicStorage/databases/checkm2 &

checkm2 testrun ## looks good

 
#################################################################
## denbi VM is misbehaving, they need to know what I have installed:
conda env list
conda list > allCondaPackages_FunMicVM.txt
conda activate RawReadProcessing
conda list >> allCondaPackages_FunMicVM.txt
conda deactivate
conda activate assembly          
conda list >> allCondaPackages_FunMicVM.txt
conda deactivate
conda binning           
conda list >> allCondaPackages_FunMicVM.txt
conda deactivate

## get it local.
file=/home/ubuntu/allCondaPackages_FunMicVM.txt
scp -i /home/daniel/.ssh -P 30476 -r ubuntu@129.70.51.6:$file .
#################################################################

## for now, back to installations:

## classify genomes with gtdbtk
## following: https://ecogenomics.github.io/GTDBTk/installing/bioconda.html

conda create -n gtdbtk -c conda-forge -c bioconda gtdbtk=2.1.1

## try their script for getting the db:
conda activate gtdbtk

nohup download-db.sh ## was cut off, but looks right

## this install doesn't work, some issue with numpy
## see this issue: https://github.com/Ecogenomics/GTDBTk/issues/459

## try forcing down the numpy version?

conda install numpy=1.23.1 ## seems better

## annotations
Prodigal https://github.com/hyattpd/Prodigal
diamond https://github.com/bbuchfink/diamond
EggNOG mapper https://github.com/eggnogdb/eggnog-mapper
InterProScan https://github.com/ebi-pf-team/interproscan
Prokka https://github.com/tseemann/prokka

## try one big environment:

conda create -n annotation -c bioconda -c conda-forge prodigal diamond eggnog-mapper interproscan prokka

## databases and testing:

## eggnog:

conda activate annotation

export EGGNOG_DATA_DIR=/vol/funMicStorage/databases/eggnog-mapper-data/

## also EGGNOG_DATA_DIRneed to add that to bashrc?

echo $EGGNOG_DATA_DIR

## looks like. It's kind of complicated. Following:
https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#macos-linux-save-env-variables

conda activate annotation; echo $CONDA_PREFIX

cd $CONDA_PREFIX

mkdir -p ./etc/conda/activate.d
mkdir -p ./etc/conda/deactivate.d
touch ./etc/conda/activate.d/env_vars.sh
touch ./etc/conda/deactivate.d/env_vars.sh

vim ./etc/conda/activate.d/env_vars.sh ## to say:

#############
#!/bin/sh


export EGGNOG_DATA_DIR=/vol/funMicStorage/databases/eggnog-mapper-data/
###############

vim ./etc/conda/deactivate.d/env_vars.sh

##############
#!/bin/sh

unset EGGNOG_DATA_DIR
##############

## I guess that's what it takes to redefine a conda environment-specific
## environmental variable every time...

## anyway, having set the variable...download eggnog databases
download_eggnog_data.py 

## test InterProScan install
./interproscan.sh ## runs without errors. Enough for now.

## test Prokka install
Prokka https://github.com/tseemann/prokka

prokka --listdb
## seems to work

#### tree visualization ####

## for arb, following:
http://vc.arb-home.de/readonly/trunk/arb_INSTALL.txt

wget --recursive \
  --no-parent -nH --cut-dirs=2 \
  http://download.arb-home.de/release/latest/

## I guess we really just need the ubuntu installation script
## there is none for ubuntu 22...try the one for ubuntu 20?
## apt doesn't have arb7, btw, just arb6

## let's try: arb-7.0.ubuntu2004-amd64.tgz 

sudo bash arb_install.sh

sudo bash /usr/arb/SH/arb_installubuntu4arb.sh arb

## according to this, add the following to .bashrc,
## for path update with ARB binary
######
ARBHOME=/usr/arb;export ARBHOME
LD_LIBRARY_PATH=${ARBHOME}/lib:${LD_LIBRARY_PATH}
export LD_LIBRARY_PATH
PATH=${ARBHOME}/bin:${PATH}
export PATH
######

ARB http://www.arb-home.de/downloads.html

## enable x11 forwarding in 

sudo vim /etc/ssh/ssh_config

## all the other instances will have to modified, when they come into being...
## actually, if we take a snapshot of this VM, we should be able to 
## avoid all the manual 

## set the following:
ForwardX11 yes
ForwardX11Trusted yes

## restart:
sudo systemctl restart sshd

## do we need set client (local) permissions?
## nope. runs well, just make sure to use the -X or -Y flag
## on the ssh command

conda create -n fasttree -c bioconda fasttree

conda activate fasttree


#### metabarcoding data and software ####

## our local file:
sipData=/home/daniel/Documents/teaching/functionalMicrobiomes/barcode2023/16S_amps_SIP_2022.zip
## put it here:
barcodeDir=/vol/funMicStorage/datasets/metabarcoding/
scp -i /home/daniel/.ssh -P 30476 -r $sipData ubuntu@129.70.51.6:$barcodeDir

## also get the new milan data from alfons:
file=/home/daniel/Documents/teaching/functionalMicrobiomes/barcode2023/Oemik-Exp014.zip
barcodeDir=/vol/funMicStorage/datasets/metabarcoding/
scp -i /home/daniel/.ssh -P 30476 -r $file ubuntu@129.70.51.6:$barcodeDir

## and his metadata
file=/home/daniel/Documents/teaching/functionalMicrobiomes/barcode2023/mapping.tsv
barcodeDir=/vol/funMicStorage/datasets/metabarcoding/
scp -i /home/daniel/.ssh -P 30476 -r $file ubuntu@129.70.51.6:$barcodeDir

## we need just milan's reads:

cd /vol/funMicStorage/datasets/metabarcoding/20230210_060720/Fastq

milanReadDir=/vol/funMicStorage/datasets/metabarcoding/milanReads
mv Oemik-001254* $milanReadDir
mv Oemik-001255* $milanReadDir
mv Oemik-001256* $milanReadDir
mv Oemik-001257* $milanReadDir
mv Oemik-001258* $milanReadDir
mv Oemik-001259* $milanReadDir
mv Oemik-001260* $milanReadDir
mv Oemik-001261* $milanReadDir
mv Oemik-001262* $milanReadDir 

## that's our data. now install Qiime: https://docs.qiime2.org/2023.9/install/native/

wget https://data.qiime2.org/distro/amplicon/qiime2-amplicon-2023.9-py38-linux-conda.yml
conda env create -n qiime2-amplicon-2023.9 --file qiime2-amplicon-2023.9-py38-linux-conda.yml
conda rename -n qiime2-amplicon-2023.9 qiime
conda activate qiime
qiime --help ## works for the moment

## I think we'll still need to get classifiers, silva etc
## we'll come back to this. 

## R, vegan ##

## qiime has a pretty up-to-date R version
## but can we get phyloseq going?



R

install.packages("BiocManager")
BiocManager::install("phyloseq")
## huh, already on there



########################################

#### trials with mc data #######


## how do they look?
## let's get the quality, think about trimming, and run phyloflash

conda activate RawReadProcessing

## our mc data from zymogen/sereika is here:
cd /vol/funMicStorage/datasets/zymo

mkdir /vol/funMicStorage/rawReadQC

outDir=/vol/funMicStorage/rawReadQC
file="ERR7255689_1.fastq.gz"
fastqc -t 10 \
  -o /vol/funMicStorage/rawReadQC \
  $file &> $outDir/$file"fastqcLog.txt" &

## and R2:
cd /vol/funMicStorage/datasets/zymo
outDir=/vol/funMicStorage/rawReadQC
file="ERR7255689_2.fastq.gz"
fastqc -t 10 \
  -o /vol/funMicStorage/rawReadQC \
  $file &> $outDir/$file"fastqcLog.txt" &

## get these locally:

getFile=/vol/funMicStorage/rawReadQC/ERR7255689_?_fastqc.html
putDir=/home/daniel/Documents/teaching/functionalMicrobiomes/readQC/zymoQC
scp -i /home/daniel/.ssh -P 30476 -r ubuntu@129.70.51.6:$getFile $putDir

## can these be viewed with x11 forwarding?

cd /vol/funMicStorage/rawReadQC

firefox ERR7255689_1_fastqc.html ## doesn't work for me. 

## possible solutions (that didn't work for me):
https://superuser.com/questions/310197/how-do-i-fix-a-cannot-open-display-error-when-opening-an-x-program-after-sshi
https://www.thegeekstuff.com/2010/06/xhost-cannot-open-display/

## for the moment, ARB works. Just download the other 
## files for firefox, etc:

##### phyloflash #####

## following: https://hrgv.github.io/phyloFlash/usage.html

conda activate RawReadProcessing

phyloFlash.pl -h

cd /vol/funMicStorage/phyloFlashOut
fast1=/vol/funMicStorage/datasets/zymo/ERR7255689_1.fastq.gz
fast2=/vol/funMicStorage/datasets/zymo/ERR7255689_2.fastq.gz
phyloDB=/vol/funMicStorage/databases/phyloFlash/138.1
\time phyloFlash.pl \
  -lib zymoPhyloFlashOut \
  -read1 $fast1 \
  -read2 $fast2 \
  -readlength 150 \
  -clusterid 98 \
  -taxlevel 7 \
  -dbhome $phyloDB \
  -CPUs 12 &> phyloflashlog.txt &

## took ~20 min, 17 gig RAM

getFile=/vol/funMicStorage/phyloFlashOut/
putDir=/home/daniel/Documents/teaching/functionalMicrobiomes/
scp -i /home/daniel/.ssh -P 30476 -r ubuntu@129.70.51.6:$getFile $putDir

####### assembly #####

conda activate assembly

mkdir -p /vol/funMicStorage/assemblies/zymoMC/

fast1=/vol/funMicStorage/datasets/zymo/ERR7255689_1.fastq.gz
fast2=/vol/funMicStorage/datasets/zymo/ERR7255689_2.fastq.gz
outdir=/vol/funMicStorage/assemblies/zymoMC/
\time -v megahit -1 $fast1 \
          -2 $fast2 \
          -t 12 \
          -o $outdir &> megahit.log &

## this takes ~50 min to complete, max use of ~6 gig RAM

## okay, let's look at it with quast

conda activate quast

mkdir /vol/funMicStorage/assemblyQC

cd /vol/funMicStorage/assemblyQC
zymoMCassembly=/vol/funMicStorage/assemblies/zymoMC/final.contigs.fa
zymoMCquastOut=/vol/funMicStorage/assemblyQC
## run the command
\time quast -t 12 \
  -o $zymoMCquastOut \
  $zymoMCassembly &> quastLog.txt &

less quastLog.txt
## trivial amount of time and memory used.

getFile=/vol/funMicStorage/assemblyQC/
putDir=/home/daniel/Documents/teaching/functionalMicrobiomes/quastOut/
scp -i /home/daniel/.ssh -P 30476 -r ubuntu@129.70.51.6:$getFile $putDir

cd /home/daniel/Documents/teaching/functionalMicrobiomes/quastOut/
## looks fine, n50 of ~100,000, if we don't filter anything
## pacbio says with long reads, your n50 should be over 1Mb, but whatever.
## https://www.pacb.com/blog/beyond-contiguity/

## for specifics on the quast stats: https://quast.sourceforge.net/docs/manual.html#sec3.3

####### binning ######

## let's try metabat on a couple different settings
## and VAMB on default

## VAMB ##

conda activate binning

mkdir /vol/funMicStorage/binning

cd /vol/funMicStorage/binning

## VAMB requires a different format for the alignment of
## read than we used above for metabat

mkdir -p zymo/vamb

cd /vol/funMicStorage/binning/zymo/vamb

## vamb has a custom script to start it off:
## define our variables
illuminaAssembly="/vol/funMicStorage/assemblies/zymoMC/final.contigs.fa"
reads1="/vol/funMicStorage/datasets/zymo/ERR7255689_1.fastq.gz"
reads2="/vol/funMicStorage/datasets/zymo/ERR7255689_2.fastq.gz"
concatenate.py illumcatalogue.fna.gz $illuminaAssembly

## they use different aligner, minimap2, so back to our alignment environment
## super quick, 1 min, no RAM needed

## make an index with minimap
minimap2 -d illumcatalogue.mmi illumcatalogue.fna.gz
## also quick, no RAM needed

## map our reads to metagenome assembly with minimap and samtools
## This takes some time, maybe 20 minutes

rm *bam; rm *tmp; rm *log

illuminaAssembly="/vol/funMicStorage/assemblies/zymoMC/final.contigs.fa"
reads1="/vol/funMicStorage/datasets/zymo/ERR7255689_1.fastq.gz"
reads2="/vol/funMicStorage/datasets/zymo/ERR7255689_2.fastq.gz"
minimap2 -t 14 -N 5 \
  -ax sr illumcatalogue.mmi \
  --split-prefix mmsplit $reads1 $reads2 |
samtools view -F 3584 \
  -b --threads 14 > illumReadsAligned2Contigs.bam 

## that is quick

## line 445 in last years script

## minimap does the alignments, outputs sam, pipes to samtools
## we use samtools to make a binary, 
## and get rid of some of the low quality reads and weird alignments
## explanation of samtool flags: https://broadinstitute.github.io/picard/explain-flags.html

## once we have our reads aligned to our metagenome assembly, run vamb itself

\time nohup vamb --outdir vambOut \
  --fasta illumcatalogue.fna.gz \
  --bamfiles illumReadsAligned2Contigs.bam \
  -t 7 \
  -o C \
  --minfasta 200000 &> vambLog.txt &

## -t 8 is for very simple communities, this number can be much higher
## --minfasta is bp, I thik

## finds nine bins, all over 2 million bp

#### onto metabat ####

conda deactivate

conda activate binning

## make our output directory

mkdir -p /vol/funMicStorage/binning/zymo/metabat
cd /vol/funMicStorage/binning/zymo/metabat

## we need to the reads back to the genome
## metabat documentation suggests bowtie, don't
## know why

mkdir /vol/funMicStorage/binning/zymo/metabat/readAlignmentsForBinning

illuminaAssembly="/vol/funMicStorage/assemblies/zymoMC/final.contigs.fa"
reads1="/vol/funMicStorage/datasets/zymo/ERR7255689_1.fastq.gz"
reads2="/vol/funMicStorage/datasets/zymo/ERR7255689_2.fastq.gz"
outdir="/vol/funMicStorage/binning/zymo/metabat/readAlignmentsForBinning"

## for our alignments we use bowtie, which needs an index of our assembly
## we could probably use another mapper (bbmap? minimap?), if we have
## time to tweak the scripts.

\time bowtie2-build $illuminaAssembly $outdir/zymoIlluminaAssembly
## ~5 minutes?

## do the alignment. 
\time nohup bowtie2 \
  -x $outdir/zymoIlluminaAssembly \
  -1 $reads1 -2 $reads2 \
  -S rawReads2Contigs.sam \
  --threads 7 \
  --local &

## started ~11:50

## This takes a long time, an hour with 7 cores. Use all cores
## and maybe we can find a better solution


## sort it

samtools sort -l 1 \
    -@14 \
    -o rawReads2ContigsSorted.bam \
    -O BAM \
    rawReads2Contigs.sam &

## to test, can we use the minimap-created alignment
## from above? if we sort it, it should be basically 
## the same...
## tried, some headers are lost with minimap. Debug later if time.

## now try actual binning with metabat2

cd /vol/funMicStorage/binning/zymo/metabat

## define our variables
illuminaAssembly="/vol/funMicStorage/assemblies/zymoMC/final.contigs.fa"
bam="readAlignmentsForBinning/rawReads2ContigsSorted.bam"

\time nohup runMetaBat.sh $illuminaAssembly $bam

## two minutes, didn't seem to use much memory

#### metabat, different settings ####

## let's try some non-default settings, and use these in our 
## bin refinement process

conda activate binning

cd /vol/funMicStorage/binning/zymo/metabat

illuminaAssembly="/vol/funMicStorage/assemblies/zymoMC/final.contigs.fa"
bam="readAlignmentsForBinning/rawReads2ContigsSorted.bam"

runMetaBat.sh $illuminaAssembly $bam 

runMetaBat.sh \
  --maxP 98 \
  --maxEdges 500 \
  --minS 80 \
  $illuminaAssembly $bam 

## give them better names
mv final.contigs.fa.metabat-bins-20240216_120148 metabatContigs_default
mv final.contigs.fa.metabat-bins80-20240216_140752 metabatContigs_sensitive

ls metabatContigs_default | wc -l ## 14 bins
ls metabatContigs_sensitive | wc -l ## 13 bins

#### concoct ####

conda deactivate 


conda activate concoct

mkdir -p /vol/funMicStorage/binning/zymo/concoct

cd /vol/funMicStorage/binning/zymo/concoct

## define our variables
illuminaAssembly="/vol/funMicStorage/assemblies/zymoMC/final.contigs.fa"
readAlignments="/vol/funMicStorage/binning/zymo/metabat/readAlignmentsForBinning/rawReads2ContigsSorted.bam"
outdir=/vol/funMicStorage/binning/zymo/concoct
contigs=/vol/funMicStorage/binning/zymo/concoct

## run the software
## concoct has several steps, each with it's own script:

## samtools needs us to index the sorted read alignments:

samtools index -@ 14 $readAlignments

cut_up_fasta.py $illuminaAssembly -c 10000 -o 0 --merge_last -b concoctContigs_10K.bed > concoctContigs_10K.fa

concoct_coverage_table.py concoctContigs_10K.bed $readAlignments > coverage_table.tsv

concoct \
  --composition_file concoctContigs_10K.fa \
  --coverage_file coverage_table.tsv \
  -t 14 \
  -b $outdir


merge_cutup_clustering.py clustering_gt1000.csv > clustering_merged.csv

mkdir fasta_bins

extract_fasta_bins.py $illuminaAssembly clustering_merged.csv --output_path fasta_bins/

## let's rename these, I think the raw numbers are causing problems
cd fasta_bins
for i in *; do
mv $i ${i/\.fa/_concat\.fa}
done

######## refining bins ########

conda deactivate

conda activate binning

## okay, let's try combining these bins:

mkdir -p /vol/funMicStorage/refiningBins/zymo

cd /vol/funMicStorage/refiningBins/zymo

## define variables
metabatDefaultBins=/vol/funMicStorage/binning/zymo/metabat/metabatContigs_default
metabatSensitiveBins=/vol/funMicStorage/binning/zymo/metabat/metabatContigs_sensitive
vambBins=/vol/funMicStorage/binning/zymo/vamb/vambOut/bins
concoctBins=/vol/funMicStorage/binning/zymo/concoct/fasta_bins
illuminaAssembly=/vol/funMicStorage/assemblies/zymoMC/final.contigs.fa

## for each binner, we need a table to tell das_tools
#m which contig belogs to which bin

## metabat default
Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $metabatDefaultBins \
    > metabatDefault.contigs2bin.tsv

head metabatDefault.contigs2bin.tsv

## metabat sensitive
Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $metabatSensitiveBins \
    > metabatSensitive.contigs2bin.tsv

head metabatSensitive.contigs2bin.tsv

## vamb
Fasta_to_Contig2Bin.sh \
    -e fna \
    -i $vambBins \
    > vamb.contigs2bin.tsv

head vamb.contigs2bin.tsv ## also looks weird

## we need to cut the first three letters out of or so...
cut --complement -c 1-3 vamb.contigs2bin.tsv > vamb.contigs2bin_edited.tsv

head vamb.contigs2bin_edited.tsv ## looks better

### concoct 
Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $concoctBins \
    > concoct.contigs2bin.tsv

head concoct.contigs2bin.tsv

## I don't think that quite worked...
## let's keep just the first and last columns of that
paste <(cut -d " " -f 1 concoct.contigs2bin.tsv) <(cut -f 2 concoct.contigs2bin.tsv) > concoct.contigs2bin_edited.tsv

head concoct.contigs2bin_edited.tsv

## with these we can run DAS tool to refine bins:

DAS_Tool \
    -i metabatDefault.contigs2bin.tsv,metabatSensitive.contigs2bin.tsv,vamb.contigs2bin_edited.tsv,concoct.contigs2bin_edited.tsv \
    -l metabat,metabat,vamb,concoct \
    -c $illuminaAssembly \
    -t 14 \
    --write_bins \
    -o zymoMCBinsRefined

## let's secure these a bit. 

chmod 444 /vol/funMicStorage/refiningBins/zymo/zymoMCBinsRefined_DASTool_bins/*

## started 15:38

## huh, only 5 bins. Weird. so some serious tweaking needed to defaults. 

## okay, but for the moment, let's move on

#### check mag qualities ######

conda deactivate 

conda activate magQC

mkdir -p /vol/funMicStorage/magQC/zymo

cd /vol/funMicStorage/magQC/zymo


dasBins=/vol/funMicStorage/refiningBins/zymo/zymoMCBinsRefined_DASTool_bins
outdir=/vol/funMicStorage/magQC/zymo
checkm2 predict --threads 14 --input $dasBins -x "fa" --output-directory $outdir

## 5 min or so
## funny, didn't tell it where the diamond database was...
## not necessary?

## next time, maybe declare it before running checkm, might be
## faster?

##### gtdbtk #####

## let's try assigning taxonomy

## following: https://ecogenomics.github.io/GTDBTk/examples/classify_wf.html

conda activate gtdbtk

mkdir -p /vol/funMicStorage/assignTax/zymo

cd /vol/funMicStorage/assignTax/zymo

## temp dirs

rmdir ./identify

mags=/vol/funMicStorage/refiningBins/zymo/zymoMCBinsRefined_DASTool_bins

## do the gene predictions, and find the marker genes present in each mag
gtdbtk identify --genome_dir $mags --out_dir ./identify --extension "fa" --cpus 7

## run multiple alignments of these markers, figure out which domain of life
## we are dealing with.
gtdbtk align --identify_dir ./identify --out_dir ./align --cpus 7

\time \
gtdbtk classify --genome_dir $mags --align_dir ./align --out_dir ./classify -x "fa" --cpus 7

## that took ~half hour and 51 gig max RAM

## main results are here:

cd /vol/funMicStorage/assignTax/zymo/classify/classify/

less gtdbtk.bac120.summary.tsv
less gtdbtk.bac120.summary.tsv

## how to parse these results?
## explanation is here:
## https://ecogenomics.github.io/GTDBTk/files/summary.tsv.html

head -n2 gtdbtk.bac120.summary.tsv | cut -f 2

cut -f 16 gtdbtk.bac120.summary.tsv | head -n2

cut -f1,2 gtdbtk.bac120.summary.tsv | head -n 2

## all 5 identified:

## Staphylococcus aureus
## Enterococcus faecalis
## Bacillus spizizenii
## Pseudomonas aeruginosa
## Listeria monocytogenes


## correctly?

cd /vol/funMicStorage/assignTax/zymo/classify

cut -f2 gtdbtk.bac120.summary.tsv | cut -d"_" -f 15

## we have Bacillus spizizenii, instead of Bacillus subtilis
## ah yup, LPSN.dsmz says this is a synonym

## https://lpsn.dsmz.de/species/bacillus-spizizenii

## so we lost:
Escherichia coli
Salmonella enterica

## and of course the Fungi
Saccharomyces cerevisiae

######## metabarcoding #########

## data for barcoding is here:

milanReads=/vol/funMicStorage/datasets/metabarcoding/milanReads
classReads=/vol/funMicStorage/datasets/metabarcoding/old16S

## for qiime, we need a manifest table and a metadata table
## and all reads need to be found in a single directory

## we have the tables from last year, copy them here 
## and files into shape to match.

cd /vol/funMicStorage/metabarcoding

wget https://raw.githubusercontent.com/danchurch/FunctionalMicrobiomePractical/main/funmic2023/metabarcodeTables/sipManifest2023.tsv

wget https://raw.githubusercontent.com/danchurch/FunctionalMicrobiomePractical/main/funmic2023/metabarcodeTables/sipMeta2023.tsv

## I think last year I ran the students through looking at 
## qualities and trimming primers. 

## I think I remember pretty similar error profiles for
## both read sets. So let's combine them into one file 
## for the students to look at:

cd /vol/funMicStorage/datasets/metabarcoding

mkdir comboMetabarcodeData

cp milanReads/* comboMetabarcodeData/

cp old16S/* comboMetabarcodeData/

## 33 files, combined

## for students to have something to look at:

cd /vol/funMicStorage/datasets/metabarcoding/comboMetabarcodeData
#gunzip *
cat * > ../comboMetabarcodeData.fastq
#gzip *

## to check this out:

conda activate RawReadProcessing

cd /vol/funMicStorage/metabarcoding

mkdir rawFASTQCreports
allCombo16S="/vol/funMicStorage/datasets/metabarcoding/comboMetabarcodeData.fastq"
outdir="/vol/funMicStorage/metabarcoding/rawFASTQCreports"
fastqc -o $outdir -t 7 $allCombo16S

## get this local and look at them with your web browser

## local computer ##
getFile=/vol/funMicStorage/metabarcoding/rawFASTQCreports/comboMetabarcodeData_fastqc.html
putDir=/home/daniel/Documents/teaching/functionalMicrobiomes/readQC/metabarcodeData
scp -i /home/daniel/.ssh -P 30476 -r ubuntu@129.70.51.6:$getFile $putDir

## from this, and what we know about our primers, make a few decisions:

## make output directory

mkdir /vol/funMicStorage/metabarcoding/trimmedSIPreads


conda activate trim-galore 

cd /vol/funMicStorage/metabarcoding/
## define some variables
sipRawReads=/vol/funMicStorage/datasets/metabarcoding/comboMetabarcodeData
output=/vol/funMicStorage/metabarcoding/trimmedSIPreads
## run it
trim_galore \
  --cores 7 \
  -o $output \
  --clip_R1 20 \
  --illumina \
  --length 200 \
  $sipRawReads/*fastq.gz

## we can check these with fastqc if we want.

## but for now keep moving.

## we need to get these into qiime.

## get rid of the reports:
cd /vol/funMicStorage/metabarcoding/trimmedSIPreads

rm *txt

## and... change manifest file to 
## match new file path. Metadata should be the same
## as last year.

mkdir /vol/funMicStorage/metabarcoding/metabarcodeTables

cd /vol/funMicStorage/metabarcoding/metabarcodeTables

getFile=/vol/funMicStorage/metabarcoding/metabarcodeTables/
putDir=/home/daniel/Documents/teaching/functionalMicrobiomes/FunctionalMicrobiomePractical_repo/funmic2024
scp -i /home/daniel/.ssh -P 30476 -r ubuntu@129.70.51.6:$getFile $putDir

## can qiime read these in?

conda activate qiime

mkdir /vol/funMicStorage/metabarcoding/qiime

cd /vol/funMicStorage/metabarcoding/qiime

## check github links
wget "https://raw.githubusercontent.com/danchurch/FunctionalMicrobiomePractical/main/funmic2024/metabarcodeTables/sipManifest2024.tsv"

wget "https://raw.githubusercontent.com/danchurch/FunctionalMicrobiomePractical/main/funmic2024/metabarcodeTables/sipMeta2024.tsv"

manifest=/vol/funMicStorage/metabarcoding/sipManifest2024.tsv
metadata=/vol/funMicStorage/metabarcoding/sipMeta2024.tsv
outdir=/vol/funMicStorage/metabarcoding/qiime
qiime tools import \
  --type 'SampleData[SequencesWithQuality]' \
  --input-path $manifest \
  --output-path SIPreads_qiime \
  --input-format SingleEndFastqManifestPhred33V2

## now let's try getting ASV's out of this:

qiime dada2 denoise-single \
  --i-demultiplexed-seqs SIPreads_qiime.qza \
  --p-trunc-len 0 \
  --p-n-threads 7 \
  --o-table dada2_table.qza \
  --o-representative-sequences dada2_rep_set.qza \
  --o-denoising-stats dada2_stats.qza

## qiime can make some tables and graphics for us to use
## to understand what just happened:


qiime metadata tabulate \
  --m-input-file dada2_stats.qza \
  --o-visualization dada2_stats_bymetadata.qzv

qiime feature-table summarize \
  --i-table dada2_table.qza \
  --m-sample-metadata-file $metadata \
  --o-visualization dada2_featureTableSummary_visualization.qzv

qiime feature-table tabulate-seqs \
  --i-data dada2_rep_set.qza \
  --o-visualization dada2_rep_set.qzv



## put the visualizations here:
https://view.qiime2.org/
#
mkdir dada2visualizations
mv dada2_stats_bymetadata.qzv dada2_featureTableSummary_visualization.qzv dada2_rep_set.qzv dada2visualizations/

mkdir dada2OutPut
mv dada2_table.qza dada2_rep_set.qza dada2_stats.qza dada2OutPut/

## get them local and look at them!

getFile=/vol/funMicStorage/metabarcoding/qiime/dada2visualizations
putDir=/home/daniel/Documents/teaching/functionalMicrobiomes/qiimeOutputs
scp -i /home/daniel/.ssh -P 30476 -r ubuntu@129.70.51.6:$getFile $putDir

cd /home/daniel/Documents/teaching/functionalMicrobiomes/qiimeOutputs/dada2visualizations

## put the visualizations here:
https://view.qiime2.org/

### rarefaction and alpha/beta diversity ###

## back onto denbi

conda activate qiime

cd /vol/funMicStorage/metabarcoding/qiime

dada2_table=/vol/funMicStorage/metabarcoding/qiime/dada2OutPut/dada2_table.qza
metadata=/vol/funMicStorage/metabarcoding/qiime/sipMeta2024.tsv
qiime diversity alpha-rarefaction \
  --i-table $dada2_table \
  --o-visualization alpha_rarefaction_curves.qzv \
  --m-metadata-file $metadata \
  --p-max-depth 10000

## get it local and let's talk about it

## local computer ###
getFile=/vol/funMicStorage/metabarcoding/qiime/alpha_rarefaction_curves.qzv
putDir=/home/daniel/Documents/teaching/functionalMicrobiomes/qiimeOutputs/
scp -i /home/daniel/.ssh -P 30476 -r ubuntu@129.70.51.6:$getFile $putDir

###assigning phylogeny and taxonomy###

## a first step for any phylogenetic
## analysis would be to generate a tree



conda activate qiime

mkdir /vol/funMicStorage/metabarcoding/qiime/aligned2Tree/

cd /vol/funMicStorage/metabarcoding/qiime/aligned2Tree/

repSet=/vol/funMicStorage/metabarcoding/qiime/dada2OutPut/dada2_rep_set.qza
qiime phylogeny align-to-tree-mafft-fasttree \
  --i-sequences $repSet \
  --o-alignment aligned_dada2_rep_set.qza \
  --o-masked-alignment masked_aligned_dada2_rep_set.qza \
  --o-tree unrooted_tree.qza \
  --o-rooted-tree rooted_tree.qza \
  --p-n-threads 7 \
  --verbose

## quick, 2 min

## once we have this tree, it can be used to do diversity-based
## distance metrics like unifrac

cd /vol/funMicStorage/metabarcoding/qiime

repSet=/vol/funMicStorage/metabarcoding/qiime/dada2OutPut/dada2_rep_set.qza
dada2_table=/vol/funMicStorage/metabarcoding/qiime/dada2OutPut/dada2_table.qza
metadata=/vol/funMicStorage/metabarcoding/qiime/sipMeta2024.tsv
rootedTree=/vol/funMicStorage/metabarcoding/qiime/aligned2Tree/rooted_tree.qza

qiime diversity core-metrics-phylogenetic --help

qiime diversity core-metrics-phylogenetic \
  --i-table $dada2_table \
  --i-phylogeny $rootedTree \
  --m-metadata-file $metadata \
  --p-sampling-depth 6000 \
  --output-dir core-metrics-results

getFile=/vol/funMicStorage/metabarcoding/qiime/core-metrics-results
putDir=/home/daniel/Documents/teaching/functionalMicrobiomes/qiimeOutputs/
scp -i /home/daniel/.ssh -P 30476 -r ubuntu@129.70.51.6:$getFile $putDir

## okay

#### assign taxonomy ####

### last metabarcoding task: assign taxonomy ###

conda activate qiime

mkdir /vol/funMicStorage/metabarcoding/qiime/taxClassification

cd /vol/funMicStorage/metabarcoding/qiime/taxClassification


## we need to get the algorithm for assigning taxonomy
## as part of this, we have to choose our database of
## taxonomy. We will use the well-known Silva 16S database,
## and one of their standard classifiers:

wget https://data.qiime2.org/2024.2/common/silva-138-99-515-806-nb-classifier.qza

repSet=/vol/funMicStorage/metabarcoding/qiime/dada2OutPut/dada2_rep_set.qza
ourClassifier=/vol/funMicStorage/metabarcoding/qiime/taxClassification/silva-138-99-515-806-nb-classifier.qza
nohup qiime feature-classifier classify-sklearn \
  --i-classifier $ourClassifier \
  --i-reads $repSet \
  --p-n-jobs 7 \
  --o-classification dada2_rep_set_classified.qza &


qiime metadata tabulate \
  --m-input-file dada2_rep_set_classified.qza \
  --o-visualization taxonomy.qzv

## to make a nice visualization out of this...

## let's put it in our visualization folder:

cd /vol/funMicStorage/metabarcoding/qiime/dada2visualizations
repSetClassified=/vol/funMicStorage/metabarcoding/qiime/taxClassification/dada2_rep_set_classified.qza
dada2_table=/vol/funMicStorage/metabarcoding/qiime/dada2OutPut/dada2_table.qza
metadata=/vol/funMicStorage/metabarcoding/qiime/sipMeta2024.tsv
dada2Reps=/vol/funMicStorage/metabarcoding/qiime/dada2OutPut/dada2_rep_set.qza

qiime taxa barplot \
  --i-table $dada2_table \
  --i-taxonomy $repSetClassified \
  --m-metadata-file $metadata \
  --o-visualization all_taxa_bar_plots.qzv

## get local, look at it:

getFile=/vol/funMicStorage/metabarcoding/qiime/dada2visualizations/all_taxa_bar_plots.qzv
putDir=/home/daniel/Documents/teaching/functionalMicrobiomes/qiimeOutputs/dada2visualizations/
scp -i /home/daniel/.ssh -P 30476 -r ubuntu@129.70.51.6:$getFile $putDir

#### statistical tests #####

## let's do a permanova test of our 
## various predictors 

## we need a community matrix and an environmental 
## matrix out of qiime, and R with vegan. 

## place to put all this:

mkdir /vol/funMicStorage/metabarcoding/stats
cd /vol/funMicStorage/metabarcoding/stats

## tax table: 
cd /vol/funMicStorage/metabarcoding/qiime/taxClassification/
unzip dada2_rep_set_classified.qza
mv taxonomy.tsv /vol/funMicStorage/metabarcoding/stats/

## rep set sequences

cd /vol/funMicStorage/metabarcoding/qiime/dada2OutPut/
unzip dada2_rep_set.qza 
mv dna-sequences.fasta /vol/funMicStorage/metabarcoding/stats/

## get community matrix ##

## qiime data is ridiculously hard to get into
## phyloseq

## there should be a biom table in there, but not sure 
## if it is what we need:

cd /vol/funMicStorage/metabarcoding/qiime/dada2OutPut/
cp dada2_table.qza dada2_table.qza.bk
unzip dada2_table.qza
mv feature-table.biom /vol/funMicStorage/metabarcoding/stats/

## nope. We need to incorporate taxonomy into this 
## before phyloseq will use it:
## following 
## https://forum.qiime2.org/t/exporting-and-modifying-biom-tables-e-g-adding-taxonomy-annotations/3630

rm /vol/funMicStorage/metabarcoding/stats/feature-table.biom

dadaTable=/vol/funMicStorage/metabarcoding/qiime/dada2OutPut/dada2_table.qza
taxAss=/vol/funMicStorage/metabarcoding/qiime/taxClassification/taxonomy.qzv
statsDir=/vol/funMicStorage/metabarcoding/stats/

qiime tools export --input-path $dadaTable --output-path $statsDir
qiime tools export --input-path $taxAss --output-path $statsDir

## modify header of metadata.tsv to say:
#OTUID  taxonomy        confidence

## name is confusing
mv metadata.tsv biom-taxonomy.tsv

## try adding with the biom package:
biom add-metadata \
  -i feature-table.biom \
  -o table-with-taxonomy.biom \
  --observation-metadata-fp biom-taxonomy.tsv \
  --sc-separated taxonomy

## looks ok

## we have a 16s tree, use it?

## unzip and pull it out of the tree object
unzip rooted_tree.qza ## etc

mv tree.nwk /vol/funMicStorage/metabarcoding/stats/

cd /vol/funMicStorage/metabarcoding/stats

## qiime actually has a pretty good R setup already built:

conda activate qiime

R

library("phyloseq")
library("vegan")

setwd("/vol/funMicStorage/metabarcoding/stats/")

## for the first time:
ps = import_biom("table-with-taxonomy.biom", 
                          treefilename="tree.nwk", 
                          refseqfilename="dna-sequences.fasta",
                        )
     
## is our metadata in there already? of course not.

## looking at this, our raw metadata table may work:
aa <- read.csv("/vol/funMicStorage/metabarcoding/qiime/sipMeta2024.tsv", sep="\t", row.names=1)
sample_data(ps) <- aa

## clean up the names a bit:
taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))

save(ps, file="classSoilps.rda")

## next time:

load("classSoilps.rda")

## great, so we have a functioning ps object. 

## what do we want to do?

## I think we just have time for a simple PERMANOVA

## so we need get vegan our community matrix and env data:

## our community matrix:

soilCom <- t(ps@otu_table@.Data)
soilEnv <- as.data.frame(as.matrix(sample_data(ps)))

## write them out for next time:

write.csv(soilCom, file="soilCom.csv")
write.csv(soilEnv, file="soilEnv.csv")

## so should be able to start script from here:

conda activate qiime

R

setwd("/vol/funMicStorage/metabarcoding/stats")

library(vegan)

soilCom <- read.csv("soilCom.csv", row.names=1)
soilEnv <- read.csv("soilEnv.csv", row.names=1)

## look at these:

dim(soilCom) ## big!!

## our community matrix big, 
## so we pick some rows and columns to look at:
soilCom[1:10,1:5] 

## with our treatment data,
## which we will pretend is an environmental matrix,
## we can use "head" and "tail" to look at.

head(soilEnv)

tail(soilEnv)

## give these to the vegan adonis function:
adonis2(soilCom ~ ., data = soilEnv, method="bray")

## let's leave it there for now.

### prep computers for students ###

## let's simplify the environment for students

## first on cloud, take a snapshot of the existing
## VM, and make 7 copies.

## then check the volumes.

## if all is good, attach volumes, get conda working

## check fstab

## make keys pairs, save on USB stick, distribute tomorrow

## we need ~ 7 key pairs, matched with port numbers

## update slide with student login, reformat 


#### getting new VMs up to speed ####

## we have 7 new VMs that are supposedly 
## exact clones of our development one

## check logins:

## funmicbot1
ssh ubuntu@129.70.51.6 -p 30405

## starts up, with conda
## files are there
## fstab ok? apparently. Volume stays mounted after reboot



## ssh commands:

## funmicbot1
ssh -p 30405 ubuntu@129.70.51.6

## funmicbot2
ssh ubuntu@129.70.51.6 -p 30451

## funmicbot3
ssh ubuntu@129.70.51.6 -p 30409

## funmicbot4
ssh ubuntu@129.70.51.6 -p 30427

## funmicbot5
ssh ubuntu@129.70.51.6 -p 30322

## funmicbot6
ssh ubuntu@129.70.51.6 -p 30391

## funmicbot7
ssh ubuntu@129.70.51.6 -p 30289

## ports:
funmicbot1,30405
funmicbot2,30451
funmicbot3,30409
funmicbot4,30427
funmicbot5,30322
funmicbot6,30391
funmicbot7,30289

## great. now each needs a keyset...

cd /home/daniel/Documents/teaching/functionalMicrobiomes/setupVMs
for i in {0..7}; do
  echo $i
  ssh-keygen -f ./funmic${i} -q -N "" && echo ${i}" done"
done



## funmicbot1

scp -P 30405 ./funmic1.pub ubuntu@129.70.51.6:~/.ssh/
## to add on remote comp
ssh -p 30405 ubuntu@129.70.51.6
cd .ssh && cat funmic1.pub >> authorized_keys

## funmicbot2
scp -P 30451 ./funmic2.pub ubuntu@129.70.51.6:~/.ssh/
ssh -p 30451 ubuntu@129.70.51.6
cd .ssh && cat funmic2.pub >> authorized_keys

## funmicbot3
scp -P 30409 ./funmic3.pub ubuntu@129.70.51.6:~/.ssh/
ssh -p 30409 ubuntu@129.70.51.6
cd .ssh && cat funmic3.pub >> authorized_keys
logout

## funmicbot4
scp -P 30427 ./funmic4.pub ubuntu@129.70.51.6:~/.ssh/
ssh -p 30427 ubuntu@129.70.51.6
cd .ssh && cat funmic4.pub >> authorized_keys
logout

## funmicbot5
scp -P 30322 ./funmic5.pub ubuntu@129.70.51.6:~/.ssh/
ssh -p 30322 ubuntu@129.70.51.6
cd .ssh && cat funmic5.pub >> authorized_keys
logout

## funmicbot6
scp -P 30391 ./funmic6.pub ubuntu@129.70.51.6:~/.ssh/

ssh -p 30391 ubuntu@129.70.51.6

cd .ssh && cat funmic6.pub >> authorized_keys
logout

## funmicbot7
scp -P 30289 ./funmic7.pub ubuntu@129.70.51.6:~/.ssh/
ssh -p 30289 ubuntu@129.70.51.6
cd .ssh && cat funmic7.pub >> authorized_keys
logout

## tomorrow, test funmic7 on windows with mobaxterm and private key

## get the public key for the others onto their VMs

scp -P 30405 funmic1.pub ubuntu@129.70.51.6:~/.ssh 

## we also need an ssh key set for teaching computer onto 
## all the student computers. 

funmicbot1,30405
funmicbot2,30451
funmicbot3,30409
funmicbot4,30427
funmicbot5,30322
funmicbot6,30391
funmicbot7,30289

## we need to get the combo fastq file onto all the student computers:

## get it from denbi:




file=/vol/funMicStorage/datasets/metabarcoding/comboMetabarcodeData.fastq.gz
scp -i /home/daniel/.ssh -P 30476 -r ubuntu@129.70.51.6:$file .

## put it one their computers

file2put=/home/daniel/Documents/teaching/functionalMicrobiomes/comboMetabarcodeData.fastq.gz
path2key=/home/daniel/Documents/teaching/functionalMicrobiomes/setupVMs/
putFileHere=/vol/funMicStorage/datasets/metabarcoding/

#funmic1
scp -P 30405 -i $path2key $file2put ubuntu@129.70.51.6:/vol/funMicStorage/datasets/metabarcoding/ &

#funmic2
scp -P 30451 -i $path2key $file2put ubuntu@129.70.51.6:/vol/funMicStorage/datasets/metabarcoding/ &

## funmicbot3
scp -P 30409 -i $path2key $file2put ubuntu@129.70.51.6:/vol/funMicStorage/datasets/metabarcoding/ &

## funmicbot4
scp -P 30427 -i $path2key $file2put ubuntu@129.70.51.6:/vol/funMicStorage/datasets/metabarcoding/ &

## funmicbot6
scp -P 30391 -i $path2key $file2put ubuntu@129.70.51.6:/vol/funMicStorage/datasets/metabarcoding/ &

## funmicbot7
scp -P 30289 -i $path2key $file2put ubuntu@129.70.51.6:/vol/funMicStorage/datasets/metabarcoding/ &

## did that work?

ssh -p 30289 -i $path2key ubuntu@129.70.51.6

## looks good. Let's pray

#### notes for second week #####

## let's try looking for electron acceptors, etc, in a sample genome

## we'll look for ERR3801502_MetaBAT.19.fa, which has 

ls /vol/funMicStorage/Helgo_analysis_dimitri/Final_bins/ERR3801502_MetaBAT.19.fa

## qualities here:
less /vol/funMicStorage/Helgo_analysis_dimitri/Final_bins_CheckM/quality_report.tsv

less /vol/funMicStorage/Helgo_analysis_dimitri/Final_bins_GTDB/classify/gtdbtk.bac120.summary.tsv

cut -f1,2 /vol/funMicStorage/Helgo_analysis_dimitri/Final_bins_GTDB/classify/gtdbtk.bac120.summary.tsv
## Sulfurovum sp.

## now, how do we find the electron acceptors, donors, etc?

## sulari's computer

ssh -p 30289 ubuntu@129.70.51.6

## get her cov table and assembly:
#file2get=/vol/funMicStorage/Sulari/assembly/megahit_out/final.contigs.fa
file2get=/vol/funMicStorage/Sulari/binning/coverage/coverage_Depths.txt 
scp -P 30289 ubuntu@129.70.51.6:$file2get . 

## put these on Bex's computer

ssh -p 30405 ubuntu@129.70.51.6

#file2put=final.contigs.fa
file2put=coverage_Depths.txt
scp -P 30405 $file2put ubuntu@129.70.51.6:/vol/funMicStorage/filesFromSulari/ 



## bex's comp
ssh -p 30405 ubuntu@129.70.51.6 "ls -l /vol/funMicStorage/filesFromSulari/" ##211231276

## sularis's comp

ssh -p 30289 ubuntu@129.70.51.6 "ls -l /vol/funMicStorage/Sulari/assembly/megahit_out/final.contigs.fa" ##211231276

## my computer
ls -l
## 211231276


## okay, these are on there.

## second problem - concoct isn't running right for anyone. 

## step one recreate the problem. Can we find the files on Lin's 
## computer?

## lin's comp:

ssh -p 30451 ubuntu@129.70.51.6 


## coverage table is here:

/vol/funMicStorage/datasets/kelp/binning/coverage/coverage_Depths.txt

## looks fine


conda activate concoct

cd /vol/funMicStorage/datasets/kelp/binning/concoct

## log shows things at least started okay...

## looking at the command history, the command that breaks is:

assembly=/vol/funMicStorage/datasets/kelp/assembly/megahit_out/final.contigs.fa
concoct   --composition_file $assembly   --coverage_file concoct_coverage.txt   -t 8

## and there it is. Issue on github here: https://github.com/BinPro/CONCOCT/issues/232
## why didn't my previous or dimitri's script have this error?

## think we need a fresh install. Setup the teaching comp with the files and repeat

## we need her assembly, the relevant files are her assembly and her coverage file,
## and we might need her original coverage table

## get local from lin's computer

#file=/vol/funMicStorage/datasets/kelp/assembly/megahit_out/final.contigs.fa
#file=/vol/funMicStorage/datasets/kelp/binning/concoct/concoct_coverage.txt
file=/vol/funMicStorage/datasets/kelp/binning/coverage/coverage_Depths.txt
scp -P 30451 ubuntu@129.70.51.6:$file . 

## put onto teaching comp:

#file=final.contigs.fa
#file=concoct_coverage.txt
file=coverage_Depths.txt
scp -P 30476 $file ubuntu@129.70.51.6:/vol/funMicStorage/fixConcoct 

## try to repeat the error on our machine:

conda activate concoct

cd /vol/funMicStorage/fixConcoct

## reorganize a bit
mkdir filesFromLin/
mv concoct_coverage.txt filesFromLin/
mv coverage_Depths.txt filesFromLin/
mv final.contigs.fa filesFromLin/


## the command that breaks is:

cd /vol/funMicStorage/fixConcoct

concoctCov=/vol/funMicStorage/fixConcoct/filesFromLin/concoct_coverage.txt
assembly=/vol/funMicStorage/fixConcoct/filesFromLin/final.contigs.fa
concoct   --composition_file $assembly --coverage_file $concoctCov -t 8


## okay, problem repeats

## try with single core? this will take forever...

cd /vol/funMicStorage/fixConcoct
concoctCov=/vol/funMicStorage/fixConcoct/filesFromLin/concoct_coverage.txt
assembly=/vol/funMicStorage/fixConcoct/filesFromLin/final.contigs.fa
concoct   --composition_file $assembly --coverage_file $concoctCov 
## started 6:09

find -newermt '9 minutes ago' 

#find -newermt '9 minutes ago' -exec rm {} \;


## let's try a reinstall with older python?:

#conda deactivate 

conda config --add channels defaults
conda config --add channels conda-forge
conda config --add channels bioconda
conda create -n concoct_reinstall python=3.8 concoct

conda config --env --add channels defaults
conda config --env --add channels conda-forge
conda config --env --add channels bioconda

conda create -n concoct_reinstalled python=3.8


## this fails. wtf, can't even start an empty environment?

conda activate concoct_reinstalled 

conda config --env --add channels defaults
conda config --env --add channels conda-forge
conda config --env --add channels bioconda
conda install concoct

## looks like having dimitri on there screwed up our 
## conda permissions. Note to self - don't put 
## multiple users on a conda install...

## try again, on lin's computer:

## lin's comp:
ssh -p 30451 ubuntu@129.70.51.6 

conda activate concoct

cd /vol/funMicStorage/datasets/kelp/binning/concoct

assembly=/vol/funMicStorage/datasets/kelp/assembly/megahit_out/final.contigs.fa
concoct   --composition_file $assembly --coverage_file concoct_coverage.txt -t 8

## try reinstall:


conda deactivate 

conda create -n concoct_reinstall python=3.8

## nope, this is broken on all of their computers. 

## in the future, do not use different users on same conda

## okay, the only option I see is to still single-core 
## mode binning for all groups. Do one from each group, 
## then start more if time allows:


## group1:
## lin's comp:

ssh -p 30451 ubuntu@129.70.51.6 

conda activate concoct

cd /vol/funMicStorage/datasets/kelp/binning/concoct

assembly=/vol/funMicStorage/datasets/kelp/assembly/megahit_out/final.contigs.fa

concoct --composition_file $assembly --coverage_file concoct_coverage.txt &

## can we run the next step, also?
## find a place to put these bins:

mkdir fasta_bins

## another custom script from concoct to collect our contigs into
## fasta files:

extract_fasta_bins.py $assembly clustering_gt1000.csv --output_path fasta_bins/

## and rename:
cd fasta_bins

for i in *; do
#  mv $i ${i/\.fa/_concat\.fa}
  mv $i ${i/\.fa/_concoct\.fa}
done

## looks ok.

## group2:
## sulari's comp:

ssh -p 30289 ubuntu@129.70.51.6

cd /vol/funMicStorage/Sulari/binning/concoct

conda activate concoct

assembly=/vol/funMicStorage/Sulari/assembly/megahit_out/final.contigs.fa
#nohup concoct --composition_file $assembly --coverage_file concoct_coverage.txt &

mkdir fasta_bins

extract_fasta_bins.py $assembly clustering_gt1000.csv --output_path fasta_bins/

for i in *; do
#  mv $i ${i/\.fa/_concat\.fa}
  mv $i ${i/\.fa/_concoct\.fa}
done

## another custom script from concoct to collect our contigs into
## fasta files:

extract_fasta_bins.py $assembly clustering_gt1000.csv --output_path fasta_bins/

## group3

## abdel's computer
ssh -p 30427 ubuntu@129.70.51.6

## his coverage table is here:
coverageTable=/vol/funMicStorage/kelpAnalysis/binning/coverage/coverage_Depths.txt

## make this into a concoct table:

cd /vol/funMicStorage/kelpAnalysis/binning/concoct

cut -f 1,4,6,8 $coverageTable | sed '1d' | sed '1i contig_id\tsample_1\tsample_2\tsample_3' > concoct_coverage.txt


conda activate concoct

assembly=/vol/funMicStorage/kelpAnalysis/assembly/megahit_out/final.contigs.fa
#nohup concoct --composition_file $assembly --coverage_file concoct_coverage.txt &

mkdir fasta_bins

extract_fasta_bins.py $assembly clustering_gt1000.csv --output_path fasta_bins/

## let's see if that runs for them...

## and let's make sure we can 

## bex? let's talk about it with him.


## the next big knot will be getting every to 
## to pool their genomes after das bin step

## have they all run vamb?

## abdel 
ssh -p 30427 ubuntu@129.70.51.6

## he ran it, but the folder is empty.
assembly=/vol/funMicStorage/kelpAnalysis/assembly/megahit_out/final.contigs.fa
coverageTable=/vol/funMicStorage/kelpAnalysis/binning/coverage/coverage_Depths.txt

conda activate binning

cd /vol/funMicStorage/kelpAnalysis/binning/danVamb

nohup vamb --fasta $assembly --jgi $coverageTable --minfasta 200000 --outdir vambOut &

## did sulari's work?

ssh -p 30289 ubuntu@129.70.51.6

## looks good


## lin's comp:

ssh -p 30451 ubuntu@129.70.51.6 

cd /vol/funMicStorage/datasets/kelp/binning/concoct

## also looks good. so, they should run das tools

## when they finish, ask them to put these on the myfilesGo




#### jana needs lin's bins:

 
ssh -p 30451 ubuntu@129.70.51.6 

cd /vol/funMicStorage/datasets/kelp/binning/concoct

scp -rP 30451 ubuntu@129.70.51.6:/vol/funMicStorage/datasets/kelp/binning/concoct/fasta_bins .

## onto jana's computer:

ssh -p 30409 ubuntu@129.70.51.6

cd /vol/funMicStorage/datasets/kelp/binning/concoct/fasta_bins

## jana needs some files

scp -rP 30409 /home/daniel/Desktop/MAGs_JJ_LQ ubuntu@129.70.51.6:/vol/funMicStorage/

scp -P 30409 fasta_bins/* ubuntu@129.70.51.6:/vol/funMicStorage/datasets/kelp/binning/concoct/fasta_bins


## take everyone's mags, run derep, get tax and checkm information for each

## put in a new folder on myfiles

## make spreadsheet with: magname, source (team), taxonomy,  

## maybe best to do this on the PC.


## start a new script just  for this, to avoid git issues. 

## download from efiles:

cd /home/daniel/Documents/teaching/functionalMicrobiomes/

mv /home/daniel/Desktop/MAGs.zip /home/daniel/Documents/teaching/functionalMicrobiomes/


## put these onto my denbi
scp -P 30476 MAGs.zip ubuntu@129.70.51.6:/vol/funMicStorage/

## make some wd
mkdir /vol/funMicStorage/drep
mkdir /vol/funMicStorage/drep/all_bins
mkdir /vol/funMicStorage/drep/dereplicated_bins

## put these onto my denbi
scp -P 30476 MAGs.zip ubuntu@129.70.51.6:/vol/funMicStorage/

## on denbbi

unzip MAGs.zip

cd /vol/funMicStorage/MAGs/MAGs_JJ_LQ
for i in *; do
  mv $i "JJ_LQ_"${i} 
done

cd /vol/funMicStorage/MAGs/MAGs_SA_BY
for i in *; do
  mv $i "SA_BY_"${i} 
done

cd /vol/funMicStorage/MAGs/MAGs_Abdel
for i in *; do
  mv $i "Abdel_"${i} 
done

mkdir /vol/funMicStorage/drep
mkdir /vol/funMicStorage/drep/all_bins
mkdir /vol/funMicStorage/drep/dereplicated_bins

## flatten mag dir

cd /vol/funMicStorage

cp MAGs/*/* /vol/funMicStorage/drep/all_bins

conda activate drep

## need these?
#export OMP_NUM_THREADS=12
#export NUMEXPR_MAX_THREADS=12

cd /vol/funMicStorage/drep

ALL_BINS_DIRECTORY=/vol/funMicStorage/drep/all_bins
DEREP_BINS_DIRECTORY=/vol/funMicStorage/binning/drep/dereplicated_bins
nohup dRep dereplicate \
    ${DEREP_BINS_DIRECTORY} \
    -p 12 \
    -g ${ALL_BINS_DIRECTORY}/*.fa \
    -comp 40 \
    -l 500000 \
    -comW 1 \
    -sizeW 1 \
    -conW 5 \
    -strW 1 \
    -N50W 0.5 \
    --run_tertiary_clustering  &

## seems to be running ok....

## great. But we need bins. Let's run dasTool on Abdel's bins:

## abdel 
ssh -p 30427 ubuntu@129.70.51.6

cd /vol/funMicStorage/danWD/vambBinsBackup

## clean up names
for i in *; do
  #mv $i ${i/\.fna/_vamb\.fna}
done



conda activate binning

cd /vol/funMicStorage/danWD/das

## where are his bins?
metabatBins=/vol/funMicStorage/kelpAnalysis/binning/metabat
vambBins=/vol/funMicStorage/danWD/vambBinsBackup
concoctBins=/vol/funMicStorage/kelpAnalysis/binning/concoct/fasta_bins
assembly=/vol/funMicStorage/kelpAnalysis/assembly/megahit_out/final.contigs.fa

## find the contigs in our metabat bins
Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $metabatBins \
    > metabat.contigs2bin.tsv

head metabat.contigs2bin.tsv
tail metabat.contigs2bin.tsv

## and vamb
Fasta_to_Contig2Bin.sh \
    -e fna \
    -i $vambBins \
    > vamb.contigs2bin.tsv

head vamb.contigs2bin.tsv
tail vamb.contigs2bin.tsv

## and concoct:
Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $concoctBins \
    > concoct.contigs2bin.tsv

head concoct.contigs2bin.tsv
tail concoct.contigs2bin.tsv

## now run das
DAS_Tool \
    -i metabat.contigs2bin.tsv,vamb.contigs2bin.tsv,concoct.contigs2bin.tsv \
    -l metabat,vamb,concoct \
    --score_threshold 0.25 \
    -c $assembly \
    -t 12 \
    --write_bins \
    -o zymoMC_das

## get results locally
file2get=/vol/funMicStorage/binning/drep/dereplicated_bins/dereplicated_genomes/
scp -rP 30476 ubuntu@129.70.51.6:$file2get .

ls /vol/funMicStorage/binning/drep/dereplicated_bins/dereplicated_genomes/ | wc -l

## switch to lab comp, enter these names onto the spreadsheet

Abdel_0_concat.fa       Abdel_73_concat.fa      Abdel_metabat.9.fa       JJ_LQ_metabat.17_sub.fa  SA_BY_metabat.25.fa
Abdel_15_concat.fa      Abdel_89_concat_sub.fa  JJ_LQ_18_concoct.fa      JJ_LQ_metabat.24.fa      SA_BY_metabat.8_sub.fa
Abdel_22_concat_sub.fa  Abdel_8_concat.fa       JJ_LQ_24_concoct_sub.fa  SA_BY_10_concoct.fa
Abdel_39_concat_sub.fa  Abdel_metabat.23.fa     JJ_LQ_27_concoct.fa      SA_BY_5_concoct.fa
Abdel_54_concat.fa      Abdel_metabat.7.fa      JJ_LQ_64_concoct_sub.fa  SA_BY_metabat.22.fa

## put this on myfiles for the students

## did Jana run das_tools?

## her old bins are here:
ls /vol/funMicStorage/datasets/kelp/binning/concoct/fasta_bins

mkdir -p /vol/funMicStorage/.concoctoldbinsFromLin/fasta_bins 
mv /vol/funMicStorage/datasets/kelp/binning/concoct/fasta_bins /vol/funMicStorage/.concoctoldbinsFromLin/

cd /vol/funMicStorage/datasets/kelp/binning/concoct/

## does she have any newer ones?

ssh -p 30409 ubuntu@129.70.51.6

find . -name *concoct.fa

find -newermt '9 minutes ago' 

#find -newermt '9 minutes ago' -exec ls {} \;

#### jana's organism phylogeny

## on lins computer
30451

ssh -p 30451 ubuntu@129.70.51.6

file=/vol/funMicStorage/datasets/kelp/assignTaxonomy/gtdbtk_Out/classify/gtdbtk.bac120.summary.tsv
scp -P 30451 ubuntu@129.70.51.6:$file .
