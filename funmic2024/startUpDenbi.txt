## let's set up our denbi pilot machine for 2024

ssh ubuntu@129.70.51.6 -p 30487

## let's add Dimitri:
## Dimitri ##

pwgen 1 10


sudo adduser dimitri

sudo usermod -aG sudo dimitri

## made an ssh folder and authorized key file...

cd /home/dimitri

mkdir .ssh

cd .ssh
touch authorized_keys ## then paste in key

## I think these need to belong to him...
chown dimitri:dimitri .ssh
chown dimitri:dimitri authorized_keys 

## this VM is behaving really funny. 
## constantly checking the ssh connection, and kicking 
## us out when it does this.

## so we try a new machine.

ssh -i ~/.ssh ubuntu@129.70.51.6 -p 30476

## meh, that old VM is basically dead, can't connect

## seems to work ok. 

## need to mount the volume...

## following: https://simplevm.denbi.de/wiki/simple_vm/volumes/

lsblk ## vde, I think

lsblk -o NAME,SIZE,MOUNTPOINT,FSTYPE,TYPE | egrep -v "^loop"

blkid /dev/vde ## not in there?

## guess it is not yet formatted? 

#sudo mkfs.ext4 /dev/vde

sudo mkdir -p /vol/funMicStorage

sudo mount /dev/vde /vol/funMicStorage

## add it to the fstab

## get the UUID
lsblk -o NAME,SIZE,MOUNTPOINT,FSTYPE,TYPE,UUID | egrep -v "^loop"

## add line to fstab 
UUID=a29e2283-ddb8-477b-ae03-14d276642b7f       /vol/funMicStorage      auto    defaults        0       2
## check after shutting down, seems stable

## how many cores do we have?

lscpu -p ## looks like 14, one core/cpu

### we need to get conda going ###

wget https://repo.anaconda.com/archive/Anaconda3-2023.09-0-Linux-x86_64.sh

## works, but conda not being found, probably because I put it on the 
## volume, so add to bashrc path def

export PATH=/vol/funMicStorage/anaconda3/bin:$PATH

## seems to work. 

## also, since we are always on the volume, can we change this to our home
## directory?

## meh, maybe better to add an alias to get students quickly?

alias vol="cd /vol/funMicStorage"

#### preliminary data #####

## let's grab the zymogen mock community reads, as per last year:

cd /vol/funMicStorage/datasets/

nohup wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR725/009/ERR7255689/ERR7255689_1.fastq.gz &
nohup wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR725/009/ERR7255689/ERR7255689_2.fastq.gz &

## the dataset that Dmitri wants to use, kelp on beaches:
https://www.ncbi.nlm.nih.gov/bioproject/?term=PRJEB36085

## made an accession list. put in on denbi

## script:
#####################
#!/bin/bash

cd /vol/funMicStorage/datasets/kelp
for i in $(cat SraAccList.csv); do
  echo $i
  fastq-dump $i -O .
done
#####################

nohup sh sraDownload.sh &


##################

#### software installs ####

## let's start on the installs

## conda and mamba are working together now. 
## to get mamba's solver into conda,
## following this page: https://www.anaconda.com/blog/a-faster-conda-for-a-growing-community

conda update -n base conda
conda install -n base conda-libmamba-solver
conda config --set solver libmamba

## get bioconda channel
conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge

## first environment: raw read processing 
## Dimitri would like to use:
conda create -n RawReadProcessing -c bioconda trim-galore bbmap seqtk phyloflash
## I added seqtk

## phyloFlash needs a database:

conda activate RawReadProcessing

cd /vol/funMicStorage/databases/phyloFlash

phyloFlash.pl -check_env

## as per: http://hrgv.github.io/phyloFlash/

wget https://zenodo.org/record/7892522/files/138.1.tar.gz # 5.5 GB download
tar -xzf 138.1.tar.gz

phyloFlash.pl -dbhome /vol/funMicStorage/databases/phyloFlash/138.1 -lib TEST -CPUs 14 \
 -read1 ${CONDA_PREFIX}/lib/phyloFlash/test_files/test_F.fq.gz \
 -read2 ${CONDA_PREFIX}/lib/phyloFlash/test_files/test_R.fq.gz \
 -almosteverything

## next, assembler:
conda create -n assembly -c bioconda megahit

## let's add quast for assembly quality:
## quast seems to rely on old versions of 
## python, can't install it with our assembler,
## probably would cause problems with other
## newer software. Gets its own environment.

conda create -n quast -c bioconda quast

## check to make sure that didn't break our assembler

## binning and refining

conda create -n binning -c bioconda metabat2 vamb das_tool
## ran without errors...

## we also need the following, according to the vamb github repo:
conda activate binning
conda install -c pytorch pytorch torchvision cudatoolkit=10.2
## but we don't have GPUs, so not sure why we need cuda
## that did some weird downgrades...not a good sign...
## we also need minimap2, will this install in this env?
conda install -c bioconda minimap2
## and samtools...
conda install -c bioconda samtools
## and bowtie2, oh jeez...
conda install -c bioconda bowtie2
## remember to test these. Stuff in this environment is the most likely to fail.

## let's also install concoct. Probably won't use it, but 
## JIC:

## make sure channels ordered right
conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge
conda create -n concoct python=3 concoct

conda activate concoct

## genome quality checks:

## following https://github.com/chklovski/CheckM2

conda create -n magQC -c bioconda checkm2
## get some weird typeError from tensorflow
## so do lots of other folks: https://github.com/chklovski/CheckM2/issues/82

conda remove -n magQC --all 

## according to one answer, you have to make sure python isn't too old/new
conda create -n magQC -c bioconda -c conda-forge checkm2 'python>=3.7, <3.9'

conda activate magQC
checkm2 -h

nohup checkm2 database --download --path /vol/funMicStorage/databases/checkm2 &

checkm2 testrun ## looks good

 
#################################################################
## denbi VM is misbehaving, they need to know what I have installed:
conda env list
conda list > allCondaPackages_FunMicVM.txt
conda activate RawReadProcessing
conda list >> allCondaPackages_FunMicVM.txt
conda deactivate
conda activate assembly          
conda list >> allCondaPackages_FunMicVM.txt
conda deactivate
conda binning           
conda list >> allCondaPackages_FunMicVM.txt
conda deactivate

## get it local.
file=/home/ubuntu/allCondaPackages_FunMicVM.txt
scp -i /home/daniel/.ssh -P 30487 -r ubuntu@129.70.51.6:$file .
#################################################################

## for now, back to installations:

## classify genomes with gtdbtk
## following: https://ecogenomics.github.io/GTDBTk/installing/bioconda.html

conda create -n gtdbtk -c conda-forge -c bioconda gtdbtk=2.1.1

## try their script for getting the db:
conda activate gtdbtk

nohup download-db.sh ## was cut off, but looks right

## this install doesn't work, some issue with numpy
## see this issue: https://github.com/Ecogenomics/GTDBTk/issues/459

## try forcing down the numpy version?

conda install numpy=1.23.1 ## seems better

## annotations
Prodigal https://github.com/hyattpd/Prodigal
diamond https://github.com/bbuchfink/diamond
EggNOG mapper https://github.com/eggnogdb/eggnog-mapper
InterProScan https://github.com/ebi-pf-team/interproscan
Prokka https://github.com/tseemann/prokka

## try one big environment:

conda create -n annotation -c bioconda -c conda-forge prodigal diamond eggnog-mapper interproscan prokka

## databases and testing:

## eggnog:

conda activate annotation

export EGGNOG_DATA_DIR=/vol/funMicStorage/databases/eggnog-mapper-data/

## also EGGNOG_DATA_DIRneed to add that to bashrc?

echo $EGGNOG_DATA_DIR

## looks like. It's kind of complicated. Following:
https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#macos-linux-save-env-variables

conda activate annotation; echo $CONDA_PREFIX

cd $CONDA_PREFIX

mkdir -p ./etc/conda/activate.d
mkdir -p ./etc/conda/deactivate.d
touch ./etc/conda/activate.d/env_vars.sh
touch ./etc/conda/deactivate.d/env_vars.sh

vim ./etc/conda/activate.d/env_vars.sh ## to say:

#############
#!/bin/sh


export EGGNOG_DATA_DIR=/vol/funMicStorage/databases/eggnog-mapper-data/
###############

vim ./etc/conda/deactivate.d/env_vars.sh

##############
#!/bin/sh

unset EGGNOG_DATA_DIR
##############

## I guess that's what it takes to redefine a conda environment-specific
## environmental variable every time...

## anyway, having set the variable...download eggnog databases
download_eggnog_data.py 

## test InterProScan install
./interproscan.sh ## runs without errors. Enough for now.

## test Prokka install
Prokka https://github.com/tseemann/prokka

prokka --listdb
## seems to work

#### tree visualization ####

## for arb, following:
http://vc.arb-home.de/readonly/trunk/arb_INSTALL.txt

wget --recursive \
  --no-parent -nH --cut-dirs=2 \
  http://download.arb-home.de/release/latest/

## I guess we really just need the ubuntu installation script
## there is none for ubuntu 22...try the one for ubuntu 20?
## apt doesn't have arb7, btw, just arb6

## let's try: arb-7.0.ubuntu2004-amd64.tgz 

sudo bash arb_install.sh

sudo bash /usr/arb/SH/arb_installubuntu4arb.sh arb

## according to this, add the following to .bashrc,
## for path update with ARB binary
######
ARBHOME=/usr/arb;export ARBHOME
LD_LIBRARY_PATH=${ARBHOME}/lib:${LD_LIBRARY_PATH}
export LD_LIBRARY_PATH
PATH=${ARBHOME}/bin:${PATH}
export PATH
######

ARB http://www.arb-home.de/downloads.html

## enable x11 forwarding in 

sudo vim /etc/ssh/ssh_config

## all the other instances will have to modified, when they come into being...
## actually, if we take a snapshot of this VM, we should be able to 
## avoid all the manual 

## set the following:
ForwardX11 yes
ForwardX11Trusted yes

## restart:
sudo systemctl restart sshd

## do we need set client (local) permissions?
## nope. runs well, just make sure to use the -X or -Y flag
## on the ssh command

conda create -n fasttree -c bioconda fasttree

conda activate fasttree


#### metabarcoding data and software ####

## our local file:
sipData=/home/daniel/Documents/teaching/functionalMicrobiomes/barcode2023/16S_amps_SIP_2022.zip
## put it here:
barcodeDir=/vol/funMicStorage/datasets/metabarcoding/
scp -i /home/daniel/.ssh -P 30487 -r $sipData ubuntu@129.70.51.6:$barcodeDir

## also get the new milan data from alfons:
file=/home/daniel/Documents/teaching/functionalMicrobiomes/barcode2023/Oemik-Exp014.zip
barcodeDir=/vol/funMicStorage/datasets/metabarcoding/
scp -i /home/daniel/.ssh -P 30487 -r $file ubuntu@129.70.51.6:$barcodeDir

## and his metadata
file=/home/daniel/Documents/teaching/functionalMicrobiomes/barcode2023/mapping.tsv
barcodeDir=/vol/funMicStorage/datasets/metabarcoding/
scp -i /home/daniel/.ssh -P 30487 -r $file ubuntu@129.70.51.6:$barcodeDir

## we need just milan's reads:

cd /vol/funMicStorage/datasets/metabarcoding/20230210_060720/Fastq

milanReadDir=/vol/funMicStorage/datasets/metabarcoding/milanReads
mv Oemik-001254* $milanReadDir
mv Oemik-001255* $milanReadDir
mv Oemik-001256* $milanReadDir
mv Oemik-001257* $milanReadDir
mv Oemik-001258* $milanReadDir
mv Oemik-001259* $milanReadDir
mv Oemik-001260* $milanReadDir
mv Oemik-001261* $milanReadDir
mv Oemik-001262* $milanReadDir 

## that's our data. now install Qiime: https://docs.qiime2.org/2023.9/install/native/

wget https://data.qiime2.org/distro/amplicon/qiime2-amplicon-2023.9-py38-linux-conda.yml
conda env create -n qiime2-amplicon-2023.9 --file qiime2-amplicon-2023.9-py38-linux-conda.yml
conda rename -n qiime2-amplicon-2023.9 qiime
conda activate qiime
qiime --help ## works for the moment

## I think we'll still need to get classifiers, silva etc
## we'll come back to this. 

########################################

#### trials with mc data #######


## how do they look?
## let's get the quality, think about trimming, and run phyloflash

conda activate RawReadProcessing

## our mc data from zymogen/sereika is here:
cd /vol/funMicStorage/datasets/zymo

mkdir /vol/funMicStorage/rawReadQC

outDir=/vol/funMicStorage/rawReadQC
file="ERR7255689_1.fastq.gz"
fastqc -t 10 \
  -o /vol/funMicStorage/rawReadQC \
  $file &> $outDir/$file"fastqcLog.txt" &

## and R2:
cd /vol/funMicStorage/datasets/zymo
outDir=/vol/funMicStorage/rawReadQC
file="ERR7255689_2.fastq.gz"
fastqc -t 10 \
  -o /vol/funMicStorage/rawReadQC \
  $file &> $outDir/$file"fastqcLog.txt" &

## get these locally:

getFile=/vol/funMicStorage/rawReadQC/ERR7255689_?_fastqc.html
putDir=/home/daniel/Documents/teaching/functionalMicrobiomes/readQC/zymoQC
scp -i /home/daniel/.ssh -P 30487 -r ubuntu@129.70.51.6:$getFile $putDir

## can these be viewed with x11 forwarding?

cd /vol/funMicStorage/rawReadQC

firefox ERR7255689_1_fastqc.html ## doesn't work for me. 

## possible solutions (that didn't work for me):
https://superuser.com/questions/310197/how-do-i-fix-a-cannot-open-display-error-when-opening-an-x-program-after-sshi
https://www.thegeekstuff.com/2010/06/xhost-cannot-open-display/

## for the moment, ARB works. Just download the other 
## files for firefox, etc:

##### phyloflash #####

## following: https://hrgv.github.io/phyloFlash/usage.html

conda activate RawReadProcessing

phyloFlash.pl -h

cd /vol/funMicStorage/phyloFlashOut
fast1=/vol/funMicStorage/datasets/zymo/ERR7255689_1.fastq.gz
fast2=/vol/funMicStorage/datasets/zymo/ERR7255689_2.fastq.gz
phyloDB=/vol/funMicStorage/databases/phyloFlash/138.1
\time phyloFlash.pl \
  -lib zymoPhyloFlashOut \
  -read1 $fast1 \
  -read2 $fast2 \
  -readlength 150 \
  -clusterid 98 \
  -taxlevel 7 \
  -dbhome $phyloDB \
  -CPUs 12 &> phyloflashlog.txt &

## took ~20 min, 17 gig RAM

getFile=/vol/funMicStorage/phyloFlashOut/
putDir=/home/daniel/Documents/teaching/functionalMicrobiomes/
scp -i /home/daniel/.ssh -P 30487 -r ubuntu@129.70.51.6:$getFile $putDir

####### assembly #####

conda activate assembly

mkdir -p /vol/funMicStorage/assemblies/zymoMC/

fast1=/vol/funMicStorage/datasets/zymo/ERR7255689_1.fastq.gz
fast2=/vol/funMicStorage/datasets/zymo/ERR7255689_2.fastq.gz
outdir=/vol/funMicStorage/assemblies/zymoMC/
\time -v megahit -1 $fast1 \
          -2 $fast2 \
          -t 12 \
          -o $outdir &> megahit.log &

## this takes ~50 min to complete, max use of ~6 gig RAM

## okay, let's look at it with quast

conda activate quast

mkdir /vol/funMicStorage/assemblyQC

cd /vol/funMicStorage/assemblyQC
zymoMCassembly=/vol/funMicStorage/assemblies/zymoMC/final.contigs.fa
zymoMCquastOut=/vol/funMicStorage/assemblyQC
## run the command
\time quast -t 12 \
  -o $zymoMCquastOut \
  $zymoMCassembly &> quastLog.txt &

less quastLog.txt
## trivial amount of time and memory used.

getFile=/vol/funMicStorage/assemblyQC/
putDir=/home/daniel/Documents/teaching/functionalMicrobiomes/quastOut/
scp -i /home/daniel/.ssh -P 30487 -r ubuntu@129.70.51.6:$getFile $putDir

cd /home/daniel/Documents/teaching/functionalMicrobiomes/quastOut/
## looks fine, n50 of ~100,000, if we don't filter anything
## pacbio says with long reads, your n50 should be over 1Mb, but whatever.
## https://www.pacb.com/blog/beyond-contiguity/

## for specifics on the quast stats: https://quast.sourceforge.net/docs/manual.html#sec3.3

####### binning ######

## let's try metabat on a couple different settings
## and VAMB on default

## VAMB ##

conda activate binning

mkdir /vol/funMicStorage/binning

cd /vol/funMicStorage/binning

## VAMB requires a different format for the alignment of
## read than we used above for metabat

mkdir -p zymo/vamb

cd /vol/funMicStorage/binning/zymo/vamb

## vamb has a custom script to start it off:
## define our variables
illuminaAssembly="/vol/funMicStorage/assemblies/zymoMC/final.contigs.fa"
reads1="/vol/funMicStorage/datasets/zymo/ERR7255689_1.fastq.gz"
reads2="/vol/funMicStorage/datasets/zymo/ERR7255689_2.fastq.gz"
concatenate.py illumcatalogue.fna.gz $illuminaAssembly

## they use different aligner, minimap2, so back to our alignment environment
## super quick, 1 min, no RAM needed

## make an index with minimap
minimap2 -d illumcatalogue.mmi illumcatalogue.fna.gz
## also quick, no RAM needed

## map our reads to metagenome assembly with minimap and samtools
## This takes some time, maybe 20 minutes

rm *bam; rm *tmp; rm *log

illuminaAssembly="/vol/funMicStorage/assemblies/zymoMC/final.contigs.fa"
reads1="/vol/funMicStorage/datasets/zymo/ERR7255689_1.fastq.gz"
reads2="/vol/funMicStorage/datasets/zymo/ERR7255689_2.fastq.gz"
minimap2 -t 14 -N 5 \
  -ax sr illumcatalogue.mmi \
  --split-prefix mmsplit $reads1 $reads2 |
samtools view -F 3584 \
  -b --threads 14 > illumReadsAligned2Contigs.bam 

## that is quick

## line 445 in last years script

## minimap does the alignments, outputs sam, pipes to samtools
## we use samtools to make a binary, 
## and get rid of some of the low quality reads and weird alignments
## explanation of samtool flags: https://broadinstitute.github.io/picard/explain-flags.html

## once we have our reads aligned to our metagenome assembly, run vamb itself

\time nohup vamb --outdir vambOut \
  --fasta illumcatalogue.fna.gz \
  --bamfiles illumReadsAligned2Contigs.bam \
  -t 7 \
  -o C \
  --minfasta 200000 &> vambLog.txt &

## -t 8 is for very simple communities, this number can be much higher
## --minfasta is bp, I thik

## finds nine bins, all over 2 million bp

#### onto metabat ####

conda deactivate

conda activate binning

## make our output directory

mkdir -p /vol/funMicStorage/binning/zymo/metabat
cd /vol/funMicStorage/binning/zymo/metabat

## we need to the reads back to the genome
## metabat documentation suggests bowtie, don't
## know why

mkdir /vol/funMicStorage/binning/zymo/metabat/readAlignmentsForBinning

illuminaAssembly="/vol/funMicStorage/assemblies/zymoMC/final.contigs.fa"
reads1="/vol/funMicStorage/datasets/zymo/ERR7255689_1.fastq.gz"
reads2="/vol/funMicStorage/datasets/zymo/ERR7255689_2.fastq.gz"
outdir="/vol/funMicStorage/binning/zymo/metabat/readAlignmentsForBinning"

## for our alignments we use bowtie, which needs an index of our assembly
## we could probably use another mapper (bbmap? minimap?), if we have
## time to tweak the scripts.

\time bowtie2-build $illuminaAssembly $outdir/zymoIlluminaAssembly
## ~5 minutes?

## do the alignment. 
\time nohup bowtie2 \
  -x $outdir/zymoIlluminaAssembly \
  -1 $reads1 -2 $reads2 \
  -S rawReads2Contigs.sam \
  --threads 7 \
  --local &

## started ~11:50

## This takes a long time, an hour with 7 cores. Use all cores
## and maybe we can find a better solution

## not run ## 

## sort it

samtools sort -l 1 \
    -@14 \
    -o rawReads2ContigsSorted.bam \
    -O BAM \
    rawReads2Contigs.sam &

## to test, can we use the minimap-created alignment
## from above? if we sort it, it should be basically 
## the same...
## tried, some headers are lost with minimap. Debug later if time.

## now try actual binning with metabat2

cd /vol/funMicStorage/binning/zymo/metabat

## define our variables
illuminaAssembly="/vol/funMicStorage/assemblies/zymoMC/final.contigs.fa"
bam="readAlignmentsForBinning/rawReads2ContigsSorted.bam"

\time nohup runMetaBat.sh $illuminaAssembly $bam

## two minutes, didn't seem to use much memory

#### metabat, different settings ####

## let's try some non-default settings, and use these in our 
## bin refinement process

conda activate binning

cd /vol/funMicStorage/binning/zymo/metabat

illuminaAssembly="/vol/funMicStorage/assemblies/zymoMC/final.contigs.fa"
bam="readAlignmentsForBinning/rawReads2ContigsSorted.bam"

runMetaBat.sh $illuminaAssembly $bam 

runMetaBat.sh \
  --maxP 98 \
  --maxEdges 500 \
  --minS 80 \
  $illuminaAssembly $bam 

## give them better names
mv final.contigs.fa.metabat-bins-20240216_120148 metabatContigs_default
mv final.contigs.fa.metabat-bins80-20240216_140752 metabatContigs_sensitive

ls metabatContigs_default | wc -l ## 14 bins
ls metabatContigs_sensitive | wc -l ## 13 bins

#### concoct ####

conda deactivate 


conda activate concoct

mkdir -p /vol/funMicStorage/binning/zymo/concoct

cd /vol/funMicStorage/binning/zymo/concoct

## define our variables
illuminaAssembly="/vol/funMicStorage/assemblies/zymoMC/final.contigs.fa"
readAlignments="/vol/funMicStorage/binning/zymo/metabat/readAlignmentsForBinning/rawReads2ContigsSorted.bam"
outdir=/vol/funMicStorage/binning/zymo/concoct
contigs=/vol/funMicStorage/binning/zymo/concoct

## run the software
## concoct has several steps, each with it's own script:

## samtools needs us to index the sorted read alignments:

samtools index -@ 14 $readAlignments

cut_up_fasta.py $illuminaAssembly -c 10000 -o 0 --merge_last -b concoctContigs_10K.bed > concoctContigs_10K.fa

concoct_coverage_table.py concoctContigs_10K.bed $readAlignments > coverage_table.tsv

concoct \
  --composition_file concoctContigs_10K.fa \
  --coverage_file coverage_table.tsv \
  -t 14 \
  -b $outdir


merge_cutup_clustering.py clustering_gt1000.csv > clustering_merged.csv

mkdir fasta_bins

extract_fasta_bins.py $illuminaAssembly clustering_merged.csv --output_path fasta_bins/

## let's rename these, I think the raw numbers are causing problems
cd fasta_bins
for i in *; do
mv $i ${i/\.fa/_concat\.fa}
done

######## refining bins ########

conda deactivate

conda activate binning

## okay, let's try combining these bins:

mkdir -p /vol/funMicStorage/refiningBins/zymo

cd /vol/funMicStorage/refiningBins/zymo

## define variables
metabatDefaultBins=/vol/funMicStorage/binning/zymo/metabat/metabatContigs_default
metabatSensitiveBins=/vol/funMicStorage/binning/zymo/metabat/metabatContigs_sensitive
vambBins=/vol/funMicStorage/binning/zymo/vamb/vambOut/bins
concoctBins=/vol/funMicStorage/binning/zymo/concoct/fasta_bins
illuminaAssembly=/vol/funMicStorage/assemblies/zymoMC/final.contigs.fa

## for each binner, we need a table to tell das_tools
#m which contig belogs to which bin

## metabat default
Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $metabatDefaultBins \
    > metabatDefault.contigs2bin.tsv

head metabatDefault.contigs2bin.tsv

## metabat sensitive
Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $metabatSensitiveBins \
    > metabatSensitive.contigs2bin.tsv

head metabatSensitive.contigs2bin.tsv

## vamb
Fasta_to_Contig2Bin.sh \
    -e fna \
    -i $vambBins \
    > vamb.contigs2bin.tsv

head vamb.contigs2bin.tsv ## also looks weird

## we need to cut the first three letters out of or so...
cut --complement -c 1-3 vamb.contigs2bin.tsv > vamb.contigs2bin_edited.tsv

head vamb.contigs2bin_edited.tsv ## looks better

### concoct 
Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $concoctBins \
    > concoct.contigs2bin.tsv

head concoct.contigs2bin.tsv

## I don't think that quite worked...
## let's keep just the first and last columns of that
paste <(cut -d " " -f 1 concoct.contigs2bin.tsv) <(cut -f 2 concoct.contigs2bin.tsv) > concoct.contigs2bin_edited.tsv

head concoct.contigs2bin_edited.tsv

## with these we can run DAS tool to refine bins:

DAS_Tool \
    -i metabatDefault.contigs2bin.tsv,metabatSensitive.contigs2bin.tsv,vamb.contigs2bin_edited.tsv,concoct.contigs2bin_edited.tsv \
    -l metabat,metabat,vamb,concoct \
    -c $illuminaAssembly \
    -t 14 \
    --write_bins \
    -o zymoMCBinsRefined

## let's secure these a bit. 

chmod 444 /vol/funMicStorage/refiningBins/zymo/zymoMCBinsRefined_DASTool_bins/*

## started 15:38

## huh, only 5 bins. Weird. so some serious tweaking needed to defaults. 

## okay, but for the moment, let's move on

#### check mag qualities ######

conda deactivate 

conda activate magQC

mkdir -p /vol/funMicStorage/magQC/zymo

cd /vol/funMicStorage/magQC/zymo


dasBins=/vol/funMicStorage/refiningBins/zymo/zymoMCBinsRefined_DASTool_bins
outdir=/vol/funMicStorage/magQC/zymo
checkm2 predict --threads 14 --input $dasBins -x "fa" --output-directory $outdir

## 5 min or so
## funny, didn't tell it where the diamond database was...
## not necessary?

## next time, maybe declare it before running checkm, might be
## faster?

##### gtdbtk #####

## let's try assigning taxonomy

## following: https://ecogenomics.github.io/GTDBTk/examples/classify_wf.html

conda activate gtdbtk

mkdir -p /vol/funMicStorage/assignTax/zymo

cd /vol/funMicStorage/assignTax/zymo

## temp dirs

rmdir ./identify

mags=/vol/funMicStorage/refiningBins/zymo/zymoMCBinsRefined_DASTool_bins

## do the gene predictions, and find the marker genes present in each mag
gtdbtk identify --genome_dir $mags --out_dir ./identify --extension "fa" --cpus 7

## run multiple alignments of these markers, figure out which domain of life
## we are dealing with.
gtdbtk align --identify_dir ./identify --out_dir ./align --cpus 7

\time \
gtdbtk classify --genome_dir $mags --align_dir ./align --out_dir ./classify -x "fa" --cpus 7

## that took ~half hour and 51 gig max RAM

## main results are here:

cd /vol/funMicStorage/assignTax/zymo/classify/classify/

less gtdbtk.bac120.summary.tsv
less gtdbtk.bac120.summary.tsv

## how to parse these results?
## explanation is here:
## https://ecogenomics.github.io/GTDBTk/files/summary.tsv.html

head -n2 gtdbtk.bac120.summary.tsv | cut -f 2

cut -f 16 gtdbtk.bac120.summary.tsv | head -n2

cut -f1,2 gtdbtk.bac120.summary.tsv | head -n 2
## all 5 identified

## correctly?
cut -f2 gtdbtk.bac120.summary.tsv | cut -d"_" -f 15

## we have Bacillus spizizenii, instead of Bacillus subtilis
## ah yup, LPSN.dsmz says this is a synonym

## https://lpsn.dsmz.de/species/bacillus-spizizenii

## so we lost:
Escherichia coli
Salmonella enterica
Lactobacillus fermentum

## and of course the Fungi
Saccharomyces cerevisiae
Cryptococcus neoformans


