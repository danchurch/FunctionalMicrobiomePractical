## okay, starting the setup for the funmic practical, spring 2025

## synch up our desktop computer with github:

https://github.com/danchurch/FunctionalMicrobiomePractical.git

git remote add origin https://github.com/danchurch/FunctionalMicrobiomePractical.git
git branch -M main
git remote set-url origin git@github.com:danchurch/FunctionalMicrobiomePractical.git
git push -u origin main

## our setup instance is here:

ssh ubuntu@129.70.51.6 -p 30423

## oops. Wrong key. do we have the denbi key on this work computer?
## this is probably a key on my laptop....

## I had to add a public key from desktop, via the notebook.
## let's do the same for dimitri, using the key he gave me last year.

## for conda, I think Dimitri and I need to share a user on this computer. 
## will just need to add his key. 

## now, is the volume already mounted?

lsblk

## mounted for the moment, 

lsblk -o NAME,SIZE,MOUNTPOINT,FSTYPE,TYPE  ##vdc


## change to user, not root
sudo chown ubuntu:ubuntu /vol/funmic

## if we restart the machine, do we lose the volume?:

## yup. 


## need the uuid, either with:
blkid /dev/vdc

## or 
lsblk -o NAME,SIZE,MOUNTPOINT,FSTYPE,TYPE,UUID | egrep -v "^loop"

cp /etc/fstab ~/fstab.bk

## let's add this to the fstab:
UUID=861ebc07-e98a-4c2f-aacd-f9a96bfeac6f       /vol/funmic      auto    defaults        0       2

## works on reboot.
## we may need to change the UUIDs for the student computers when we start them up.

## need the old fashioned zip tools for alfons data?
sudo apt install unzip

## and now get conda going

####### install conda #############
## let's start with miniconda, and put it on the volume:

mkdir -p /vol/funmic/miniconda3

wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /vol/funmic/miniconda3/miniconda.sh
bash /vol/funmic/miniconda3/miniconda.sh -b -u -p /vol/funmic/miniconda3/miniconda3
rm /vol/funmic/miniconda3/miniconda.sh

source /vol/funmic/miniconda3/miniconda3/bin/activate

## add to path env
export PATH=/vol/funmic/miniconda3/miniconda3/bin:$PATH

## initiate with defaults.
conda init

##### metagenome datasets #####

## let's use the mock community data and the kelp dataset, as per last year.

## mock community dataset:

nohup wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR725/009/ERR7255689/ERR7255689_1.fastq.gz -O /vol/funmic/datasets/ERR7255689_1.fastq.gz &
nohup wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR725/009/ERR7255689/ERR7255689_2.fastq.gz -O /vol/funmic/datasets/ERR7255689_2.fastq.gz &

## and the kelp dataset. This was tricky last year.

## let's try the SRA toolkit:

## got the binaries here:
wget https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/current/sratoolkit.current-ubuntu64.tar.gz

tar -xzf sratoolkit.current-ubuntu64.tar.gz

cd /home/ubuntu/sratoolkit.3.1.1-ubuntu64/bin


## set the working directory and temp file location to somewhere on the volume, then 
## seems to work, add to path

export PATH=/home/ubuntu/sratoolkit.3.1.1-ubuntu64/bin:$PATH


## test: 

vdb-config -i

fastq-dump --stdout -X 2 SRR390728

https://www.ncbi.nlm.nih.gov/bioproject/?term=PRJEB36085

## I think we just need the kelp-associated samples, not the water or sediment samples?
## we need SRA run numbers for these, keep clicking till you fun the link for runs.

## I see only 3 non-amplicon files associates with the kelp biofilms.  
## these are:

ERR3801502
ERR3801542
ERR3801603

## saved in 

cd /vol/funmic/datasets/kelpBiofilm

## following this website for 
## https://bioinformaticsworkbook.org/dataAcquisition/fileTransfer/sra.html#gsc.tab=0 

## also these from ncbi:
## https://github.com/ncbi/sra-tools/wiki/08.-prefetch-and-fasterq-dump
## https://github.com/ncbi/sra-tools/wiki/HowTo:-fasterq-dump

/vol/funmic/datasets/kelpBiofilm

## can use old fashioned way:
fastq-dump --split-files --origfmt --gzip ERR3801502

## seems to work, but let's try the prefetch/fasterq combination:

prefetch ERR3801502 
#fasterq-dump --split-spot ERR3801502  ## but this is interleaved. 
fasterq-dump --split-files ERR3801502  ## this seems to works better.

## so a script for this would be:

####### getKelpReads.sh ############
names=(
"ERR3801502"
"ERR3801542"
"ERR3801603"
)

for i in ${names[@]}; do
  prefetch $i
  fasterq-dump --split-files $i
done
####################################

nohup bash getKelpReads.sh &

## looks like it worked. 

########### software installs ###########

## this section is "what worked". 
## there is a section below for testing and for what didn't work.

### solver ###

## first, is conda using the mamba solver?

conda update -n base conda
conda install -n base conda-libmamba-solver
conda config --set solver libmamba

## get bioconda and conda forge

conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge

## let's try to keep our environments in line with the boxes 
## in the overal schematic of the class.

## The first is "quality control of raw sequences". So, let's call it "qualityControlRawSequences"

## the installation of trim-galore should be all we need, this includes fastqc:

conda create -n qualityControlRawSequences -c bioconda trim-galore 
 
## install phyloflash

conda create -n communityComposition -c bioconda phyloflash

## we need a formatted silva database for phyloFlash
cd /vol/funmic/databases

## looks like they haven't updated from last year:
wget https://zenodo.org/record/7892522/files/138.1.tar.gz
mv 138.1/ phyloflashSilvaDB/


tar -xzf 138.1.tar.gz

### megahit ###

conda create -n assembly -c bioconda megahit

### quast ###

conda create -n assemblyQC -c bioconda quast

### minimap ### 

## the repo for minimap (https://github.com/lh3/minimap2?tab=readme-ov-file#install)
## says just download binaries. so:

mkdir /vol/funmic/.minimap2

cd /vol/funmic/.minimap2

curl -L https://github.com/lh3/minimap2/releases/download/v2.28/minimap2-2.28_x64-linux.tar.bz2 | tar -jxvf -

## link to somewhere we can find it
sudo ln -s /vol/funmic/.minimap2/minimap2 /usr/local/bin/minimap2

### bbmap ###

## jgi also recommends a direct download 

## try:

cd /vol/funmic

wget https://altushost-swe.dl.sourceforge.net/project/bbmap/BBMap_39.14.tar.gz

tar -xvzf BBMap_39.14.tar.gz

mv bbmap /vol/funmic/.bbmap/

## I guess we just add it to our path, add the following to our bashrc.
## not ideal, but...

export PATH="/vol/funmic/.bbmap/:$PATH"
## added to bashrc file, seems to work

## needs java
sudo apt install default-jre

##### binning #####

## what are the chances the binning software will all play nice with each other?

## vamb behaving weird. never works anyway, leave it out this year, try maxbin2 again

#conda create -n binning -c bioconda metabat2 maxbin2 concoct
conda create -n binning -c bioconda maxbin2 concoct

#conda remove -n binning --all ## keep needing this

## this was useful when maxbin doesn't completely install:
## conda update maxbin2

## no errors thrown, but can't believe that actually worked...
## this automatically installed samtools as a dependency, newest version.

## and doesn't work. There is a bug in metabat2 concerning 
## the abundance calculation program "jgi_summarize_bam_contig_depths"
## as per: https://bitbucket.org/berkeleylab/metabat/issues/172/jgi_summarize_bam_contig_depths-producing
## also several other bugs concerning negative abundances. 
## the author recommends against using bioconda/conda

## the author suggest docker or compiling from source. I should learn to use docker, but not now.
## https://bitbucket.org/berkeleylab/metabat/src/master/

## try from source. Dependencies:

## 1 boost
apt search libboost-all-dev
sudo apt install libboost-all-dev

## 2 cmake
sudo apt install cmake

## 3 g++
sudo apt install g++

cd /vol/funmic

## try the development version:
wget https://bitbucket.org/berkeleylab/metabat/get/master.tar.gz

tar xzvf master.tar.gz

cd berkeleylab-metabat-*

mkdir build && cd build && cmake .. [ -DCMAKE_INSTALL_PREFIX=/vol/funmic/ ] && make && make test && make install

## seems to have worked, hide it and put in on the path:

mv /vol/funmic/bin /vol/funmic/.metabat

## do we still need this?
rm -r berkeleylab-metabat-453915fb5bbc/

## added to path in bashrc

##### refining of bins #####

### das_tool ###

## as per: https://github.com/cmks/DAS_Tool

conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge
conda create -n refine -c bioconda das_tool


######### software tests ############

## here is all the "trash" scripting of the process of me testing
## different installs.

### trim-galore: ###

conda activate qualityControlRawSequences 

trim_galore  -help

## let's check it with our new data:

outDir=~/test
file="/vol/funmic/datasets/Barcode2024/raw_reads/M13-07.fastq.gz"
fastqc -t 10 \
  -o $outDir \
  $file 

getFile=/home/ubuntu/test
putDir=/home/daniel/Documents/teaching/funmic/scratchpad
scp -i /home/daniel/.ssh -P 30423 -r ubuntu@129.70.51.6:$getFile $putDir

## fastqc works

## does the cut-adapt side work?
sipRawReads="/vol/funmic/datasets/Barcode2024/raw_reads"
output=/home/ubuntu/test
## run it
trim_galore \
  --cores 7 \
  -o $output \
  --clip_R1 20 \
  --illumina \
  --length 200 \
  ${sipRawReads}/*fastq.gz

## looks good. 

### phyloflash: ###

conda activate communityComposition

phyloFlash.pl -check_env

phyloFlash.pl -dbhome /vol/funmic/databases/phyloflashSilvaDB -lib TEST -CPUs 14 \
 -read1 ${CONDA_PREFIX}/lib/phyloFlash/test_files/test_F.fq.gz \
 -read2 ${CONDA_PREFIX}/lib/phyloFlash/test_files/test_R.fq.gz \
 -almosteverything

## seems to work

### megahit ###

## test megahit on our metagenomes from the kelp:

conda activate assembly

cd /vol/funmic/test

megahit -h

#kelpDir="/vol/funmic/datasets/kelpBiofilm/"
#fast1=${kelpDir}ERR3801502_1.fastq
#fast2=${kelpDir}ERR3801502_2.fastq
#fast1=${kelpDir}ERR3801542_1.fastq
#fast2=${kelpDir}ERR3801542_2.fastq
#fast1=${kelpDir}ERR3801603_1.fastq
#fast2=${kelpDir}ERR3801603_2.fastq

## the first two kelp files result in the weird negative 
## values in the coverage table. Can't figure out why.
## try with the mock community:

fast1=/vol/funmic/datasets/zymogenMock/ERR7255689_1.fastq
fast2=/vol/funmic/datasets/zymogenMock/ERR7255689_2.fastq
rm -r $outdir
outdir=/vol/funmic/test
\time -v megahit -1 $fast1 \
          -2 $fast2 \
          -t 12 \
          -o $outdir

## here ##

## this took 3569928 kbytes of RAM, or 3.5 gig, and 2 hours human time
## something funny if I try stdout and stderr redirection, but
## resolve this later. for now install looks good. 


### quast ###

conda activate assemblyQC 

cd /vol/funmic/test

mkdir /vol/funmic/test/assemblyQC 

assembly=/vol/funmic/test/final.contigs.fa
quastOut=/vol/funmic/test/assemblyQC

## run the command
\time quast -t 12 \
  -o $quastOut \
  $assembly &> quastLog.txt &

less quastLog.txt
## trivial amount of time and memory used.
getFile=/vol/funmic/test/assemblyQC/
putDir=/home/daniel/Documents/teaching/funmic/scratchpad/
scp -i /home/daniel/.ssh -P 30423 -r ubuntu@129.70.51.6:$getFile $putDir

## quast works

#### binning ####



## mapping first, with bbmap:

mkdir -p /vol/funmic/test/binning/coverage

cd /vol/funmic/test/binning/coverage


###### Kelp data keeps giving me negative coverage values####
#kelpDir="/vol/funmic/datasets/kelpBiofilm/"
#fast1=${kelpDir}ERR3801502_1.fastq
#fast2=${kelpDir}ERR3801502_2.fastq
############################################################

## try with the mock community

fast1=/vol/funmic/datasets/zymogenMock/ERR7255689_1.fastq
fast2=/vol/funmic/datasets/zymogenMock/ERR7255689_2.fastq
assembly=/vol/funmic/test/final.contigs.fa
\time -v bbmap.sh \
   threads=14 \
   minid=.97 \
   idfilter=.95 \
   ref=$assembly \
   in=$fast1 \
   in2=$fast2 \
   outm=rawReads_to_assembly.sam \
   bamscript=rawReads_to_assembly_to_bam.sh

## with first kelp files that took 14 minutes, 35 gig of ram
## with mock comm files that took 13 min, 33 gig of ram 

## map reads back to contigs

conda activate binning

\time -v bash rawReads_to_assembly_to_bam.sh ## for mock community, ~20 gig ram, 3 min

## coverage table

cd /vol/funmic/test/binning/coverage

assembly=/vol/funmic/test/final.contigs.fa
sortedBAM=/vol/funmic/test/binning/coverage/rawReads_to_assembly_sorted.bam

jgi_summarize_bam_contig_depths --help

\time -v jgi_summarize_bam_contig_depths \
           --outputDepth coverage_Depths.txt \
           --referenceFasta  $assembly \
           $sortedBAM
## no RAM used, really, <1 min

### metabat ###

mkdir /vol/funmic/test/binning/metabat

cd /vol/funmic/test/binning/metabat

assembly=/vol/funmic/test/final.contigs.fa
coverageTable=/vol/funmic/test/binning/coverage/coverage_Depths.txt
\time -v  metabat2 \
            -i $assembly \
            -a $coverageTable \
            -o "metabat" \
            -t 12 

## some errors concerning negative values, check this out later
## errors gone now, with updated metabat2

### maxbin2 ###

## try it, if it works, use this instead of vamb:

mkdir -p /vol/funmic/test/binning/maxbin/maxbinOut

cd /vol/funmic/test/binning/maxbin

cut -f1,3 /vol/funmic/test/binning/coverage/coverage_Depths.txt > abundance.list

assembly=/vol/funmic/test/final.contigs.fa
\time -v run_MaxBin.pl -thread 12 -min_contig_length 1500 \
              -contig $assembly \
              -abund abundance.list \
              -out  /vol/funmic/test/binning/maxbin/maxbinOut/maxbin

## 1 min, 0.5 gig RAM

### concoct ###

conda activate binning

mkdir -p /vol/funmic/test/binning/concoct

cd /vol/funmic/test/binning/concoct

## define our variables
assembly=/vol/funmic/test/final.contigs.fa
sortedBAM=/vol/funmic/test/binning/coverage/rawReads_to_assembly_sorted.bam
outdir=/vol/funmic/test/binning/concoct

## run the software
## concoct has several steps, each with it's own script:

## samtools needs us to index the sorted read alignments:

#samtools index -@ 12 $readAlignments ## think this is already indexed

# cut_up_fasta.py $assembly -c 10000 -o 0 --merge_last -b concoctContigs_10K.bed > concoctContigs_10K.fa
# concoct_coverage_table.py concoctContigs_10K.bed $sortedBAM > coverage_table.tsv 

## still not working. Try an install of concoct with older python, like last year.

## can we skip this step by reformatting our old coverage table?:

cut -f 1,4 $coverageTable | sed '1d' | sed '1i contig_id\tsample_1' > concoct_coverage.txt

assembly=/vol/funmic/test/final.contigs.fa
\time -v concoct \
            --composition_file $assembly \
            --coverage_file concoct_coverage.txt \
            -t 12

## find a place to put these bins:

mkdir fasta_bins

extract_fasta_bins.py $assembly clustering_gt1000.csv --output_path fasta_bins/

## looks promising, but we want pretty file names
## time for more BASH magic:

cd fasta_bins
for i in *; do
  mv $i ${i/\.fa/_concoct\.fa}
done

#### refining ####

### test das_tool ###

conda activate refine

mkdir /vol/funmic/test/refine

cd /vol/funmic/test/refine

metabatBins=/vol/funmic/test/binning/metabat
concoctBins=/vol/funmic/test/binning/concoct/fasta_bins
maxbinBins=/vol/funmic/test/binning/maxbin/maxbinOut
assembly=/vol/funmic/test/final.contigs.fa

## assign contigs to bins

## metabat bins
Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $metabatBins \
    > metabat.contigs2bin.tsv

paste <(cut  -f 1 metabat.contigs2bin.tsv) <(cut -f 4 metabat.contigs2bin.tsv) > metabat.contigs2bin_edited.tsv

head metabat*

## concoct:
Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $concoctBins \
    > concoct.contigs2bin.tsv

paste <(cut -d " " -f 1 concoct.contigs2bin.tsv) <(cut -f 2 concoct.contigs2bin.tsv) > concoct.contigs2bin_edited.tsv

head concoct*

## maxbin:
Fasta_to_Contig2Bin.sh \
    -e fasta \
    -i $maxbinBins \
    > maxbin.contigs2bin.tsv

head maxbin.contigs2bin.tsv
## don't need to edit this table, looks good

DAS_Tool \
    -i metabat.contigs2bin_edited.tsv,concoct.contigs2bin_edited.tsv,maxbin.contigs2bin.tsv \
    -l metabat,concoct,maxbin \
    --score_threshold 0.25 \
    -c $assembly \
    -t 12 \
    --write_bins \
    -o zymoMC_das

## works. 
