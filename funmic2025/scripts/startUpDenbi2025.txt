## okay, starting the setup for the funmic practical, spring 2025

## synch up our desktop computer with github:

https://github.com/danchurch/FunctionalMicrobiomePractical.git

git remote add origin https://github.com/danchurch/FunctionalMicrobiomePractical.git
git branch -M main
git remote set-url origin git@github.com:danchurch/FunctionalMicrobiomePractical.git
git push -u origin main

## our setup instance is here:

ssh ubuntu@129.70.51.6 -p 30423

## oops. Wrong key. do we have the denbi key on this work computer?
## this is probably a key on my laptop....

## I had to add a public key from desktop, via the notebook.
## let's do the same for dimitri, using the key he gave me last year.

## for conda, I think Dimitri and I need to share a user on this computer. 
## will just need to add his key. 

## now, is the volume already mounted?

lsblk

## mounted for the moment, 

lsblk -o NAME,SIZE,MOUNTPOINT,FSTYPE,TYPE  ##vdc


## change to user, not root
sudo chown ubuntu:ubuntu /vol/funmic

## if we restart the machine, do we lose the volume?:

## yup. 


## need the uuid, either with:
blkid /dev/vdc

## or 
lsblk -o NAME,SIZE,MOUNTPOINT,FSTYPE,TYPE,UUID | egrep -v "^loop"

cp /etc/fstab ~/fstab.bk

## let's add this to the fstab:
UUID=861ebc07-e98a-4c2f-aacd-f9a96bfeac6f       /vol/funmic      auto    defaults        0       2

## works on reboot.
## we may need to change the UUIDs for the student computers when we start them up.

## need the old fashioned zip tools for alfons data?
sudo apt install unzip

## and now get conda going

### x11 forwarding ###

## We will be using the R plotter, can we setup x11 forwarding?

## following: https://www.strongdm.com/what-is/x11-forwarding

## server side:

sudo apt-get install xauth xorg

cp /etc/ssh/sshd_config ~/sshd_config.bk

vim /etc/ssh/sshd_config

## changed the following lines to
X11Forwarding yes  
X11DisplayOffset 10 
X11UseLocalhost yes
## lines 90,91, 92

## find the service:

systemctl -l --type service --all|grep ssh

sudo systemctl restart ssh.service

## on local side, turned on the same at:
/etc/ssh/ssh_config

## remember to check x11 forwarding on mobaXterm



####### install conda #############
## let's start with miniconda, and put it on the volume:

mkdir -p /vol/funmic/miniconda3

wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /vol/funmic/miniconda3/miniconda.sh
bash /vol/funmic/miniconda3/miniconda.sh -b -u -p /vol/funmic/miniconda3/miniconda3
rm /vol/funmic/miniconda3/miniconda.sh

source /vol/funmic/miniconda3/miniconda3/bin/activate

## add to path env
export PATH=/vol/funmic/miniconda3/miniconda3/bin:$PATH

## initiate with defaults.
conda init

##### metagenome datasets #####

## let's use the mock community data and the kelp dataset, as per last year.

## mock community dataset:

nohup wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR725/009/ERR7255689/ERR7255689_1.fastq.gz -O /vol/funmic/datasets/ERR7255689_1.fastq.gz &
nohup wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR725/009/ERR7255689/ERR7255689_2.fastq.gz -O /vol/funmic/datasets/ERR7255689_2.fastq.gz &

## and the kelp dataset. This was tricky last year.

## let's try the SRA toolkit:

## got the binaries here:
wget https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/current/sratoolkit.current-ubuntu64.tar.gz

tar -xzf sratoolkit.current-ubuntu64.tar.gz

cd /home/ubuntu/sratoolkit.3.1.1-ubuntu64/bin


## set the working directory and temp file location to somewhere on the volume, then 
## seems to work, add to path

export PATH=/home/ubuntu/sratoolkit.3.1.1-ubuntu64/bin:$PATH


## test: 

vdb-config -i

fastq-dump --stdout -X 2 SRR390728

https://www.ncbi.nlm.nih.gov/bioproject/?term=PRJEB36085

## I think we just need the kelp-associated samples, not the water or sediment samples?
## we need SRA run numbers for these, keep clicking till you fun the link for runs.

## I see only 3 non-amplicon files associates with the kelp biofilms.  
## these are:

ERR3801502
ERR3801542
ERR3801603

## saved in 

cd /vol/funmic/datasets/kelpBiofilm

## following this website for 
## https://bioinformaticsworkbook.org/dataAcquisition/fileTransfer/sra.html#gsc.tab=0 

## also these from ncbi:
## https://github.com/ncbi/sra-tools/wiki/08.-prefetch-and-fasterq-dump
## https://github.com/ncbi/sra-tools/wiki/HowTo:-fasterq-dump

/vol/funmic/datasets/kelpBiofilm

## can use old fashioned way:
fastq-dump --split-files --origfmt --gzip ERR3801502

## seems to work, but let's try the prefetch/fasterq combination:

prefetch ERR3801502 
#fasterq-dump --split-spot ERR3801502  ## but this is interleaved. 
fasterq-dump --split-files ERR3801502  ## this seems to works better.

## so a script for this would be:

####### getKelpReads.sh ############
names=(
"ERR3801502"
"ERR3801542"
"ERR3801603"
)

for i in ${names[@]}; do
  prefetch $i
  fasterq-dump --split-files $i
done
####################################

nohup bash getKelpReads.sh &

## looks like it worked. 

########### software installs ###########

## this section is "what worked". 
## there is a section below for testing and for what didn't work.

### solver ###

## first, is conda using the mamba solver?

conda update -n base -c defaults conda

conda update -n base conda
conda install -n base conda-libmamba-solver
conda config --set solver libmamba

## get bioconda and conda forge

conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge

## let's try to keep our environments in line with the boxes 
## in the overal schematic of the class.

## The first is "quality control of raw sequences". So, let's call it "qualityControlRawSequences"

## the installation of trim-galore should be all we need, this includes fastqc:

conda create -n qualityControlRawSequences -c bioconda trim-galore 
 
## install phyloflash

conda create -n communityComposition -c bioconda phyloflash

## we need a formatted silva database for phyloFlash
cd /vol/funmic/databases

## looks like they haven't updated from last year:
wget https://zenodo.org/record/7892522/files/138.1.tar.gz
mv 138.1/ phyloflashSilvaDB/


tar -xzf 138.1.tar.gz

### megahit ###

conda create -n assembly -c bioconda megahit

### quast ###

conda create -n assemblyQC -c bioconda quast

### minimap ### 

## the repo for minimap (https://github.com/lh3/minimap2?tab=readme-ov-file#install)
## says just download binaries. so:

mkdir /vol/funmic/.minimap2

cd /vol/funmic/.minimap2

curl -L https://github.com/lh3/minimap2/releases/download/v2.28/minimap2-2.28_x64-linux.tar.bz2 | tar -jxvf -

## link to somewhere we can find it
sudo ln -s /vol/funmic/.minimap2/minimap2 /usr/local/bin/minimap2

### bbmap ###

## jgi also recommends a direct download 

## try:

cd /vol/funmic

wget https://altushost-swe.dl.sourceforge.net/project/bbmap/BBMap_39.14.tar.gz

tar -xvzf BBMap_39.14.tar.gz

mv bbmap /vol/funmic/.bbmap/

## I guess we just add it to our path, add the following to our bashrc.
## not ideal, but...

export PATH="/vol/funmic/.bbmap/:$PATH"
## added to bashrc file, seems to work

## needs java
sudo apt install default-jre

##### binning #####

## what are the chances the binning software will all play nice with each other?

## vamb behaving weird. never works anyway, leave it out this year, try maxbin2 again

conda create -n binning -c bioconda maxbin2 concoct

#conda remove -n binning --all ## keep needing this

## this was useful when maxbin doesn't completely install:
## conda update maxbin2

## no errors thrown, but can't believe that actually worked...
## this automatically installed samtools as a dependency, newest version.

## and doesn't work. There is a bug in metabat2 concerning 
## the abundance calculation program "jgi_summarize_bam_contig_depths"
## as per: https://bitbucket.org/berkeleylab/metabat/issues/172/jgi_summarize_bam_contig_depths-producing
## also several other bugs concerning negative abundances. 
## the author recommends against using bioconda/conda

## the author suggest docker or compiling from source. I should learn to use docker, but not now.
## https://bitbucket.org/berkeleylab/metabat/src/master/

## try from source. Dependencies:

## 1 boost
apt search libboost-all-dev
sudo apt install libboost-all-dev

## 2 cmake
sudo apt install cmake

## 3 g++
sudo apt install g++

cd /vol/funmic

## try the development version:
wget https://bitbucket.org/berkeleylab/metabat/get/master.tar.gz

tar xzvf master.tar.gz

cd berkeleylab-metabat-*

mkdir build && cd build && cmake .. [ -DCMAKE_INSTALL_PREFIX=/vol/funmic/ ] && make && make test && make install

## seems to have worked, hide it and put in on the path:

mv /vol/funmic/bin /vol/funmic/.metabat

## do we still need this?
rm -r berkeleylab-metabat-453915fb5bbc/

## added to path in bashrc

##### refining of bins #####

### das_tool ###

## as per: https://github.com/cmks/DAS_Tool

conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge
conda create -n refine -c bioconda das_tool

##### depreplication #####

## we need derep for the multiple metagenomes that dimitri has

conda create -n dereplication -c bioconda  drep

##### assign taxonomy to bins #####

## gtdb-tk, as per: https://ecogenomics.github.io/GTDBTk/installing/bioconda.html

conda create -n assignTaxonomy -c conda-forge -c bioconda gtdbtk=2.4.0

conda remove -n assignTaxonomy --all

conda activate assignTaxonomy 

## looks ok, we also need the database:

cd /vol/funmic/databases/gtdb

nohup wget https://data.ace.uq.edu.au/public/gtdb/data/releases/latest/auxillary_files/gtdbtk_package/full_package/gtdbtk_data.tar.gz &> gtdb-download.txt &

tar xvzf gtdbtk_data.tar.gz

## I think we need to tell gtbk-tk this path, I guess they have a script for this?i
## can't find it, try manually:

export GTDBTK_DATA_PATH=/vol/funmic/databases/gtdb/release220/

## but this is more sustainable:

conda env config vars set GTDBTK_DATA_PATH="/vol/funmic/databases/gtdb/release220/"

## check it with their script:

gtdbtk check_install ## this takes a long time, they check all genomes

gtdbtk check_install --db_version

#### metabarcode software ####

### bioconductor ###

## in R, as sudo:

install.packages("BiocManager", repos = "https://cloud.r-project.org")

### dada2 ###

## actually, looks like there is a newer version of dada2 than the documentation says:

BiocManager::install("dada2", version = "3.20")

## of course this failed. Always does.
## debugging dada2 install:

sudo apt install libcurl4-openssl-dev

install.packages("curl")

sudo apt install libssl-dev

install.packages('openssl')

## /usr/bin/ld: cannot find -llapack: No such file or directory

ld -llapack --verbose

sudo apt install libblas-dev liblapack-dev

install.packages('RcppEigen')

sudo apt install libpng-dev

install.packages("png")

sudo apt install libjpeg-dev

install.packages("jpeg")

sudo apt install liblzma-dev
sudo apt install libbz2-dev

BiocManager::install('Rhtslib')

## this fixed the dada2 install, as far as I can tell.

## also need to get the dada2 formatted version of silva:

cd /vol/funmic/databases/metabarcodingSilvaDB

wget https://zenodo.org/records/14169026/files/silva_nr99_v138.2_toGenus_trainset.fa.gz

### vegan ###

install.packages('vegan')

### phyloseq ###

BiocManager::install("phyloseq")

### microtraits  ###

library(BiocManager)

devtools::install_github("jlw-ecoevo/gRodon") ## fails, something about API rates...weird

## try it without dependencies.

devtools::install_github("jlw-ecoevo/gRodon", dependencies = FALSE) ## fails, same prob

## try getting a local copy of the repo:

git clone https://github.com/jlw-ecoevo/gRodon.git

## or get zip:
wget https://github.com/jlw-ecoevo/gRodon2/archive/refs/heads/master.zip

## it's here /vol/funmic/.gRodon

#devtools::install_github("jlw-ecoevo/gRodon", dependencies = FALSE) ## fails, same prob
devtools::install_local("/vol/funmic/.gRodon/master.zip") ## this works.

## now microtraits 

devtools::install_github("ukaraoz/microtrait") ## fails as above

mkdir /vol/funmic/.microtraits

cd /vol/funmic/.microtraits

wget https://github.com/ukaraoz/microtrait/archive/refs/heads/master.zip

## again in R:

## needs:
install.packages("seqinr")

BiocManager::install("Biostrings")

install.packages("seqinr")

devtools::install_local("/vol/funmic/.microtraits/master.zip") ## this works

######### software tests ############

## here is all the "trash" scripting of the process of me testing
## different installs.

### trim-galore: ###

conda activate qualityControlRawSequences 

trim_galore  -help

## let's check it with our new data:

outDir=~/test
file="/vol/funmic/datasets/Barcode2024/raw_reads/M13-07.fastq.gz"
fastqc -t 10 \
  -o $outDir \
  $file 

getFile=/home/ubuntu/test
putDir=/home/daniel/Documents/teaching/funmic/scratchpad
scp -i /home/daniel/.ssh -P 30423 -r ubuntu@129.70.51.6:$getFile $putDir

## fastqc works

## does the cut-adapt side work?
sipRawReads="/vol/funmic/datasets/Barcode2024/raw_reads"
output=/home/ubuntu/test
## run it
trim_galore \
  --cores 7 \
  -o $output \
  --clip_R1 20 \
  --illumina \
  --length 200 \
  ${sipRawReads}/*fastq.gz

## looks good. 

### phyloflash: ###

conda activate communityComposition

phyloFlash.pl -check_env

phyloFlash.pl -dbhome /vol/funmic/databases/phyloflashSilvaDB -lib TEST -CPUs 14 \
 -read1 ${CONDA_PREFIX}/lib/phyloFlash/test_files/test_F.fq.gz \
 -read2 ${CONDA_PREFIX}/lib/phyloFlash/test_files/test_R.fq.gz \
 -almosteverything

## seems to work

### megahit ###

## test megahit on our metagenomes from the kelp:

conda activate assembly

cd /vol/funmic/test

megahit -h

## try with the mock community:

fast1=/vol/funmic/datasets/zymogenMock/ERR7255689_1.fastq
fast2=/vol/funmic/datasets/zymogenMock/ERR7255689_2.fastq
rm -r $outdir
outdir=/vol/funmic/test
\time -v megahit -1 $fast1 \
          -2 $fast2 \
          -t 12 \
          -o $outdir

## this took 3569928 kbytes of RAM, or 3.5 gig, and 2 hours human time
## something funny if I try stdout and stderr redirection, but
## resolve this later. for now install looks good. 

## we can make a script for the kelp assemblies:

kelpDir="/vol/funmic/datasets/kelpBiofilm/"
fast1_1=${kelpDir}ERR3801502_1.fastq
fast1_2=${kelpDir}ERR3801502_2.fastq
fast2_1=${kelpDir}ERR3801542_1.fastq
fast2_2=${kelpDir}ERR3801542_2.fastq
fast3_1=${kelpDir}ERR3801603_1.fastq
fast3_2=${kelpDir}ERR3801603_2.fastq

megahit -1 $fast1_1 \
 -2 $fast1_2 \
 -t 12 \
 -o /vol/funmic/test/kelpAssemblies/kelp1/

megahit -1 $fast2_1 \
 -2 $fast2_2 \
 -t 12 \
 -o /vol/funmic/test/kelpAssemblies/kelp2/

megahit -1 $fast3_1 \
 -2 $fast3_2 \
 -t 12 \
 -o /vol/funmic/test/kelpAssemblies/kelp3/

## saved in assembleKelp.sh

cd /vol/funmic/test/kelpAssemblies

conda activate assembly

nohup bash assembleKelp.sh &



### quast ###

conda activate assemblyQC 

cd /vol/funmic/test

mkdir /vol/funmic/test/assemblyQC 

assembly=/vol/funmic/test/final.contigs.fa
quastOut=/vol/funmic/test/assemblyQC

## run the command
\time quast -t 12 \
  -o $quastOut \
  $assembly &> quastLog.txt &

less quastLog.txt
## trivial amount of time and memory used.
getFile=/vol/funmic/test/assemblyQC/
putDir=/home/daniel/Documents/teaching/funmic/scratchpad/
scp -i /home/daniel/.ssh -P 30423 -r ubuntu@129.70.51.6:$getFile $putDir

## quast works

#### binning ####

## mapping first, with bbmap:

mkdir -p /vol/funmic/test/binning/coverage
cd /vol/funmic/test/binning/coverage


############################################################

## try with the mock community
fast1=/vol/funmic/datasets/zymogenMock/ERR7255689_1.fastq
fast2=/vol/funmic/datasets/zymogenMock/ERR7255689_2.fastq
assembly=/vol/funmic/test/final.contigs.fa
\time -v bbmap.sh \
   threads=14 \
   minid=.97 \
   idfilter=.95 \
   ref=$assembly \
   in=$fast1 \
   in2=$fast2 \
   outm=rawReads_to_assembly.sam \
   bamscript=rawReads_to_assembly_to_bam.sh

## with first kelp files that took 14 minutes, 35 gig of ram
## with mock comm files that took 13 min, 33 gig of ram 

## map reads back to contigs

conda activate binning

\time -v bash rawReads_to_assembly_to_bam.sh ## for mock community, ~20 gig ram, 3 min

## coverage table

cd /vol/funmic/test/binning/coverage

assembly=/vol/funmic/test/final.contigs.fa
sortedBAM=/vol/funmic/test/binning/coverage/rawReads_to_assembly_sorted.bam

jgi_summarize_bam_contig_depths --help

\time -v jgi_summarize_bam_contig_depths \
           --outputDepth coverage_Depths.txt \
           --referenceFasta  $assembly \
           $sortedBAM
## no RAM used, really, <1 min

## repeat for the three kelp metagenomes ##

## kelp1:
#mkdir -p /vol/funmic/test/kelpbinning/kelp1
cd /vol/funmic/test/kelpbinning/kelp1

fast1=/vol/funmic/datasets/kelpBiofilm/ERR3801502_1.fastq
fast2=/vol/funmic/datasets/kelpBiofilm/ERR3801502_2.fastq
assembly=/vol/funmic/test/kelpAssemblies/kelp1/final.contigs.fa
\time -v bbmap.sh \
   threads=14 \
   minid=.97 \
   idfilter=.95 \
   ref=$assembly \
   in=$fast1 \
   in2=$fast2 \
   outm=rawReads_to_assembly.sam \
   bamscript=rawReads_to_assembly_to_bam.sh


conda activate binning

\time -v bash rawReads_to_assembly_to_bam.sh ## 


mkdir /vol/funmic/test/kelpbinning/kelp1/coverage

cd /vol/funmic/test/kelpbinning/kelp1/coverage

assembly=/vol/funmic/test/kelpAssemblies/kelp1/final.contigs.fa
sortedBAM=/vol/funmic/test/kelpbinning/kelp1/rawReads_to_assembly_sorted.bam
\time -v jgi_summarize_bam_contig_depths \
           --outputDepth coverage_Depths.txt \
           --referenceFasta  $assembly \
           $sortedBAM
## kelp2:

#mkdir -p /vol/funmic/test/kelpbinning/kelp2
cd /vol/funmic/test/kelpbinning/kelp2

fast1=/vol/funmic/datasets/kelpBiofilm/ERR3801542_1.fastq
fast2=/vol/funmic/datasets/kelpBiofilm/ERR3801542_2.fastq
assembly=/vol/funmic/test/kelpAssemblies/kelp2/final.contigs.fa
nohup bbmap.sh \
   threads=14 \
   minid=.97 \
   idfilter=.95 \
   ref=$assembly \
   in=$fast1 \
   in2=$fast2 \
   outm=rawReads_to_assembly.sam \
   bamscript=rawReads_to_assembly_to_bam.sh &


conda activate binning ## need samtools

bash rawReads_to_assembly_to_bam.sh 

#mkdir /vol/funmic/test/kelpbinning/kelp2/coverage
cd /vol/funmic/test/kelpbinning/kelp2/coverage

assembly=/vol/funmic/test/kelpAssemblies/kelp2/final.contigs.fa
sortedBAM=/vol/funmic/test/kelpbinning/kelp2/rawReads_to_assembly_sorted.bam

jgi_summarize_bam_contig_depths \
  --outputDepth coverage_Depths.txt \
  --referenceFasta  $assembly \
  $sortedBAM


## kelp3:

mkdir -p /vol/funmic/test/kelpbinning/kelp3

cd /vol/funmic/test/kelpbinning/kelp3

fast1=/vol/funmic/datasets/kelpBiofilm/ERR3801603_1.fastq
fast2=/vol/funmic/datasets/kelpBiofilm/ERR3801603_2.fastq
assembly=/vol/funmic/test/kelpAssemblies/kelp3/final.contigs.fa

nohup bbmap.sh \
   threads=14 \
   minid=.97 \
   idfilter=.95 \
   ref=$assembly \
   in=$fast1 \
   in2=$fast2 \
   outm=rawReads_to_assembly.sam \
   bamscript=rawReads_to_assembly_to_bam.sh &

conda activate binning ## need samtools

bash rawReads_to_assembly_to_bam.sh 

mkdir /vol/funmic/test/kelpbinning/kelp3/coverage

cd /vol/funmic/test/kelpbinning/kelp3/coverage

assembly=/vol/funmic/test/kelpAssemblies/kelp3/final.contigs.fa
sortedBAM=/vol/funmic/test/kelpbinning/kelp3/rawReads_to_assembly_sorted.bam

jgi_summarize_bam_contig_depths \
  --outputDepth coverage_Depths.txt \
  --referenceFasta  $assembly \
  $sortedBAM

### metabat ###

mkdir /vol/funmic/test/binning/metabat

cd /vol/funmic/test/binning/metabat

assembly=/vol/funmic/test/final.contigs.fa
coverageTable=/vol/funmic/test/binning/coverage/coverage_Depths.txt
\time -v  metabat2 \
            -i $assembly \
            -a $coverageTable \
            -o "metabat" \
            -t 12 

## some errors concerning negative values, check this out later
## errors gone now, with updated metabat2

## kelp metabat ##

## kelp1

mkdir /vol/funmic/test/kelpbinning/kelp1/metabat
cd /vol/funmic/test/kelpbinning/kelp1/metabat

assembly=/vol/funmic/test/kelpAssemblies/kelp1/final.contigs.fa
coverageTable=/vol/funmic/test/kelpbinning/kelp1/coverage/coverage_Depths.txt
\time -v  metabat2 \
            -i $assembly \
            -a $coverageTable \
            -o "metabat" \
            -t 12 


## kelp2
mkdir /vol/funmic/test/kelpbinning/kelp2/metabat
cd /vol/funmic/test/kelpbinning/kelp2/metabat
assembly=/vol/funmic/test/kelpAssemblies/kelp2/final.contigs.fa
coverageTable=/vol/funmic/test/kelpbinning/kelp2/coverage/coverage_Depths.txt
\time -v  metabat2 \
            -i $assembly \
            -a $coverageTable \
            -o "metabat" \
            -t 12 

## kelp3
mkdir /vol/funmic/test/kelpbinning/kelp3/metabat
cd /vol/funmic/test/kelpbinning/kelp3/metabat
assembly=/vol/funmic/test/kelpAssemblies/kelp3/final.contigs.fa
coverageTable=/vol/funmic/test/kelpbinning/kelp3/coverage/coverage_Depths.txt
\time -v  metabat2 \
            -i $assembly \
            -a $coverageTable \
            -o "metabat" \
            -t 12 

### maxbin2 ###

## try it, if it works, use this instead of vamb:

mkdir -p /vol/funmic/test/binning/maxbin/maxbinOut

cd /vol/funmic/test/binning/maxbin

cut -f1,3 /vol/funmic/test/binning/coverage/coverage_Depths.txt > abundance.list

assembly=/vol/funmic/test/final.contigs.fa
\time -v run_MaxBin.pl -thread 12 -min_contig_length 1500 \
              -contig $assembly \
              -abund abundance.list \
              -out  /vol/funmic/test/binning/maxbin/maxbinOut/maxbin

## 1 min, 0.5 gig RAM

## kelp1

mkdir -p /vol/funmic/test/kelpbinning/kelp1/maxbin/maxbinOut
cd /vol/funmic/test/kelpbinning/kelp1/maxbin

cut -f1,3 /vol/funmic/test/kelpbinning/kelp1/coverage/coverage_Depths.txt > abundance.list

assembly=/vol/funmic/test/kelpAssemblies/kelp1/final.contigs.fa
run_MaxBin.pl -thread 12 -min_contig_length 1500 \
     -contig $assembly \
     -abund abundance.list \
     -out /vol/funmic/test/kelpbinning/kelp1/maxbin/maxbinOut


## kelp2
mkdir -p /vol/funmic/test/kelpbinning/kelp2/maxbin/maxbinOut
cd /vol/funmic/test/kelpbinning/kelp2/maxbin
cut -f1,3 /vol/funmic/test/kelpbinning/kelp2/coverage/coverage_Depths.txt > abundance.list
assembly=/vol/funmic/test/kelpAssemblies/kelp2/final.contigs.fa
run_MaxBin.pl -thread 12 -min_contig_length 1500 \
     -contig $assembly \
     -abund abundance.list \
     -out /vol/funmic/test/kelpbinning/kelp2/maxbin/maxbinOut

## kelp3
mkdir -p /vol/funmic/test/kelpbinning/kelp3/maxbin/maxbinOut
cd /vol/funmic/test/kelpbinning/kelp3/maxbin
cut -f1,3 /vol/funmic/test/kelpbinning/kelp3/coverage/coverage_Depths.txt > abundance.list
assembly=/vol/funmic/test/kelpAssemblies/kelp3/final.contigs.fa
run_MaxBin.pl -thread 12 -min_contig_length 1500 \
     -contig $assembly \
     -abund abundance.list \
     -out /vol/funmic/test/kelpbinning/kelp3/maxbin/maxbinOut

## for some reason, outputs aren't going to maxbinout. Oh well, moving them there.

### concoct ###

conda activate binning

mkdir -p /vol/funmic/test/binning/concoct

cd /vol/funmic/test/binning/concoct

## define our variables
assembly=/vol/funmic/test/final.contigs.fa
sortedBAM=/vol/funmic/test/binning/coverage/rawReads_to_assembly_sorted.bam
outdir=/vol/funmic/test/binning/concoct

## run the software
## concoct has several steps, each with it's own script:

## samtools needs us to index the sorted read alignments:

#samtools index -@ 12 $readAlignments ## think this is already indexed

# cut_up_fasta.py $assembly -c 10000 -o 0 --merge_last -b concoctContigs_10K.bed > concoctContigs_10K.fa
# concoct_coverage_table.py concoctContigs_10K.bed $sortedBAM > coverage_table.tsv 

## still not working. Try an install of concoct with older python, like last year.

## can we skip this step by reformatting our old coverage table?:

cut -f 1,4 $coverageTable | sed '1d' | sed '1i contig_id\tsample_1' > concoct_coverage.txt

assembly=/vol/funmic/test/final.contigs.fa
\time -v concoct \
            --composition_file $assembly \
            --coverage_file concoct_coverage.txt \
            -t 12

mkdir fasta_bins

extract_fasta_bins.py $assembly clustering_gt1000.csv --output_path fasta_bins/

## looks promising, but we want pretty file names
## time for more BASH magic:

cd fasta_bins
for i in *; do
  mv $i ${i/\.fa/_concoct\.fa}
done

## kelp1 concoct:

mkdir -p /vol/funmic/test/kelpbinning/kelp1/concoct/fasta_bins

cd /vol/funmic/test/kelpbinning/kelp1/concoct/

assembly=/vol/funmic/test/kelpAssemblies/kelp1/final.contigs.fa
sortedBAM=/vol/funmic/test/kelpbinning/kelp1/rawReads_to_assembly_sorted.bam
outdir=/vol/funmic/test/kelpbinning/kelp1/concoct/
coverageTable=/vol/funmic/test/kelpbinning/kelp1/coverage/coverage_Depths.txt
cut -f 1,4 $coverageTable | sed '1d' | sed '1i contig_id\tsample_1' > concoct_coverage.txt
concoct \
   --composition_file $assembly \
   --coverage_file concoct_coverage.txt \
   -t 12

## that actually took some time:

extract_fasta_bins.py $assembly clustering_gt1000.csv --output_path fasta_bins/

cd /vol/funmic/test/kelpbinning/kelp1/concoct/fasta_bins
for i in *; do
  mv $i ${i/\.fa/_concoct\.fa}
done

## kelp2 concoct:

## that took some time. let's make a script:

### kelp2concoct.sh ###################
mkdir -p /vol/funmic/test/kelpbinning/kelp2/concoct/fasta_bins
cd /vol/funmic/test/kelpbinning/kelp2/concoct/
assembly=/vol/funmic/test/kelpAssemblies/kelp2/final.contigs.fa
sortedBAM=/vol/funmic/test/kelpbinning/kelp2/rawReads_to_assembly_sorted.bam
outdir=/vol/funmic/test/kelpbinning/kelp2/concoct/
coverageTable=/vol/funmic/test/kelpbinning/kelp2/coverage/coverage_Depths.txt
cut -f 1,4 $coverageTable | sed '1d' | sed '1i contig_id\tsample_1' > concoct_coverage.txt
concoct \
   --composition_file $assembly \
   --coverage_file concoct_coverage.txt \
   -t 6
extract_fasta_bins.py $assembly clustering_gt1000.csv --output_path fasta_bins/
cd /vol/funmic/test/kelpbinning/kelp2/concoct/fasta_bins
for i in *; do
  mv $i ${i/\.fa/_concoct\.fa}
done
###########################################

### kelp3concoct.sh ###################
mkdir -p /vol/funmic/test/kelpbinning/kelp3/concoct/fasta_bins
cd /vol/funmic/test/kelpbinning/kelp3/concoct/
assembly=/vol/funmic/test/kelpAssemblies/kelp3/final.contigs.fa
sortedBAM=/vol/funmic/test/kelpbinning/kelp3/rawReads_to_assembly_sorted.bam
outdir=/vol/funmic/test/kelpbinning/kelp3/concoct/

coverageTable=/vol/funmic/test/kelpbinning/kelp3/coverage/coverage_Depths.txt

cut -f 1,4 $coverageTable | sed '1d' | sed '1i contig_id\tsample_1' > concoct_coverage.txt
concoct \
   --composition_file $assembly \
   --coverage_file concoct_coverage.txt \
   -t 6
extract_fasta_bins.py $assembly clustering_gt1000.csv --output_path fasta_bins/
cd /vol/funmic/test/kelpbinning/kelp3/concoct/fasta_bins
for i in *; do
  mv $i ${i/\.fa/_concoct\.fa}
done
###########################################

nohup bash ./kelp2concoct.sh &
nohup bash ./kelp3concoct.sh &> kelpsconcoct.log &

#### refining ####

### test das_tool ###

conda activate refine

mkdir /vol/funmic/test/refine

cd /vol/funmic/test/refine

metabatBins=/vol/funmic/test/binning/metabat
concoctBins=/vol/funmic/test/binning/concoct/fasta_bins
maxbinBins=/vol/funmic/test/binning/maxbin/maxbinOut
assembly=/vol/funmic/test/final.contigs.fa

## assign contigs to bins

## metabat bins
Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $metabatBins \
    > metabat.contigs2bin.tsv

paste <(cut  -f 1 metabat.contigs2bin.tsv) <(cut -f 4 metabat.contigs2bin.tsv) > metabat.contigs2bin_edited.tsv

head metabat*

## concoct:
Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $concoctBins \
    > concoct.contigs2bin.tsv

paste <(cut -d " " -f 1 concoct.contigs2bin.tsv) <(cut -f 2 concoct.contigs2bin.tsv) > concoct.contigs2bin_edited.tsv

head concoct*

## maxbin:
Fasta_to_Contig2Bin.sh \
    -e fasta \
    -i $maxbinBins \
    > maxbin.contigs2bin.tsv

head maxbin.contigs2bin.tsv
## don't need to edit this table, looks good

DAS_Tool \
    -i metabat.contigs2bin_edited.tsv,concoct.contigs2bin_edited.tsv,maxbin.contigs2bin.tsv \
    -l metabat,concoct,maxbin \
    --score_threshold 0.25 \
    -c $assembly \
    -t 12 \
    --write_bins \
    -o zymoMC_das

## works. now try with the kelp files

## dastool kelp1

conda activate refine

mkdir -p /vol/funmic/test/kelprefine/kelp1

cd /vol/funmic/test/kelprefine/kelp1

metabatBins=/vol/funmic/test/kelpbinning/kelp1/metabat
concoctBins=/vol/funmic/test/kelpbinning/kelp1/concoct/fasta_bins
maxbinBins=/vol/funmic/test/kelpbinning/kelp1/maxbin/maxbinOut
assembly=/vol/funmic/test/kelpAssemblies/kelp1/final.contigs.fa

## metabat bins
Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $metabatBins \
    > metabat.contigs2bin.tsv

paste <(cut  -f 1 metabat.contigs2bin.tsv) <(cut -f 4 metabat.contigs2bin.tsv) > metabat.contigs2bin_edited.tsv

## concoct:
Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $concoctBins \
    > concoct.contigs2bin.tsv

paste <(cut -d " " -f 1 concoct.contigs2bin.tsv) <(cut -f 2 concoct.contigs2bin.tsv) > concoct.contigs2bin_edited.tsv

## maxbin:
Fasta_to_Contig2Bin.sh \
    -e fasta \
    -i $maxbinBins \
    > maxbin.contigs2bin.tsv

DAS_Tool \
    -i metabat.contigs2bin_edited.tsv,concoct.contigs2bin_edited.tsv,maxbin.contigs2bin.tsv \
    -l metabat,concoct,maxbin \
    --score_threshold 0.25 \
    -c $assembly \
    -t 14 \
    --write_bins \
    -o zymoMC_das

## kelp2
mkdir -p /vol/funmic/test/kelprefine/kelp2
cd /vol/funmic/test/kelprefine/kelp2
metabatBins=/vol/funmic/test/kelpbinning/kelp2/metabat
concoctBins=/vol/funmic/test/kelpbinning/kelp2/concoct/fasta_bins
maxbinBins=/vol/funmic/test/kelpbinning/kelp2/maxbin/maxbinOut
assembly=/vol/funmic/test/kelpAssemblies/kelp2/final.contigs.fa
## metabat bins
Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $metabatBins \
    > metabat.contigs2bin.tsv

paste <(cut  -f 1 metabat.contigs2bin.tsv) <(cut -f 4 metabat.contigs2bin.tsv) > metabat.contigs2bin_edited.tsv
## concoct:
Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $concoctBins \
    > concoct.contigs2bin.tsv

paste <(cut -d " " -f 1 concoct.contigs2bin.tsv) <(cut -f 2 concoct.contigs2bin.tsv) > concoct.contigs2bin_edited.tsv
## maxbin:
Fasta_to_Contig2Bin.sh \
    -e fasta \
    -i $maxbinBins \
    > maxbin.contigs2bin.tsv

DAS_Tool \
    -i metabat.contigs2bin_edited.tsv,concoct.contigs2bin_edited.tsv,maxbin.contigs2bin.tsv \
    -l metabat,concoct,maxbin \
    --score_threshold 0.25 \
    -c $assembly \
    -t 14 \
    --write_bins \
    -o zymoMC_das

## kelp3
mkdir -p /vol/funmic/test/kelprefine/kelp3
cd /vol/funmic/test/kelprefine/kelp3
metabatBins=/vol/funmic/test/kelpbinning/kelp3/metabat
concoctBins=/vol/funmic/test/kelpbinning/kelp3/concoct/fasta_bins
maxbinBins=/vol/funmic/test/kelpbinning/kelp3/maxbin/maxbinOut
assembly=/vol/funmic/test/kelpAssemblies/kelp3/final.contigs.fa
## metabat bins
Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $metabatBins \
    > metabat.contigs2bin.tsv
paste <(cut  -f 1 metabat.contigs2bin.tsv) <(cut -f 4 metabat.contigs2bin.tsv) > metabat.contigs2bin_edited.tsv
## concoct:
Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $concoctBins \
    > concoct.contigs2bin.tsv
paste <(cut -d " " -f 1 concoct.contigs2bin.tsv) <(cut -f 2 concoct.contigs2bin.tsv) > concoct.contigs2bin_edited.tsv
## maxbin:
Fasta_to_Contig2Bin.sh \
    -e fasta \
    -i $maxbinBins \
    > maxbin.contigs2bin.tsv
DAS_Tool \
    -i metabat.contigs2bin_edited.tsv,concoct.contigs2bin_edited.tsv,maxbin.contigs2bin.tsv \
    -l metabat,concoct,maxbin \
    --score_threshold 0.25 \
    -c $assembly \
    -t 14 \
    --write_bins \
    -o zymoMC_das

## should have changed the output directory. But anyway, looks like it worked. 

###### dRep ######

conda activate dereplication

## for these, we need to give unique names to each kelp metagenome

## names of directories are screwed up, because reused old code. But here they are:

cd /vol/funmic/test/kelprefine/kelp1/zymoMC_das_DASTool_bins
ls -1 | wc -l ## 23

for i in *; do
  mv $i "kelp1_"${i}
done

cd /vol/funmic/test/kelprefine/kelp2/zymoMC_das_DASTool_bins
ls -1 | wc -l ## 28

for i in *; do
  mv $i "kelp2_"${i}
done

cd /vol/funmic/test/kelprefine/kelp3/zymoMC_das_DASTool_bins
ls -1 | wc -l ## 29

for i in *; do
  mv $i "kelp3_"${i}
done

## get them all into one directory:

mkdir -p /vol/funmic/test/kelpDrep/all_bins
mkdir /vol/funmic/test/kelpDrep/dereplicated_bins

cp /vol/funmic/test/kelprefine/kelp?/zymoMC_das_DASTool_bins/* /vol/funmic/test/kelpDrep/all_bins

ls /vol/funmic/test/kelpDrep/all_bins -1 | wc -l ## all there

cd /vol/funmic/test/kelpDrep

## try dRep
ALL_BINS_DIRECTORY=/vol/funmic/test/kelpDrep/all_bins
DEREP_BINS_DIRECTORY=/vol/funmic/test/kelpDrep/dereplicated_bins
\time -v dRep dereplicate \
      ${DEREP_BINS_DIRECTORY} \
      -p 12 \
      -g ${ALL_BINS_DIRECTORY}/*.fa \
      -comp 40 \
      -l 500000 \
      -comW 1 \
      -sizeW 1 \
      -conW 5 \
      -strW 1 \
      -N50W 0.5 \
      --run_tertiary_clustering 

## this runs for a ~long time, 24 min, 36 gig

## great, now what? 

## looks like we get checkm scores of the final contigs
## with "dereplicated_bins/data_tables/Widb.csv"

## now onto taxonomy...

##### gtdb-tk ########

conda activate assignTaxonomy

cd /vol/funmic/test/kelpAssignTaxonomy 

mags=/vol/funmic/test/kelpDrep/dereplicated_bins/dereplicated_genomes


## do the gene predictions, and find the marker genes present in each mag

\time -v gtdbtk identify --genome_dir $mags --out_dir ./identify --extension "fa" --cpus 12 &> gtdb-identify.log &
## less than a gig, 2 minutes

## run multiple alignments of these markers, figure out which domain of life
## we are dealing with.
\time -v gtdbtk align --identify_dir ./identify --out_dir ./align --cpus 12 &> gtdb-align.log &
## 9 gig ram, 6 minutes

\time -v gtdbtk classify --genome_dir $mags \
                         --align_dir ./align \
                         --out_dir ./classify -x "fa" \
                         --out_dir ./classify -x "fa" \
                         --mash_db ./mashDB &> gtdb-classify.log &

## that took 80 gig RAM (expected), 5 hours (!!)

## did it work?

cd ./kelpAssignTaxonomy/classify/

less gtdbtk.bac120.summary.tsv

## this seems to have worked. 

## now deal with the new metabarcode pipeline and see what Dimitri is thinking for annotation.

###############################################

### metabarcoding ###

cd /vol/funmic/test/metabarcoding


conda activate qualityControlRawSequences 

## start with a fastqc review:

## data is here:

metabarcodeD=/vol/funmic/datasets/Barcode2024/raw_reads/

## combine into one fasta


cat ${metabarcodeD}*.gz > allfastqs.fastq.gz

## look at it:

fastqc -o . -t 12 allfastqs.fastq.gz

## get it local:

getFile=/vol/funmic/test/metabarcoding/allfastqs_fastqc.html
putDir=/home/daniel/Documents/teaching/funmic/scratchpad/
scp -i /home/daniel/.ssh -P 30423 -r ubuntu@129.70.51.6:$getFile $putDir

## we'll need to cut the primers.

## use cutadapt. What are our primers again?  

## if they are regular 16sV4 with illumina tags:

gunzip  allfastqs.fastq.gz

## here are the phased versions of the 515f:
tcgtcggcagcgtcagatgtgtataagagacag  gtgycagcmgccgcggtaa
tcgtcggcagcgtcagatgtgtataagagacag cgtgycagcmgccgcggtaa
tcgtcggcagcgtcagatgtgtataagagacagtcgtgycagcmgccgcggtaa

grep  GTG.CAGC.GCCGCGGTAA allfastqs.fastq

grep -c  GTG.CAGC.GCCGCGGTAA allfastqs.fastq ## 794,095 primers found

grep -c  @FS1 allfastqs.fastq ## 1,241,090 reads total

## anyway, how do we get rid of these with cutadapt? 


## test out cutadapt

cutadapt -g TCGTGYCAGCMGCCGCGGTAA -o test.fastq allfastqs.fastq | tee test.log

grep GTG.CAGC.GCCGCGGTAA test.fastq ## one slipped through:

cutadapt -g XTCGTGYCAGCMGCCGCGGTAA -o test2.fastq allfastqs.fastq | tee test2.log

grep GTG.CAGC.GCCGCGGTAA test2.fastq ## two slipped through. 

cutadapt -g XTCGTGYCAGCMGCCGCGGTAA -o test2.fastq --rc allfastqs.fastq | tee test2.log

## reverse compliment is:

GGACTACNVGGGTWTCTAAT

grep -c GGACTAC..GGGT.TCTAAT allfastqs.fastq ## 32, not many. But there. 

## to remove these:
cutadapt -g GGACTACNVGGGTWTCTAAT -o test3.fastq --rc test2.fastq 

grep -c GGACTAC..GGGT.TCTAAT test3.fastq ## 

## so try some thing like this, for all files:

cd /vol/funmic/test/metabarcoding

cat ${metabarcodeD}*.gz > allfastqs.fastq.gz

conda activate qualityControlRawSequences 

mkdir /vol/funmic/test/metabarcoding/trimmed_reads

conda activate qualityControlRawSequences 

metabarcodeD=/vol/funmic/datasets/Barcode2024/raw_reads/
outDir=/vol/funmic/test/metabarcoding/trimmed_reads/
cd $metabarcodeD
for i in *.gz; do
  ls $i
  trimmedFASTQ=${outDir}${i/.fastq\.gz/_trimmed.fastq.gz }
  echo $trimmedFASTQ
  cutadapt --rc -g TCGTGYCAGCMGCCGCGGTAA -o ${trimmedFASTQ} $i | tee -a cutadapt.log
done

## too complicated for students. Maybe?:

## set up files, maybe do this for them.

cd /vol/funmic/datasets/Barcode2024/raw_reads/


## define some variables
inputFastq=A12-05.fastq.gz 
outputFastq=A12-05_trimmed.fastq.gz
outputDir=/vol/funmic/test/metabarcoding/trimmed_reads/

cutadapt \
  --rc \
  -g TCGTGYCAGCMGCCGCGGTAA \
  --cut 10 \
  -o $outputDir$outputFastq \
     $inputFastq  

ls $outputDir$inputFastq  

## then show them a loop:

cd /vol/funmic/datasets/Barcode2024/raw_reads/

for inputFastq in *; do
  outputFastq=${inputFastq/.fastq.gz/_trimmed.fastq.gz } ## BASH string magic
  cutadapt \
    --rc \
  --length 280 \
    -g TCGTGYCAGCMGCCGCGGTAA \
    -o $outputDir${outputFastq} \
       $inputFastq  
done

## check on these:

cd /vol/funmic/test/metabarcoding/trimmed_reads

cat *.fastq.gz > alltrimmedreads.fastq.gz

mv alltrimmedreads.fastq.gz alltrimmedreads280.fastq.gz

conda activate qualityControlRawSequences 

fastqc -o . -t 12 alltrimmedreads280.fastq.gz

#getFile=/vol/funmic/test/metabarcoding/trimmed_reads/alltrimmedreads_fastqc.html
getFile=/vol/funmic/test/metabarcoding/trimmed_reads/alltrimmedreads280_fastqc.html
putDir=/home/daniel/Documents/teaching/funmic/scratchpad/
scp -i /home/daniel/.ssh -P 30423 -r ubuntu@129.70.51.6:$getFile $putDir

## something like that. 

## if we had more time, would do more checks. But we have just an afternoon.

## and then?

### dada2 ###

## following the same old tutorial: http://benjjneb.github.io/dada2/tutorial.html

cd /vol/funmic/test/metabarcoding/

## trimmed reads are here:

R

setwd("/vol/funmic/test/metabarcoding/")

library(dada2); packageVersion("dada2")

path <- "/vol/funmic/test/metabarcoding/trimmed_reads" 

list.files(path)

fns <- sort(list.files(path, pattern=".fastq.gz", full.names = TRUE))

sample.names <- sapply(strsplit(basename(fns), "_"), '[', 1)

plotQualityProfile(fns[1])

# Place filtered files in filtered/ subdirectory

filterPath <- "/vol/funmic/test/metabarcoding/filtered"

filtFns <- file.path(filterPath, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))

names(filtFns) <- sample.names

out <- filterAndTrim(fns, filtFns,
              maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=TRUE) 

head(out)

## we could probably do the above code ourselves, give the filtered reads
## to the class, preprepared?

out

### computationally intensive dada2 steps ###
print ("started:"); Sys.time()
errF <- learnErrors(filtFns, multithread=TRUE) ## ran for ~15 min. memory no problem
plotErrors(errF, nominalQ=TRUE)
dadaF <- dada(filtFns, err=errF, pool=TRUE, multithread=TRUE)
seqtab <- makeSequenceTable(dadaF) ## 
saveRDS(dadaF, "dadaF.rds")
saveRDS(errF, "errF.rds")
saveRDS(seqtab, "seqtab.rds")
print ("finished:"); Sys.time()
### end computationally intensive dada2 steps ####
## that took 1 hour, 11 minutes
## jeezus. Try to get them there over lunch? 

dadaF[[1]] ## 1769 ASVs

### starting back, rerun the above code:
#dadaF <- readRDS("dadaF.rds")
#errF <- readRDS("errF.rds")
#seqtab <- readRDS("seqtab.rds")

dim(seqtab) ## 2614 columns. how can this have more columns than our number of asvs? 

table(nchar(getSequences(seqtab))) ## most are 273 bp long.

barplot(table(nchar(getSequences(seqtab)))) 

table(nchar(getSequences(seqtab))) |> barplot() 

seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE) ## 746 bimeras


dim(seqtab.nochim)

table(nchar(getSequences(seqtab.nochim))) |> barplot() 

## this dropped us to 1868 columns. Still don't understand how I can have more columns than ASVs...

getN <- function(x) sum(getUniques(x))

track <- cbind(out, sapply(dadaF, getN), rowSums(seqtab.nochim))

colnames(track) <- c("input", "filtered", "denoised","nonchim")

rownames(track) <- sample.names

track

path2silvaDB <- "/vol/funmic/databases/metabarcodingSilvaDB/silva_nr99_v138.2_toGenus_trainset.fa.gz"
print ("started:"); Sys.time()
taxa <- assignTaxonomy(seqtab.nochim, path2silvaDB, multithread=TRUE) ## started 11:58 on tuesday
print ("finished:"); Sys.time() ## only took 3 minutes. Why do I remember that being more intensive?

taxa.print <- taxa # Removing sequence rownames for display only
rownames(taxa.print) <- NULL

head(taxa.print)

taxa.print[2,]

## from here play with phyloseq
## rarify down
## stacked tax graphs
## ordinations 
## permanova by treatments of interest.
## PCoAs also.

##### phyloseq #####

library(dada2)
library(phyloseq)
library(Biostrings)
library(ggplot2)

samples.out <- rownames(seqtab.nochim)
substrate <- sapply(samples.out, substring, 1,1)
isotope <- sapply(samples.out, substring, 2,3)
fraction <- sapply(samples.out, substring, 5,6)
samdf <- data.frame(Substrate=substrate, Isotope=isotope, Fraction=fraction)
rownames(samdf) <- samples.out

## check it: 
samdf

## probably best to make a csv of this and have the students import it

ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               sample_data(samdf), 
               tax_table(taxa))

dna <- Biostrings::DNAStringSet(taxa_names(ps))
names(dna) <- taxa_names(ps)
ps <- merge_phyloseq(ps, dna)
taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))
#save(ps, file="ps.rda")

plot_richness(ps, x="Substrate", measures=c("Observed", "Simpson"), color="Isotope")

## maybe do this after rarefaction

ps.prop <- transform_sample_counts(ps, function(otu) otu/sum(otu))
ord.nmds.bray <- ordinate(ps.prop, method="NMDS", distance="bray")

plot_ordination(ps.prop, ord.nmds.bray, color="Substrate", shape="Isotope", title="Bray NMDS")

## which fraction should we be looking at?

## just realized, I think there is no real repetition here. As, in
## there should be one or two fractions that represent the heavy fraction of 
## DNA, and these should be enriched in various microbes 

## shit. Not good for multivariate analysis. But, run a few permanovas?

top20 <- names(sort(taxa_sums(ps), decreasing=TRUE))[1:20]
ps.top20 <- transform_sample_counts(ps, function(OTU) OTU/sum(OTU))
ps.top20 <- prune_taxa(top20, ps.top20)


plot_bar(ps.top20, x="Fraction", fill="Family") + facet_wrap(~Substrate, scales="free_x") 

c13 <- rownames(sample_data(ps)[sample_data(ps)$Isotope == "13",])

psC13 = prune_samples(c13, ps)

top20 <- names(sort(taxa_sums(psC13), decreasing=TRUE))[1:20]
psC13.top20 <- transform_sample_counts(psC13, function(OTU) OTU/sum(OTU))
psC13.top20 <- prune_taxa(top20, psC13.top20)
plot_bar(psC13.top20, x="Fraction", fill="Family") + facet_wrap(~Substrate, scales="free_x") 

## can we export the data as Tillmann uses it?

sink('finishInstall27.2.txt')
Sys.time()
sink()

print("finished:")
Sys.time()

