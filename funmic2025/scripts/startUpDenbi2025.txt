## okay, starting the setup for the funmic practical, spring 2025

## synch up our desktop computer with github:

https://github.com/danchurch/FunctionalMicrobiomePractical.git

git remote add origin https://github.com/danchurch/FunctionalMicrobiomePractical.git
git branch -M main
git remote set-url origin git@github.com:danchurch/FunctionalMicrobiomePractical.git
git push -u origin main

## our setup instance is here:

ssh ubuntu@129.70.51.6 -p 30423

## oops. Wrong key. do we have the denbi key on this work computer?
## this is probably a key on my laptop....

## I had to add a public key from desktop, via the notebook.
## let's do the same for dimitri, using the key he gave me last year.

## for conda, I think Dimitri and I need to share a user on this computer. 
## will just need to add his key. 

## now, is the volume already mounted?

lsblk

## mounted for the moment, 

lsblk -o NAME,SIZE,MOUNTPOINT,FSTYPE,TYPE  ##vdc


## change to user, not root
sudo chown ubuntu:ubuntu /vol/funmic

## if we restart the machine, do we lose the volume?:

## yup. 


## need the uuid, either with:
blkid /dev/vdc

## or 
lsblk -o NAME,SIZE,MOUNTPOINT,FSTYPE,TYPE,UUID | egrep -v "^loop"

cp /etc/fstab ~/fstab.bk

## let's add this to the fstab:
UUID=861ebc07-e98a-4c2f-aacd-f9a96bfeac6f       /vol/funmic      auto    defaults        0       2
     861ebc07-e98a-4c2f-aacd-f9a96bfeac6f

## works on reboot.
## we may need to change the UUIDs for the student computers when we start them up.


## need the old fashioned zip tools for alfons data?
sudo apt install unzip

## and now get conda going

### x11 forwarding ###

## We will be using the R plotter, can we setup x11 forwarding?

## following: https://www.strongdm.com/what-is/x11-forwarding

## server side:

sudo apt-get install xauth xorg

cp /etc/ssh/sshd_config ~/sshd_config.bk

vim /etc/ssh/sshd_config

## changed the following lines to
X11Forwarding yes  
X11DisplayOffset 10 
X11UseLocalhost yes
## lines 90,91, 92

## find the service:

systemctl -l --type service --all|grep ssh

sudo systemctl restart ssh.service

## on local side, turned on the same at:
/etc/ssh/ssh_config

## remember to check x11 forwarding on mobaXterm



####### install conda #############
## let's start with miniconda, and put it on the volume:

mkdir -p /vol/funmic/miniconda3

wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /vol/funmic/miniconda3/miniconda.sh
bash /vol/funmic/miniconda3/miniconda.sh -b -u -p /vol/funmic/miniconda3/miniconda3
rm /vol/funmic/miniconda3/miniconda.sh

source /vol/funmic/miniconda3/miniconda3/bin/activate

## add to path env
export PATH=/vol/funmic/miniconda3/miniconda3/bin:$PATH

## initiate with defaults.
conda init

##### metagenome datasets #####

## let's use the mock community data and the kelp dataset, as per last year.

## mock community dataset:

nohup wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR725/009/ERR7255689/ERR7255689_1.fastq.gz -O /vol/funmic/datasets/ERR7255689_1.fastq.gz &
nohup wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR725/009/ERR7255689/ERR7255689_2.fastq.gz -O /vol/funmic/datasets/ERR7255689_2.fastq.gz &

## and the kelp dataset. This was tricky last year.

## let's try the SRA toolkit:

## got the binaries here:
wget https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/current/sratoolkit.current-ubuntu64.tar.gz

tar -xzf sratoolkit.current-ubuntu64.tar.gz

cd /home/ubuntu/sratoolkit.3.1.1-ubuntu64/bin


## set the working directory and temp file location to somewhere on the volume, then 
## seems to work, add to path

export PATH=/home/ubuntu/sratoolkit.3.1.1-ubuntu64/bin:$PATH


## test: 

vdb-config -i

fastq-dump --stdout -X 2 SRR390728

https://www.ncbi.nlm.nih.gov/bioproject/?term=PRJEB36085

## I think we just need the kelp-associated samples, not the water or sediment samples?
## we need SRA run numbers for these, keep clicking till you fun the link for runs.

## I see only 3 non-amplicon files associates with the kelp biofilms.  
## these are:

ERR3801502
ERR3801542
ERR3801603

## saved in 

cd /vol/funmic/datasets/kelpBiofilm

## following this website for 
## https://bioinformaticsworkbook.org/dataAcquisition/fileTransfer/sra.html#gsc.tab=0 

## also these from ncbi:
## https://github.com/ncbi/sra-tools/wiki/08.-prefetch-and-fasterq-dump
## https://github.com/ncbi/sra-tools/wiki/HowTo:-fasterq-dump

/vol/funmic/datasets/kelpBiofilm

## can use old fashioned way:
fastq-dump --split-files --origfmt --gzip ERR3801502

## seems to work, but let's try the prefetch/fasterq combination:

prefetch ERR3801502 
#fasterq-dump --split-spot ERR3801502  ## but this is interleaved. 
fasterq-dump --split-files ERR3801502  ## this seems to works better.

## so a script for this would be:

####### getKelpReads.sh ############
names=(
"ERR3801502"
"ERR3801542"
"ERR3801603"
)

for i in ${names[@]}; do
  prefetch $i
  fasterq-dump --split-files $i
done
####################################

nohup bash getKelpReads.sh &

## looks like it worked. 

########### software installs ###########

## this section is "what worked". 
## there is a section below for testing and for what didn't work.

### solver ###

## first, is conda using the mamba solver?

conda update -n base -c defaults conda

conda update -n base conda
conda install -n base conda-libmamba-solver
conda config --set solver libmamba

## get bioconda and conda forge

conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge

## let's try to keep our environments in line with the boxes 
## in the overal schematic of the class.

## The first is "quality control of raw sequences". So, let's call it "qualityControlRawSequences"

## the installation of trim-galore should be all we need, this includes fastqc:

conda create -n qualityControlRawSequences -c bioconda trim-galore 
 
## install phyloflash

conda create -n communityComposition -c bioconda phyloflash

## we need a formatted silva database for phyloFlash
cd /vol/funmic/databases

## looks like they haven't updated from last year:
wget https://zenodo.org/record/7892522/files/138.1.tar.gz
mv 138.1/ phyloflashSilvaDB/


tar -xzf 138.1.tar.gz

### megahit ###

conda create -n assembly -c bioconda megahit

### quast ###

conda create -n assemblyQC -c bioconda quast

### minimap ### 

## the repo for minimap (https://github.com/lh3/minimap2?tab=readme-ov-file#install)
## says just download binaries. so:

mkdir /vol/funmic/.minimap2

cd /vol/funmic/.minimap2

curl -L https://github.com/lh3/minimap2/releases/download/v2.28/minimap2-2.28_x64-linux.tar.bz2 | tar -jxvf -

## link to somewhere we can find it
sudo ln -s /vol/funmic/.minimap2/minimap2 /usr/local/bin/minimap2

### bbmap ###

## jgi also recommends a direct download 

## try:

cd /vol/funmic

wget https://altushost-swe.dl.sourceforge.net/project/bbmap/BBMap_39.14.tar.gz

tar -xvzf BBMap_39.14.tar.gz

mv bbmap /vol/funmic/.bbmap/

## I guess we just add it to our path, add the following to our bashrc.
## not ideal, but...

export PATH="/vol/funmic/.bbmap/:$PATH"
## added to bashrc file, seems to work

## needs java
sudo apt install default-jre

##### binning #####

## what are the chances the binning software will all play nice with each other?

## vamb behaving weird. never works anyway, leave it out this year, try maxbin2 again

conda create -n binning -c bioconda maxbin2 concoct

#conda remove -n binning --all ## keep needing this

## this was useful when maxbin doesn't completely install:
## conda update maxbin2

## no errors thrown, but can't believe that actually worked...
## this automatically installed samtools as a dependency, newest version.

## and doesn't work. There is a bug in metabat2 concerning 
## the abundance calculation program "jgi_summarize_bam_contig_depths"
## as per: https://bitbucket.org/berkeleylab/metabat/issues/172/jgi_summarize_bam_contig_depths-producing
## also several other bugs concerning negative abundances. 
## the author recommends against using bioconda/conda

## the author suggest docker or compiling from source. I should learn to use docker, but not now.
## https://bitbucket.org/berkeleylab/metabat/src/master/

## try from source. Dependencies:

## 1 boost
apt search libboost-all-dev
sudo apt install libboost-all-dev

## 2 cmake
sudo apt install cmake

## 3 g++
sudo apt install g++

cd /vol/funmic

## try the development version:
wget https://bitbucket.org/berkeleylab/metabat/get/master.tar.gz

tar xzvf master.tar.gz

cd berkeleylab-metabat-*

mkdir build && cd build && cmake .. [ -DCMAKE_INSTALL_PREFIX=/vol/funmic/ ] && make && make test && make install

## seems to have worked, hide it and put in on the path:

mv /vol/funmic/bin /vol/funmic/.metabat

## do we still need this?
rm -r berkeleylab-metabat-453915fb5bbc/

## added to path in bashrc

##### refining of bins #####

### das_tool ###

## as per: https://github.com/cmks/DAS_Tool

conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge
conda create -n refine -c bioconda das_tool

##### depreplication #####

## we need derep for the multiple metagenomes that dimitri has

conda create -n dereplication -c bioconda  drep

##### assign taxonomy to bins #####

## gtdb-tk, as per: https://ecogenomics.github.io/GTDBTk/installing/bioconda.html

conda create -n assignTaxonomy -c conda-forge -c bioconda gtdbtk=2.4.0

conda remove -n assignTaxonomy --all

conda activate assignTaxonomy 

## looks ok, we also need the database:

cd /vol/funmic/databases/gtdb

nohup wget https://data.ace.uq.edu.au/public/gtdb/data/releases/latest/auxillary_files/gtdbtk_package/full_package/gtdbtk_data.tar.gz &> gtdb-download.txt &

tar xvzf gtdbtk_data.tar.gz

## I think we need to tell gtbk-tk this path, I guess they have a script for this?i
## can't find it, try manually:

export GTDBTK_DATA_PATH=/vol/funmic/databases/gtdb/release220/

## but this is more sustainable:

conda env config vars set GTDBTK_DATA_PATH="/vol/funmic/databases/gtdb/release220/"

## check it with their script:

gtdbtk check_install ## this takes a long time, they check all genomes

gtdbtk check_install --db_version

#### metabarcode software ####

### bioconductor ###

## in R, as sudo:

install.packages("BiocManager", repos = "https://cloud.r-project.org")

### dada2 ###

## actually, looks like there is a newer version of dada2 than the documentation says:

BiocManager::install("dada2", version = "3.20")

## of course this failed. Always does.
## debugging dada2 install:

sudo apt install libcurl4-openssl-dev

install.packages("curl")

sudo apt install libssl-dev

install.packages('openssl')

## /usr/bin/ld: cannot find -llapack: No such file or directory

ld -llapack --verbose

sudo apt install libblas-dev liblapack-dev

install.packages('RcppEigen')

sudo apt install libpng-dev

install.packages("png")

sudo apt install libjpeg-dev

install.packages("jpeg")

sudo apt install liblzma-dev
sudo apt install libbz2-dev

BiocManager::install('Rhtslib')

## this fixed the dada2 install, as far as I can tell.

## also need to get the dada2 formatted version of silva:

cd /vol/funmic/databases/metabarcodingSilvaDB

wget https://zenodo.org/records/14169026/files/silva_nr99_v138.2_toGenus_trainset.fa.gz

### vegan ###

install.packages('vegan')

### phyloseq ###

BiocManager::install("phyloseq")

### microtraits  ###

library(BiocManager)

devtools::install_github("jlw-ecoevo/gRodon") ## fails, something about API rates...weird

## try it without dependencies.

devtools::install_github("jlw-ecoevo/gRodon", dependencies = FALSE) ## fails, same prob

## try getting a local copy of the repo:

git clone https://github.com/jlw-ecoevo/gRodon.git

## or get zip:
wget https://github.com/jlw-ecoevo/gRodon2/archive/refs/heads/master.zip

## it's here /vol/funmic/.gRodon

#devtools::install_github("jlw-ecoevo/gRodon", dependencies = FALSE) ## fails, same prob
devtools::install_local("/vol/funmic/.gRodon/master.zip") ## this works.

## now microtraits 

devtools::install_github("ukaraoz/microtrait") ## fails as above

mkdir /vol/funmic/.microtraits

cd /vol/funmic/.microtraits

wget https://github.com/ukaraoz/microtrait/archive/refs/heads/master.zip

## again in R:

## needs:
install.packages("seqinr")

BiocManager::install("Biostrings")

install.packages("seqinr")

devtools::install_local("/vol/funmic/.microtraits/master.zip") ## this works

######### software tests ############

## here is all the "trash" scripting of the process of me testing
## different installs.

### trim-galore: ###

conda activate qualityControlRawSequences 

trim_galore  -help

## let's check it with our new data:

outDir=~/test
file="/vol/funmic/datasets/Barcode2024/raw_reads/M13-07.fastq.gz"
fastqc -t 10 \
  -o $outDir \
  $file 

getFile=/home/ubuntu/test
putDir=/home/daniel/Documents/teaching/funmic/scratchpad
scp -i /home/daniel/.ssh -P 30423 -r ubuntu@129.70.51.6:$getFile $putDir

## fastqc works

## does the cut-adapt side work?
sipRawReads="/vol/funmic/datasets/Barcode2024/raw_reads"
output=/home/ubuntu/test
## run it
trim_galore \
  --cores 7 \
  -o $output \
  --clip_R1 20 \
  --illumina \
  --length 200 \
  ${sipRawReads}/*fastq.gz

## looks good. 

### phyloflash: ###

conda activate communityComposition

phyloFlash.pl -check_env

phyloFlash.pl -dbhome /vol/funmic/databases/phyloflashSilvaDB -lib TEST -CPUs 14 \
 -read1 ${CONDA_PREFIX}/lib/phyloFlash/test_files/test_F.fq.gz \
 -read2 ${CONDA_PREFIX}/lib/phyloFlash/test_files/test_R.fq.gz \
 -almosteverything

## seems to work

### megahit ###

## test megahit on our metagenomes from the kelp:

conda activate assembly

cd /vol/funmic/test

megahit -h

## try with the mock community:

fast1=/vol/funmic/datasets/zymogenMock/ERR7255689_1.fastq
fast2=/vol/funmic/datasets/zymogenMock/ERR7255689_2.fastq
rm -r $outdir
outdir=/vol/funmic/test
\time -v megahit -1 $fast1 \
          -2 $fast2 \
          -t 12 \
          -o $outdir

## this took 3569928 kbytes of RAM, or 3.5 gig, and 2 hours human time
## something funny if I try stdout and stderr redirection, but
## resolve this later. for now install looks good. 

## we can make a script for the kelp assemblies:

kelpDir="/vol/funmic/datasets/kelpBiofilm/"
fast1_1=${kelpDir}ERR3801502_1.fastq
fast1_2=${kelpDir}ERR3801502_2.fastq
fast2_1=${kelpDir}ERR3801542_1.fastq
fast2_2=${kelpDir}ERR3801542_2.fastq
fast3_1=${kelpDir}ERR3801603_1.fastq
fast3_2=${kelpDir}ERR3801603_2.fastq

megahit -1 $fast1_1 \
 -2 $fast1_2 \
 -t 12 \
 -o /vol/funmic/test/kelpAssemblies/kelp1/

megahit -1 $fast2_1 \
 -2 $fast2_2 \
 -t 12 \
 -o /vol/funmic/test/kelpAssemblies/kelp2/

megahit -1 $fast3_1 \
 -2 $fast3_2 \
 -t 12 \
 -o /vol/funmic/test/kelpAssemblies/kelp3/

## saved in assembleKelp.sh

cd /vol/funmic/test/kelpAssemblies

conda activate assembly

nohup bash assembleKelp.sh &



### quast ###

conda activate assemblyQC 

cd /vol/funmic/test

mkdir /vol/funmic/test/assemblyQC 

assembly=/vol/funmic/test/final.contigs.fa
quastOut=/vol/funmic/test/assemblyQC

## run the command
\time quast -t 12 \
  -o $quastOut \
  $assembly &> quastLog.txt &

less quastLog.txt
## trivial amount of time and memory used.
getFile=/vol/funmic/test/assemblyQC/
putDir=/home/daniel/Documents/teaching/funmic/scratchpad/
scp -i /home/daniel/.ssh -P 30423 -r ubuntu@129.70.51.6:$getFile $putDir

## quast works

#### binning ####

## mapping first, with bbmap:

mkdir -p /vol/funmic/test/binning/coverage
cd /vol/funmic/test/binning/coverage


############################################################

## try with the mock community
fast1=/vol/funmic/datasets/zymogenMock/ERR7255689_1.fastq
fast2=/vol/funmic/datasets/zymogenMock/ERR7255689_2.fastq
assembly=/vol/funmic/test/final.contigs.fa
\time -v bbmap.sh \
   threads=14 \
   minid=.97 \
   idfilter=.95 \
   ref=$assembly \
   in=$fast1 \
   in2=$fast2 \
   outm=rawReads_to_assembly.sam \
   bamscript=rawReads_to_assembly_to_bam.sh

## with first kelp files that took 14 minutes, 35 gig of ram
## with mock comm files that took 13 min, 33 gig of ram 

## map reads back to contigs

conda activate binning

\time -v bash rawReads_to_assembly_to_bam.sh ## for mock community, ~20 gig ram, 3 min

## coverage table

cd /vol/funmic/test/binning/coverage

assembly=/vol/funmic/test/final.contigs.fa
sortedBAM=/vol/funmic/test/binning/coverage/rawReads_to_assembly_sorted.bam

jgi_summarize_bam_contig_depths --help

\time -v jgi_summarize_bam_contig_depths \
           --outputDepth coverage_Depths.txt \
           --referenceFasta  $assembly \
           $sortedBAM
## no RAM used, really, <1 min

## repeat for the three kelp metagenomes ##

## kelp1:
#mkdir -p /vol/funmic/test/kelpbinning/kelp1
cd /vol/funmic/test/kelpbinning/kelp1

fast1=/vol/funmic/datasets/kelpBiofilm/ERR3801502_1.fastq
fast2=/vol/funmic/datasets/kelpBiofilm/ERR3801502_2.fastq
assembly=/vol/funmic/test/kelpAssemblies/kelp1/final.contigs.fa
\time -v bbmap.sh \
   threads=14 \
   minid=.97 \
   idfilter=.95 \
   ref=$assembly \
   in=$fast1 \
   in2=$fast2 \
   outm=rawReads_to_assembly.sam \
   bamscript=rawReads_to_assembly_to_bam.sh


conda activate binning

\time -v bash rawReads_to_assembly_to_bam.sh ## 


mkdir /vol/funmic/test/kelpbinning/kelp1/coverage

cd /vol/funmic/test/kelpbinning/kelp1/coverage

assembly=/vol/funmic/test/kelpAssemblies/kelp1/final.contigs.fa
sortedBAM=/vol/funmic/test/kelpbinning/kelp1/rawReads_to_assembly_sorted.bam
\time -v jgi_summarize_bam_contig_depths \
           --outputDepth coverage_Depths.txt \
           --referenceFasta  $assembly \
           $sortedBAM
## kelp2:

#mkdir -p /vol/funmic/test/kelpbinning/kelp2
cd /vol/funmic/test/kelpbinning/kelp2

fast1=/vol/funmic/datasets/kelpBiofilm/ERR3801542_1.fastq
fast2=/vol/funmic/datasets/kelpBiofilm/ERR3801542_2.fastq
assembly=/vol/funmic/test/kelpAssemblies/kelp2/final.contigs.fa
nohup bbmap.sh \
   threads=14 \
   minid=.97 \
   idfilter=.95 \
   ref=$assembly \
   in=$fast1 \
   in2=$fast2 \
   outm=rawReads_to_assembly.sam \
   bamscript=rawReads_to_assembly_to_bam.sh &


conda activate binning ## need samtools

bash rawReads_to_assembly_to_bam.sh 

#mkdir /vol/funmic/test/kelpbinning/kelp2/coverage
cd /vol/funmic/test/kelpbinning/kelp2/coverage

assembly=/vol/funmic/test/kelpAssemblies/kelp2/final.contigs.fa
sortedBAM=/vol/funmic/test/kelpbinning/kelp2/rawReads_to_assembly_sorted.bam

jgi_summarize_bam_contig_depths \
  --outputDepth coverage_Depths.txt \
  --referenceFasta  $assembly \
  $sortedBAM


## kelp3:

mkdir -p /vol/funmic/test/kelpbinning/kelp3

cd /vol/funmic/test/kelpbinning/kelp3

fast1=/vol/funmic/datasets/kelpBiofilm/ERR3801603_1.fastq
fast2=/vol/funmic/datasets/kelpBiofilm/ERR3801603_2.fastq
assembly=/vol/funmic/test/kelpAssemblies/kelp3/final.contigs.fa

nohup bbmap.sh \
   threads=14 \
   minid=.97 \
   idfilter=.95 \
   ref=$assembly \
   in=$fast1 \
   in2=$fast2 \
   outm=rawReads_to_assembly.sam \
   bamscript=rawReads_to_assembly_to_bam.sh &

conda activate binning ## need samtools

bash rawReads_to_assembly_to_bam.sh 

mkdir /vol/funmic/test/kelpbinning/kelp3/coverage

cd /vol/funmic/test/kelpbinning/kelp3/coverage

assembly=/vol/funmic/test/kelpAssemblies/kelp3/final.contigs.fa
sortedBAM=/vol/funmic/test/kelpbinning/kelp3/rawReads_to_assembly_sorted.bam

jgi_summarize_bam_contig_depths \
  --outputDepth coverage_Depths.txt \
  --referenceFasta  $assembly \
  $sortedBAM

### metabat ###

mkdir /vol/funmic/test/binning/metabat

cd /vol/funmic/test/binning/metabat

assembly=/vol/funmic/test/final.contigs.fa
coverageTable=/vol/funmic/test/binning/coverage/coverage_Depths.txt
\time -v  metabat2 \
            -i $assembly \
            -a $coverageTable \
            -o "metabat" \
            -t 12 

## some errors concerning negative values, check this out later
## errors gone now, with updated metabat2

## kelp metabat ##

## kelp1

mkdir /vol/funmic/test/kelpbinning/kelp1/metabat
cd /vol/funmic/test/kelpbinning/kelp1/metabat

assembly=/vol/funmic/test/kelpAssemblies/kelp1/final.contigs.fa
coverageTable=/vol/funmic/test/kelpbinning/kelp1/coverage/coverage_Depths.txt
\time -v  metabat2 \
            -i $assembly \
            -a $coverageTable \
            -o "metabat" \
            -t 12 


## kelp2
mkdir /vol/funmic/test/kelpbinning/kelp2/metabat
cd /vol/funmic/test/kelpbinning/kelp2/metabat
assembly=/vol/funmic/test/kelpAssemblies/kelp2/final.contigs.fa
coverageTable=/vol/funmic/test/kelpbinning/kelp2/coverage/coverage_Depths.txt
\time -v  metabat2 \
            -i $assembly \
            -a $coverageTable \
            -o "metabat" \
            -t 12 

## kelp3
mkdir /vol/funmic/test/kelpbinning/kelp3/metabat
cd /vol/funmic/test/kelpbinning/kelp3/metabat
assembly=/vol/funmic/test/kelpAssemblies/kelp3/final.contigs.fa
coverageTable=/vol/funmic/test/kelpbinning/kelp3/coverage/coverage_Depths.txt
\time -v  metabat2 \
            -i $assembly \
            -a $coverageTable \
            -o "metabat" \
            -t 12 

### maxbin2 ###

## try it, if it works, use this instead of vamb:

mkdir -p /vol/funmic/test/binning/maxbin/maxbinOut

cd /vol/funmic/test/binning/maxbin

cut -f1,3 /vol/funmic/test/binning/coverage/coverage_Depths.txt > abundance.list

assembly=/vol/funmic/test/final.contigs.fa
\time -v run_MaxBin.pl -thread 12 -min_contig_length 1500 \
              -contig $assembly \
              -abund abundance.list \
              -out  /vol/funmic/test/binning/maxbin/maxbinOut/maxbin

## 1 min, 0.5 gig RAM

## kelp1

mkdir -p /vol/funmic/test/kelpbinning/kelp1/maxbin/maxbinOut
cd /vol/funmic/test/kelpbinning/kelp1/maxbin

cut -f1,3 /vol/funmic/test/kelpbinning/kelp1/coverage/coverage_Depths.txt > abundance.list

assembly=/vol/funmic/test/kelpAssemblies/kelp1/final.contigs.fa
run_MaxBin.pl -thread 12 -min_contig_length 1500 \
     -contig $assembly \
     -abund abundance.list \
     -out /vol/funmic/test/kelpbinning/kelp1/maxbin/maxbinOut


## kelp2
mkdir -p /vol/funmic/test/kelpbinning/kelp2/maxbin/maxbinOut
cd /vol/funmic/test/kelpbinning/kelp2/maxbin
cut -f1,3 /vol/funmic/test/kelpbinning/kelp2/coverage/coverage_Depths.txt > abundance.list
assembly=/vol/funmic/test/kelpAssemblies/kelp2/final.contigs.fa
run_MaxBin.pl -thread 12 -min_contig_length 1500 \
     -contig $assembly \
     -abund abundance.list \
     -out /vol/funmic/test/kelpbinning/kelp2/maxbin/maxbinOut

## kelp3
mkdir -p /vol/funmic/test/kelpbinning/kelp3/maxbin/maxbinOut
cd /vol/funmic/test/kelpbinning/kelp3/maxbin
cut -f1,3 /vol/funmic/test/kelpbinning/kelp3/coverage/coverage_Depths.txt > abundance.list
assembly=/vol/funmic/test/kelpAssemblies/kelp3/final.contigs.fa
run_MaxBin.pl -thread 12 -min_contig_length 1500 \
     -contig $assembly \
     -abund abundance.list \
     -out /vol/funmic/test/kelpbinning/kelp3/maxbin/maxbinOut

## for some reason, outputs aren't going to maxbinout. Oh well, moving them there.

### concoct ###

conda activate binning

mkdir -p /vol/funmic/test/binning/concoct

cd /vol/funmic/test/binning/concoct

## define our variables
assembly=/vol/funmic/test/final.contigs.fa
sortedBAM=/vol/funmic/test/binning/coverage/rawReads_to_assembly_sorted.bam
outdir=/vol/funmic/test/binning/concoct

## run the software
## concoct has several steps, each with it's own script:

## samtools needs us to index the sorted read alignments:

#samtools index -@ 12 $readAlignments ## think this is already indexed

# cut_up_fasta.py $assembly -c 10000 -o 0 --merge_last -b concoctContigs_10K.bed > concoctContigs_10K.fa
# concoct_coverage_table.py concoctContigs_10K.bed $sortedBAM > coverage_table.tsv 

## still not working. Try an install of concoct with older python, like last year.

## can we skip this step by reformatting our old coverage table?:

cut -f 1,4 $coverageTable | sed '1d' | sed '1i contig_id\tsample_1' > concoct_coverage.txt

assembly=/vol/funmic/test/final.contigs.fa
\time -v concoct \
            --composition_file $assembly \
            --coverage_file concoct_coverage.txt \
            -t 12

mkdir fasta_bins

extract_fasta_bins.py $assembly clustering_gt1000.csv --output_path fasta_bins/

## looks promising, but we want pretty file names
## time for more BASH magic:

cd fasta_bins
for i in *; do
  mv $i ${i/\.fa/_concoct\.fa}
done

## kelp1 concoct:

mkdir -p /vol/funmic/test/kelpbinning/kelp1/concoct/fasta_bins

cd /vol/funmic/test/kelpbinning/kelp1/concoct/

assembly=/vol/funmic/test/kelpAssemblies/kelp1/final.contigs.fa
sortedBAM=/vol/funmic/test/kelpbinning/kelp1/rawReads_to_assembly_sorted.bam
outdir=/vol/funmic/test/kelpbinning/kelp1/concoct/
coverageTable=/vol/funmic/test/kelpbinning/kelp1/coverage/coverage_Depths.txt
cut -f 1,4 $coverageTable | sed '1d' | sed '1i contig_id\tsample_1' > concoct_coverage.txt
concoct \
   --composition_file $assembly \
   --coverage_file concoct_coverage.txt \
   -t 12

## that actually took some time:

extract_fasta_bins.py $assembly clustering_gt1000.csv --output_path fasta_bins/

cd /vol/funmic/test/kelpbinning/kelp1/concoct/fasta_bins
for i in *; do
  mv $i ${i/\.fa/_concoct\.fa}
done

## kelp2 concoct:

## that took some time. let's make a script:

### kelp2concoct.sh ###################
mkdir -p /vol/funmic/test/kelpbinning/kelp2/concoct/fasta_bins
cd /vol/funmic/test/kelpbinning/kelp2/concoct/
assembly=/vol/funmic/test/kelpAssemblies/kelp2/final.contigs.fa
sortedBAM=/vol/funmic/test/kelpbinning/kelp2/rawReads_to_assembly_sorted.bam
outdir=/vol/funmic/test/kelpbinning/kelp2/concoct/
coverageTable=/vol/funmic/test/kelpbinning/kelp2/coverage/coverage_Depths.txt
cut -f 1,4 $coverageTable | sed '1d' | sed '1i contig_id\tsample_1' > concoct_coverage.txt
concoct \
   --composition_file $assembly \
   --coverage_file concoct_coverage.txt \
   -t 6
extract_fasta_bins.py $assembly clustering_gt1000.csv --output_path fasta_bins/
cd /vol/funmic/test/kelpbinning/kelp2/concoct/fasta_bins
for i in *; do
  mv $i ${i/\.fa/_concoct\.fa}
done
###########################################

### kelp3concoct.sh ###################
mkdir -p /vol/funmic/test/kelpbinning/kelp3/concoct/fasta_bins
cd /vol/funmic/test/kelpbinning/kelp3/concoct/
assembly=/vol/funmic/test/kelpAssemblies/kelp3/final.contigs.fa
sortedBAM=/vol/funmic/test/kelpbinning/kelp3/rawReads_to_assembly_sorted.bam
outdir=/vol/funmic/test/kelpbinning/kelp3/concoct/

coverageTable=/vol/funmic/test/kelpbinning/kelp3/coverage/coverage_Depths.txt

cut -f 1,4 $coverageTable | sed '1d' | sed '1i contig_id\tsample_1' > concoct_coverage.txt
concoct \
   --composition_file $assembly \
   --coverage_file concoct_coverage.txt \
   -t 6
extract_fasta_bins.py $assembly clustering_gt1000.csv --output_path fasta_bins/
cd /vol/funmic/test/kelpbinning/kelp3/concoct/fasta_bins
for i in *; do
  mv $i ${i/\.fa/_concoct\.fa}
done
###########################################

nohup bash ./kelp2concoct.sh &
nohup bash ./kelp3concoct.sh &> kelpsconcoct.log &

#### refining ####

### test das_tool ###

conda activate refine

mkdir /vol/funmic/test/refine

cd /vol/funmic/test/refine

metabatBins=/vol/funmic/test/binning/metabat
concoctBins=/vol/funmic/test/binning/concoct/fasta_bins
maxbinBins=/vol/funmic/test/binning/maxbin/maxbinOut
assembly=/vol/funmic/test/final.contigs.fa

## assign contigs to bins

## metabat bins
Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $metabatBins \
    > metabat.contigs2bin.tsv

paste <(cut  -f 1 metabat.contigs2bin.tsv) <(cut -f 4 metabat.contigs2bin.tsv) > metabat.contigs2bin_edited.tsv

head metabat*

## concoct:
Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $concoctBins \
    > concoct.contigs2bin.tsv

paste <(cut -d " " -f 1 concoct.contigs2bin.tsv) <(cut -f 2 concoct.contigs2bin.tsv) > concoct.contigs2bin_edited.tsv

head concoct*

## maxbin:
Fasta_to_Contig2Bin.sh \
    -e fasta \
    -i $maxbinBins \
    > maxbin.contigs2bin.tsv

head maxbin.contigs2bin.tsv
## don't need to edit this table, looks good

DAS_Tool \
    -i metabat.contigs2bin_edited.tsv,concoct.contigs2bin_edited.tsv,maxbin.contigs2bin.tsv \
    -l metabat,concoct,maxbin \
    --score_threshold 0.25 \
    -c $assembly \
    -t 12 \
    --write_bins \
    -o zymoMC_das

## works. now try with the kelp files

## dastool kelp1

conda activate refine

mkdir -p /vol/funmic/test/kelprefine/kelp1

cd /vol/funmic/test/kelprefine/kelp1

metabatBins=/vol/funmic/test/kelpbinning/kelp1/metabat
concoctBins=/vol/funmic/test/kelpbinning/kelp1/concoct/fasta_bins
maxbinBins=/vol/funmic/test/kelpbinning/kelp1/maxbin/maxbinOut
assembly=/vol/funmic/test/kelpAssemblies/kelp1/final.contigs.fa

## metabat bins
Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $metabatBins \
    > metabat.contigs2bin.tsv

paste <(cut  -f 1 metabat.contigs2bin.tsv) <(cut -f 4 metabat.contigs2bin.tsv) > metabat.contigs2bin_edited.tsv

## concoct:
Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $concoctBins \
    > concoct.contigs2bin.tsv

paste <(cut -d " " -f 1 concoct.contigs2bin.tsv) <(cut -f 2 concoct.contigs2bin.tsv) > concoct.contigs2bin_edited.tsv

## maxbin:
Fasta_to_Contig2Bin.sh \
    -e fasta \
    -i $maxbinBins \
    > maxbin.contigs2bin.tsv

DAS_Tool \
    -i metabat.contigs2bin_edited.tsv,concoct.contigs2bin_edited.tsv,maxbin.contigs2bin.tsv \
    -l metabat,concoct,maxbin \
    --score_threshold 0.25 \
    -c $assembly \
    -t 14 \
    --write_bins \
    -o zymoMC_das

## kelp2
mkdir -p /vol/funmic/test/kelprefine/kelp2
cd /vol/funmic/test/kelprefine/kelp2
metabatBins=/vol/funmic/test/kelpbinning/kelp2/metabat
concoctBins=/vol/funmic/test/kelpbinning/kelp2/concoct/fasta_bins
maxbinBins=/vol/funmic/test/kelpbinning/kelp2/maxbin/maxbinOut
assembly=/vol/funmic/test/kelpAssemblies/kelp2/final.contigs.fa
## metabat bins
Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $metabatBins \
    > metabat.contigs2bin.tsv

paste <(cut  -f 1 metabat.contigs2bin.tsv) <(cut -f 4 metabat.contigs2bin.tsv) > metabat.contigs2bin_edited.tsv
## concoct:
Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $concoctBins \
    > concoct.contigs2bin.tsv

paste <(cut -d " " -f 1 concoct.contigs2bin.tsv) <(cut -f 2 concoct.contigs2bin.tsv) > concoct.contigs2bin_edited.tsv
## maxbin:
Fasta_to_Contig2Bin.sh \
    -e fasta \
    -i $maxbinBins \
    > maxbin.contigs2bin.tsv

DAS_Tool \
    -i metabat.contigs2bin_edited.tsv,concoct.contigs2bin_edited.tsv,maxbin.contigs2bin.tsv \
    -l metabat,concoct,maxbin \
    --score_threshold 0.25 \
    -c $assembly \
    -t 14 \
    --write_bins \
    -o zymoMC_das

## kelp3
mkdir -p /vol/funmic/test/kelprefine/kelp3
cd /vol/funmic/test/kelprefine/kelp3
metabatBins=/vol/funmic/test/kelpbinning/kelp3/metabat
concoctBins=/vol/funmic/test/kelpbinning/kelp3/concoct/fasta_bins
maxbinBins=/vol/funmic/test/kelpbinning/kelp3/maxbin/maxbinOut
assembly=/vol/funmic/test/kelpAssemblies/kelp3/final.contigs.fa
## metabat bins
Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $metabatBins \
    > metabat.contigs2bin.tsv
paste <(cut  -f 1 metabat.contigs2bin.tsv) <(cut -f 4 metabat.contigs2bin.tsv) > metabat.contigs2bin_edited.tsv
## concoct:
Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $concoctBins \
    > concoct.contigs2bin.tsv
paste <(cut -d " " -f 1 concoct.contigs2bin.tsv) <(cut -f 2 concoct.contigs2bin.tsv) > concoct.contigs2bin_edited.tsv
## maxbin:
Fasta_to_Contig2Bin.sh \
    -e fasta \
    -i $maxbinBins \
    > maxbin.contigs2bin.tsv
DAS_Tool \
    -i metabat.contigs2bin_edited.tsv,concoct.contigs2bin_edited.tsv,maxbin.contigs2bin.tsv \
    -l metabat,concoct,maxbin \
    --score_threshold 0.25 \
    -c $assembly \
    -t 14 \
    --write_bins \
    -o zymoMC_das

## should have changed the output directory. But anyway, looks like it worked. 

###### dRep ######

conda activate dereplication

## for these, we need to give unique names to each kelp metagenome

## names of directories are screwed up, because reused old code. But here they are:

cd /vol/funmic/test/kelprefine/kelp1/zymoMC_das_DASTool_bins
ls -1 | wc -l ## 23

for i in *; do
  mv $i "kelp1_"${i}
done

cd /vol/funmic/test/kelprefine/kelp2/zymoMC_das_DASTool_bins
ls -1 | wc -l ## 28

for i in *; do
  mv $i "kelp2_"${i}
done

cd /vol/funmic/test/kelprefine/kelp3/zymoMC_das_DASTool_bins
ls -1 | wc -l ## 29

for i in *; do
  mv $i "kelp3_"${i}
done

## get them all into one directory:

mkdir -p /vol/funmic/test/kelpDrep/all_bins
mkdir /vol/funmic/test/kelpDrep/dereplicated_bins

cp /vol/funmic/test/kelprefine/kelp?/zymoMC_das_DASTool_bins/* /vol/funmic/test/kelpDrep/all_bins

ls /vol/funmic/test/kelpDrep/all_bins -1 | wc -l ## all there

cd /vol/funmic/test/kelpDrep

## try dRep
ALL_BINS_DIRECTORY=/vol/funmic/test/kelpDrep/all_bins
DEREP_BINS_DIRECTORY=/vol/funmic/test/kelpDrep/dereplicated_bins
\time -v dRep dereplicate \
      ${DEREP_BINS_DIRECTORY} \
      -p 12 \
      -g ${ALL_BINS_DIRECTORY}/*.fa \
      -comp 40 \
      -l 500000 \
      -comW 1 \
      -sizeW 1 \
      -conW 5 \
      -strW 1 \
      -N50W 0.5 \
      --run_tertiary_clustering 

## this runs for a ~long time, 24 min, 36 gig

## great, now what? 

## looks like we get checkm scores of the final contigs
## with "dereplicated_bins/data_tables/Widb.csv"

## now onto taxonomy...

##### gtdb-tk ########

conda activate assignTaxonomy

cd /vol/funmic/test/kelpAssignTaxonomy 

mags=/vol/funmic/test/kelpDrep/dereplicated_bins/dereplicated_genomes


## do the gene predictions, and find the marker genes present in each mag

\time -v gtdbtk identify --genome_dir $mags --out_dir ./identify --extension "fa" --cpus 12 &> gtdb-identify.log &
## less than a gig, 2 minutes

## run multiple alignments of these markers, figure out which domain of life
## we are dealing with.
\time -v gtdbtk align --identify_dir ./identify --out_dir ./align --cpus 12 &> gtdb-align.log &
## 9 gig ram, 6 minutes

\time -v gtdbtk classify --genome_dir $mags \
                         --align_dir ./align \
                         --out_dir ./classify -x "fa" \
                         --out_dir ./classify -x "fa" \
                         --mash_db ./mashDB &> gtdb-classify.log &

## that took 80 gig RAM (expected), 5 hours (!!)

## did it work?

cd ./kelpAssignTaxonomy/classify/

less gtdbtk.bac120.summary.tsv

## this seems to have worked. 

## now deal with the new metabarcode pipeline and see what Dimitri is thinking for annotation.

###############################################

### metabarcoding ###

cd /vol/funmic/test/metabarcoding


conda activate qualityControlRawSequences 

## start with a fastqc review:

## data is here:

metabarcodeD=/vol/funmic/datasets/Barcode2024/raw_reads/

## combine into one fasta


cat ${metabarcodeD}*.gz > allfastqs.fastq.gz

## look at it:

fastqc -o . -t 12 allfastqs.fastq.gz

## get it local:

getFile=/vol/funmic/test/metabarcoding/allfastqs_fastqc.html
putDir=/home/daniel/Documents/teaching/funmic/scratchpad/
scp -i /home/daniel/.ssh -P 30423 -r ubuntu@129.70.51.6:$getFile $putDir

## we'll need to cut the primers.

## use cutadapt. What are our primers again?  

## if they are regular 16sV4 with illumina tags:

gunzip  allfastqs.fastq.gz

## here are the phased versions of the 515f:
tcgtcggcagcgtcagatgtgtataagagacag  gtgycagcmgccgcggtaa
tcgtcggcagcgtcagatgtgtataagagacag cgtgycagcmgccgcggtaa
tcgtcggcagcgtcagatgtgtataagagacagtcgtgycagcmgccgcggtaa

grep  GTG.CAGC.GCCGCGGTAA allfastqs.fastq

grep -c  GTG.CAGC.GCCGCGGTAA allfastqs.fastq ## 794,095 primers found

grep -c  @FS1 allfastqs.fastq ## 1,241,090 reads total

## anyway, how do we get rid of these with cutadapt? 


## test out cutadapt

cd /vol/funmic/test/metabarcoding

cp /vol/funmic/datasets/Barcode2024/raw_reads/A12-05.fastq.gz .

zgrep -c "@FS1"  A12-05.fastq.gz ## 17633

grep -c "@FS1"  allfastqs.fastq ## 1 241 090

grep -c "GTG.CAGC.GCCGCGGTAA" allfastqs.fastq  ## 794095

zgrep -c "GTG.CAGC.GCCGCGGTAA" A12-05.fastq.gz  ## 11156

cutadapt --rc -g TCGTGYCAGCMGCCGCGGTAA A12-05.fastq.gz  ## 15,827 trimmed

cutadapt --rc -g GTGYCAGCMGCCGCGGTAA A12-05.fastq.gz  ## 15,178 trimmed

cutadapt --rc -g TCGTGYCAGCMGCCGCGGTAA A12-05.fastq.gz 

cutadapt -g TCGTGYCAGCMGCCGCGGTAA -o test.fastq allfastqs.fastq | tee test.log

grep GTG.CAGC.GCCGCGGTAA test.fastq ## one slipped through:

cutadapt -g XTCGTGYCAGCMGCCGCGGTAA -o test2.fastq allfastqs.fastq | tee test2.log

grep GTG.CAGC.GCCGCGGTAA test2.fastq ## two slipped through. 

cutadapt -g XTCGTGYCAGCMGCCGCGGTAA -o test2.fastq --rc allfastqs.fastq | tee test2.log

## reverse compliment is:

GGACTACNVGGGTWTCTAAT

grep -c GGACTAC..GGGT.TCTAAT allfastqs.fastq ## 32, not many. But there. 

## to remove these:
cutadapt -g GGACTACNVGGGTWTCTAAT -o test3.fastq --rc test2.fastq 

grep -c GGACTAC..GGGT.TCTAAT test3.fastq ## 

## so try some thing like this, for all files:

cd /vol/funmic/test/metabarcoding

cat ${metabarcodeD}*.gz > allfastqs.fastq.gz

conda activate qualityControlRawSequences 

mkdir /vol/funmic/test/metabarcoding/trimmed_reads

## check for reverse 806R sequences:
GGACTACNVGGGTWTCTAAT

## so 806 forward barcoded grep:

gunzip -c allRawReads.gz > allRawReads.fastq

grep GGACTAC..GGGT.TCTAAT allRawReads.fastq
grep -c GGACTAC..GGGT.TCTAAT allRawReads ## 32 reads, huh. 

## RC of this:
ATTAGA.ACCC..GTAGTCC
## 806 reverse barcoded:

gunzip -c A12-05.fastq.gz > A12-05.fastq.fastq

zgrep ATTAGA.ACCC..GTAGTCC allRawReads.gz

grep -c  ATTAGA.ACCC..GTAGTCC allRawReads.fastq ## they are in there, 622406 reads. ~1/2
grep -c  @FS1 allRawReads.fastq

grep -c  ATTAGA.ACCC..GTAGTCC A12-05.fastq.fastq ## 8930, ~1/2
grep -c  @FS1 A12-05.fastq.fastq

## so really we need something like this:
cutadapt --rc -g TCGTGYCAGCMGCCGCGGTAA -a GGACTACNVGGGTWTCTAAT A12-05.fastq.gz


zgrep allRawReads.gz

conda activate qualityControlRawSequences 

metabarcodeD=/vol/funmic/datasets/Barcode2024/raw_reads/
outDir=/vol/funmic/test/metabarcoding/trimmed_reads/
cd $metabarcodeD
for i in *.gz; do
  ls $i
  trimmedFASTQ=${outDir}${i/.fastq\.gz/_trimmed.fastq.gz }
  echo $trimmedFASTQ
  cutadapt --rc -g TCGTGYCAGCMGCCGCGGTAA -o ${trimmedFASTQ} $i | tee -a cutadapt.log
done

## too complicated for students. Maybe?:

## set up files, maybe do this for them.

cd /vol/funmic/datasets/Barcode2024/raw_reads/


## define some variables
inputFastq=A12-05.fastq.gz 
outputFastq=A12-05_trimmed.fastq.gz
outputDir=/vol/funmic/test/metabarcoding/trimmed_reads/

cutadapt \
  --rc \
  -g TCGTGYCAGCMGCCGCGGTAA \
  --cut 10 \
  -o $outputDir$outputFastq \
     $inputFastq  

ls $outputDir$inputFastq  

## then show them a loop:

cd /vol/funmic/datasets/Barcode2024/raw_reads/

for inputFastq in *; do
  outputFastq=${inputFastq/.fastq.gz/_trimmed.fastq.gz } ## BASH string magic
  cutadapt \
    --rc \
  --length 280 \
    -g TCGTGYCAGCMGCCGCGGTAA \
    -o $outputDir${outputFastq} \
       $inputFastq  
done

## check on these:

cd /vol/funmic/test/metabarcoding/trimmed_reads

cat *.fastq.gz > alltrimmedreads.fastq.gz

mv alltrimmedreads.fastq.gz alltrimmedreads280.fastq.gz

conda activate qualityControlRawSequences 

fastqc -o . -t 12 alltrimmedreads280.fastq.gz

#getFile=/vol/funmic/test/metabarcoding/trimmed_reads/alltrimmedreads_fastqc.html
getFile=/vol/funmic/test/metabarcoding/trimmed_reads/alltrimmedreads280_fastqc.html
putDir=/home/daniel/Documents/teaching/funmic/scratchpad/
scp -i /home/daniel/.ssh -P 30423 -r ubuntu@129.70.51.6:$getFile $putDir

## something like that. 

## if we had more time, would do more checks. But we have just an afternoon.

## and then?

### dada2 ###

## following the same old tutorial: http://benjjneb.github.io/dada2/tutorial.html

cd /vol/funmic/test/metabarcoding/

## trimmed reads are here:

R

setwd("/vol/funmic/test/metabarcoding/")

library(dada2); packageVersion("dada2")

path <- "/vol/funmic/test/metabarcoding/trimmed_reads" 

list.files(path)

fns <- sort(list.files(path, pattern=".fastq.gz", full.names = TRUE))

sample.names <- sapply(strsplit(basename(fns), "_"), '[', 1)

plotQualityProfile(fns[1])

# Place filtered files in filtered/ subdirectory

filterPath <- "/vol/funmic/test/metabarcoding/filtered"

filtFns <- file.path(filterPath, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))

names(filtFns) <- sample.names

out <- filterAndTrim(fns, filtFns,
              maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=TRUE) 

head(out)

## we could probably do the above code ourselves, give the filtered reads
## to the class, preprepared?

out

### computationally intensive dada2 steps ###
print ("started:"); Sys.time()
errF <- learnErrors(filtFns, multithread=TRUE) ## ran for ~15 min. memory no problem
plotErrors(errF, nominalQ=TRUE)
dadaF <- dada(filtFns, err=errF, pool=TRUE, multithread=TRUE)
seqtab <- makeSequenceTable(dadaF) ## 
saveRDS(dadaF, "dadaF.rds")
saveRDS(errF, "errF.rds")
saveRDS(seqtab, "seqtab.rds")


print ("finished:"); Sys.time()
### end computationally intensive dada2 steps ####
## that took 1 hour, 11 minutes
## jeezus. Try to get them there over lunch? 

dadaF[[1]] ## 1769 ASVs

### starting back, rerun the above code:
#dadaF <- readRDS("dadaF.rds")
#errF <- readRDS("errF.rds")
#seqtab <- readRDS("seqtab.rds")

dim(seqtab) ## 2614 columns. how can this have more columns than our number of asvs? 

table(nchar(getSequences(seqtab))) ## most are 273 bp long.

barplot(table(nchar(getSequences(seqtab)))) 

table(nchar(getSequences(seqtab))) |> barplot() 

seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE) ## 746 bimeras
saveRDS(seqtab.nochim, "seqtab.nochim.rds")

dim(seqtab.nochim)

table(nchar(getSequences(seqtab.nochim))) |> barplot() 

## this dropped us to 1868 columns. Still don't understand how I can have more columns than ASVs...

getN <- function(x) sum(getUniques(x))

track <- cbind(out, sapply(dadaF, getN), rowSums(seqtab.nochim))

colnames(track) <- c("input", "filtered", "denoised","nonchim")

rownames(track) <- sample.names

track

path2silvaDB <- "/vol/funmic/databases/metabarcodingSilvaDB/silva_nr99_v138.2_toGenus_trainset.fa.gz"
print ("started:"); Sys.time()
taxa <- assignTaxonomy(seqtab.nochim, path2silvaDB, multithread=TRUE) 
print ("finished:"); Sys.time() ## only took 3 minutes. Why do I remember that being more intensive?
saveRDS(taxa, "taxa.rds")

taxa.print <- taxa # Removing sequence rownames for display only
rownames(taxa.print) <- NULL

head(taxa.print)

taxa.print[2,]


## from here play with phyloseq
## rarify down
## stacked tax graphs
## ordinations 
## permanova by treatments of interest.
## PCoAs also.

##### phyloseq #####

cd /vol/funmic/test/metabarcoding

R

library(dada2)
library(phyloseq)
library(ggplot2)

### can restart here, but maybe better to reload ps below ####

#dadaF <- readRDS("dadaF.rds")
#errF <- readRDS("errF.rds")
#seqtab <- readRDS("seqtab.rds")
seqtab.nochim <- readRDS("seqtab.nochim.rds")
taxa <- readRDS("taxa.rds")

samples.out <- rownames(seqtab.nochim)
substrate <- sapply(samples.out, substring, 1,1)
isotope <- sapply(samples.out, substring, 2,3)
fraction <- sapply(samples.out, substring, 5,6)
samdf <- data.frame(Substrate=substrate, Isotope=isotope, Fraction=fraction)
rownames(samdf) <- samples.out

## check it: 

samdf

samdf[samdf$Substrate == "G",]

sort_by(samdf[samdf$Substrate == "A",], ~ Substrate + Fraction)

sort_by(samdf[samdf$Substrate == "G",], ~ Substrate + Fraction)

sort_by(samdf[samdf$Substrate == "M",], ~ Substrate + Fraction)

sort_by(samdf, ~ Substrate + Fraction)



ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               sample_data(samdf), 
               tax_table(taxa))

dna <- Biostrings::DNAStringSet(taxa_names(ps))

names(dna) <- taxa_names(ps)
ps <- merge_phyloseq(ps, dna)
taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))

barplot(sort(sample_sums(ps)), las=2) ## ranges from 10 000 to 70 000. Should probably rarify

## another way to do this...
plot_bar(ps, x="Sample", y="Abundance")

#save(ps, file="ps.rda")

#################### end of preprocessing for metabarcoding data ############

#### starting from here we are checking the SIP results ("ecological analysis")  #########

load("ps.rda")

plot_richness(ps, x="Substrate", measures=c("Observed", "Simpson"), color="Isotope")

## maybe do this after rarefaction

## quick nms check
ps.prop <- transform_sample_counts(ps, function(otu) otu/sum(otu))
ord.nmds.bray <- ordinate(ps.prop, method="NMDS", distance="bray")
aa <- plot_ordination(ps.prop, ord.nmds.bray, color="Substrate", shape="Isotope", title="Bray NMDS")
aa + geom_point(size=3.5) + geom_text(
                                  label=sample_data(ps.prop)$Fraction, 
                                  nudge_x = 0.05, nudge_y = 0.05, 
                                  check_overlap = T)



## which fraction should we be looking at?
## who knows. But for now, Tillmann wants to try a PCA approach. 

## if we ordinate by individual substrates?

## let's try subsetting by substrate:

samp.a <- rownames(sample_data(ps)[sample_data(ps)$Substrate == "A",])
samp.g <- rownames(sample_data(ps)[sample_data(ps)$Substrate == "G",])
samp.m <- rownames(sample_data(ps)[sample_data(ps)$Substrate == "M",])
ps.a <- prune_samples(samp.a, ps)
ps.g <- prune_samples(samp.g, ps)
ps.m <- prune_samples(samp.m, ps)


?transform_sample_counts

## proportional transform for PCoAs
ps.a.prop <- transform_sample_counts(ps.a, function(otu) otu/sum(otu))
ps.g.prop <- transform_sample_counts(ps.g, function(otu) otu/sum(otu))
ps.m.prop <- transform_sample_counts(ps.m, function(otu) otu/sum(otu))

## hellinger for PCAs:
ps.a.hell <- transform_sample_counts(ps.a, function(x) sqrt(x / sum(x)))
ps.g.hell <- transform_sample_counts(ps.g, function(x) sqrt(x / sum(x)))
ps.m.hell <- transform_sample_counts(ps.m, function(x) sqrt(x / sum(x)))

#ord.nmds.bray.a <- ordinate(ps.a.prop, method="NMDS", distance="bray")

## pcoa acetate
ord.pcoa.bray.a <- ordinate(ps.a.prop, method="PCoA", distance="bray")
plot.a.ord <- plot_ordination(ps.a.prop, ord.pcoa.bray.a, color="Isotope", shape="Isotope")
plot.a.ord + geom_point(size=3.5) + geom_text(
                                  label=sample_data(ps.a.prop)$Fraction, 
                                  nudge_x = 0.01, nudge_y = 0.01, 
                                  check_overlap = T) + ggtitle("Acetate Bray PCoA")

## pca acetate
ord.PCA.a <- ordinate(ps.a.hell, method="RDA")
plot.a.ord <- plot_ordination(ps.a.hell, ord.PCA.a, color="Isotope", shape="Isotope")
plot.a.ord + geom_point(size=3.5) + geom_text(
                                  label=sample_data(ps.a.hell)$Fraction, 
                                  nudge_x = 0.01, nudge_y = 0.01, 
                                  check_overlap = T) + ggtitle("Acetate PCA")

## pcoa glucose
ord.pcoa.bray.g <- ordinate(ps.g.prop, method="PCoA", distance="bray")
plot.g.ord <- plot_ordination(ps.g.prop, ord.pcoa.bray.g, color="Isotope", shape="Isotope")
plot.g.ord + geom_point(size=3.5) + geom_text(
                                  label=sample_data(ps.g.prop)$Fraction, 
                                  nudge_x = 0.01, nudge_y = 0.01, 
                                  check_overlap = T) + ggtitle("Glucose Bray PCoA")

## pca glucose
ord.PCA.g <- ordinate(ps.g.hell, method="RDA")
plot.g.ord <- plot_ordination(ps.g.hell, ord.PCA.g, color="Isotope", shape="Isotope")
plot.g.ord + geom_point(size=3.5) + geom_text(
                                  label=sample_data(ps.g.hell)$Fraction, 
                                  nudge_x = 0.01, nudge_y = 0.01, 
                                  check_overlap = T) + ggtitle("Glucose PCA")

## pcoa methonal
ord.pcoa.bray.m <- ordinate(ps.m.prop, method="PCoA", distance="bray")
plot.m.ord <- plot_ordination(ps.m.prop, ord.pcoa.bray.m, color="Isotope", shape="Isotope")
plot.m.ord + geom_point(size=3.5) + geom_text(
                                  label=sample_data(ps.m.prop)$Fraction, 
                                  nudge_x = 0.01, nudge_y = 0.01, 
                                  check_overlap = T) + ggtitle("Methanol Bray PCoA")

## pca methanol
ord.PCA.m <- ordinate(ps.m.hell, method="RDA")
plot.m.ord <- plot_ordination(ps.m.hell, ord.PCA.m, color="Isotope", shape="Isotope")
plot.m.ord + geom_point(size=3.5) + geom_text(
                                  label=sample_data(ps.m.hell)$Fraction, 
                                  nudge_x = 0.01, nudge_y = 0.01, 
                                  check_overlap = T) + ggtitle("Methanol PCA")

str(ord.PCA.m)

str(ord.PCA.m$CA)

head(ord.PCA.m$CA$v)

dim(ord.PCA.m$CA$v) ## yup, this is them, species loading scores.

class(ord.PCA.m$CA$v) 

str(dim(ord.PCA.m$CA$v))

## to find our most important?:

## PC1

ord.PCA.m[1]


dim(ord.PCA.m$CA$u) ## this is sites.

## looking here: https://github.com/joey711/phyloseq/issues/816
## and here: http://deneflab.github.io/MicrobeMiseq/demos/mothur_2_phyloseq.html#constrained_ordinations

## there are two scaling methods for species scores:

scalingType=1
scalingAmount=3
speciesMat <- vegan::scores(ord.PCA.m, display="species", scaling=scalingType)
labeldf <- data.frame(labels = rownames(speciesMat), speciesMat*scalingAmount)
label_map <- aes(x = PC1, 
    y = PC2, 
    shape = NULL, 
    color = NULL, 
    label = labels,
)
plot.m.ord + geom_point(size=3.5) + 
geom_text( label=sample_data(ps.m.hell)$Fraction, 
    nudge_x = 0.01, nudge_y = 0.01, 
    check_overlap = T) + ggtitle("Methanol PCA") + 
  geom_text(
    mapping = label_map, 
    size = 4,  
    data = labeldf, 
    show.legend = FALSE,
    check_overlap = T
  )

## make a function for this:

plotPCAWithSpecies <- function (ps, scalingType=1, scalingAmount=3, ptitle) {
    ps.hell <- transform_sample_counts(ps, function(x) sqrt(x / sum(x)))
    ord.PCA <- ordinate(ps.hell, method="RDA")
    plot.ord <- plot_ordination(ps.hell, ord.PCA, color="Isotope", shape="Isotope")
    speciesMat <- vegan::scores(ord.PCA, display="species", scaling=scalingType)
    labeldf <- data.frame(labels = rownames(speciesMat), speciesMat*scalingAmount)
    label_map <- aes(x = PC1, 
        y = PC2, 
        shape = NULL, 
        color = NULL, 
        label = labels,
    )
    plot.ord + geom_point(size=3.5) + 
    geom_text( label=sample_data(ps.hell)$Fraction, 
        nudge_x = 0.01, nudge_y = 0.01, 
        check_overlap = T) + ggtitle(ptitle) + 
      geom_text(
        mapping = label_map, 
        size = 4,  
        data = labeldf, 
        show.legend = FALSE,
        check_overlap = T
      )
}

dev.new()
plotPCAWithSpecies(ps=ps.m, ptitle="Methanol PCA")

dev.new()
plotPCAWithSpecies(ps=ps.a, ptitle="Acetate PCA")

dev.new()
plotPCAWithSpecies(ps=ps.g, ptitle="Glucose PCA")
## seems to work ok.


## some code for plotting relative abundances in each
top30 <- names(sort(taxa_sums(ps), decreasing=TRUE))[1:30]
ps.top30 <- transform_sample_counts(ps, function(OTU) OTU/sum(OTU))
ps.top30 <- prune_taxa(top30, ps.top30)
plot_bar(ps.top30, x="Fraction", fill="Family") + facet_wrap(~Substrate + Isotope, scales="free_x") 

## maybe a useful overview to give the students. consider another function in the sipFunction script?

## now, how to plot abundances of a given OTU as a function of fraction?

## for example, ASV2 appears important to the heavy fraction of Methanol

## find its taxonomy:

tax_table(ps.m)["ASV2"]

## to track it's abundance among the fraction?

head(otu_table(ps.m))[,"ASV2"]

## subset to an isotope:
samp.a.12 <- rownames(sample_data(ps.m)[sample_data(ps.m)$Isotope == "12",])
ps.m.12 <- prune_samples(samp.a.12, ps)
ps.m.12.ASV2 <- prune_taxa("ASV2", ps.m.12)
ps.m.12.ASV2
aa <- as.data.frame(cbind(sample_data(ps.m.12.ASV2)$Fraction, otu_table(ps.m.12.ASV2)))
aa[] <- sapply(aa, as.numeric)
colnames(aa) <- c("Fraction","Abundance")
aa$Isotope <- 12

plot(aa$Fraction, aa$Abundance)

## repeat for 13 isos
samp.a.13 <- rownames(sample_data(ps.m)[sample_data(ps.m)$Isotope == "13",])
ps.m.13 <- prune_samples(samp.a.13, ps)
ps.m.13.ASV2 <- prune_taxa("ASV2", ps.m.13)
ps.m.13.ASV2
bb <- as.data.frame(cbind(sample_data(ps.m.13.ASV2)$Fraction, otu_table(ps.m.13.ASV2)))
bb[] <- sapply(bb, as.numeric)
colnames(bb) <- c("Fraction","Abundance")
bb$Isotope <- 13

plot(bb$Fraction, bb$Abundance)

cc <- rbind(aa,bb)
cc$Isotope <- as.factor(cc$Isotope)

## how would we do this with ggplot?
ggplot(cc, aes(x=Fraction, y=Abundance, color=Isotope, shape=Isotope)) +
  geom_point(size=4) +
  geom_smooth(se=FALSE, fullrange=FALSE, linetype="dashed")

## great website here for ggplot
https://www.sthda.com/english/wiki/ggplot2-scatter-plots-quick-start-guide-r-software-and-data-visualization

## great, how can we generalize this?

getFractionAbundances = function(ASVname,whichPS,ptitle=""){
    samp.12 <- rownames(sample_data(whichPS)[sample_data(whichPS)$Isotope == "12",])
    ps.12 <- prune_samples(samp.12, ps)
    ps.12.ASV <- prune_taxa(ASVname, ps.12)
    df12 <- as.data.frame(cbind(sample_data(ps.12.ASV)$Fraction, otu_table(ps.12.ASV)))
    df12[] <- sapply(df12, as.numeric)
    colnames(df12) <- c("Fraction","Abundance")
    df12$Isotope <- 12
    ## repeat for 13 isos
    samp.13 <- rownames(sample_data(whichPS)[sample_data(whichPS)$Isotope == "13",])
    ps.13 <- prune_samples(samp.13, ps)
    ps.13.ASV <- prune_taxa(ASVname, ps.13)
    df13 <- as.data.frame(cbind(sample_data(ps.13.ASV)$Fraction, otu_table(ps.13.ASV)))
    df13[] <- sapply(df13, as.numeric)
    colnames(df13) <- c("Fraction","Abundance")
    df13$Isotope <- 13
    df12_13 <- rbind(df12,df13)
    df12_13$Isotope <- as.factor(df12_13$Isotope)
    ## how would we do this with ggplot?
    print(tax_table(ps.12.ASV))
    ggplot(df12_13, aes(x=Fraction, y=Abundance, color=Isotope, shape=Isotope)) +
      geom_point(size=4) +
      geom_smooth(se=FALSE, fullrange=FALSE, linetype="dashed") + ggtitle(ptitle) 
}

dev.new()
getFractionAbundances (ASVname="ASV28", whichPS=ps.m)

dev.new()
getFractionAbundances (ASVname="ASV28", whichPS=ps.a)

## checking out some ASVs from 
dev.new()

getFractionAbundances (ASVname="ASV20", whichPS=ps.m, ptitle="methanol, ASV20")

getFractionAbundances (ASVname="ASV13", whichPS=ps.m, ptitle="methanol, ASV13")

## this seems to work. save these functions in a script to load.

## cleaned up, looks like this:

library(dada2)
library(phyloseq)
library(ggplot2)

load("ps.rda")

source("/vol/funmic/scripts/sipFunctions.R")

## subset our phyloseq object by substrate:
samp.a <- rownames(sample_data(ps)[sample_data(ps)$Substrate == "A",])
samp.g <- rownames(sample_data(ps)[sample_data(ps)$Substrate == "G",])
samp.m <- rownames(sample_data(ps)[sample_data(ps)$Substrate == "M",])
ps.a <- prune_samples(samp.a, ps)
ps.g <- prune_samples(samp.g, ps)
ps.m <- prune_samples(samp.m, ps)

## the overall picture with an ordination. 
plotPCAWithSpecies(ps=ps.m, ptitle="Methanol PCA")

## which species look important? use this ordination to find possibly important 
## species
 
## use the following function to examine
dev.new()
getFractionAbundances (ASVname="ASV2", whichPS=ps.m, ptitle="methanol, ASV2")

dev.new()
getFractionAbundances (ASVname="ASV20", whichPS=ps.m, ptitle="methanol, ASV20")

dev.new()
getFractionAbundances (ASVname="ASV3", whichPS=ps.m, ptitle="methanol, ASV3")

##### get everything  working on mac ####

## for the mac terminal, I setup an .ssh folder in /Users/evahoffmann

## set privileges for it and my private key

chmod 700 .ssh/
chmod 600 .ssh/testMacKey

## macs need xQuartz:

## following https://www.cyberciti.biz/faq/apple-osx-mountain-lion-mavericks-install-xquartz-server/

## install from the website:
https://www.xquartz.org/
## was easy, open dowloads, double click, follow  instructions, restart after install

## ssh command:
ssh -Xi /Users/evahoffmann/.ssh/testMacKey ubuntu@129.70.51.6 -p 30423


#### test out VMs, keys

Alica Heid
funmicStudent01
ssh -i /path/to/your/ssh/private/key ubuntu@129.70.51.6 -p 30299

Gabriela "Goscinska Twigg"
funmicStudent02
ssh -i /path/to/your/ssh/private/key ubuntu@129.70.51.6 -p 30361

Johannes Feldmeier
funmicStudent03
ssh -i /path/to/your/ssh/private/key ubuntu@129.70.51.6 -p 30311

Tobias Lehmann
funmicStudent04
ssh -i /path/to/your/ssh/private/key ubuntu@129.70.51.6 -p 30414

Sascha Neuhaus
funmicStudent05
ssh -i /path/to/your/ssh/private/key ubuntu@129.70.51.6 -p 30425

Nelly Ramser
funmicStudent06
ssh -i /path/to/your/ssh/private/key ubuntu@129.70.51.6 -p 30494

Rika Stute
funmicStudent07
ssh -i /path/to/your/ssh/private/key ubuntu@129.70.51.6 -p 30344

Hanna Whr
funmicStudent08
ssh -i /path/to/your/ssh/private/key ubuntu@129.70.51.6 -p 30337

Silena Rausch
funmicStudent09
ssh -i /path/to/your/ssh/private/key ubuntu@129.70.51.6 -p 30349

Floyd Betz
funmicStudent10
ssh -i /path/to/your/ssh/private/key ubuntu@129.70.51.6 -p 30362

Carolina Steger
funmicStudent11
ssh -i /path/to/your/ssh/private/key ubuntu@129.70.51.6 -p 30463

Ana-Lena Knll
funmicStudent12
ssh -i /path/to/your/ssh/private/key ubuntu@129.70.51.6 -p 30370

## try these out. For funmic1:
## funmicStudent01
ssh -X ubuntu@129.70.51.6 -p 30299 ## vol not mounted?
sudo mount /dev/vdd /vol/funmic/
## conda not working...
## had to run first:
conda init 
## seems okay

## rerun on all of the others:
#ssh -X ubuntu@129.70.51.6 -p 30361 
#ssh -X ubuntu@129.70.51.6 -p 30311 
#ssh -X ubuntu@129.70.51.6 -p 30414 
#ssh -X ubuntu@129.70.51.6 -p 30425 
#ssh -X ubuntu@129.70.51.6 -p 30494 
#ssh -X ubuntu@129.70.51.6 -p 30344 
#ssh -X ubuntu@129.70.51.6 -p 30337 
#ssh -X ubuntu@129.70.51.6 -p 30349 
#ssh -X ubuntu@129.70.51.6 -p 30362 
#ssh -X ubuntu@129.70.51.6 -p 30463
#ssh -X ubuntu@129.70.51.6 -p 30370

## as above, some of these aren't mounted:
sudo mount /dev/vdc /vol/funmic/
conda init #bash 

## seems okay

## store keys for these on laptop at:
cd /home/daniel/Documents/teaching/functionalMicrobiomes/funmic2025admin/keys

## make 12 keys:
for i in {1..12}; do 
  echo $i
  ssh-keygen -f funmicStudent$i -N ""
done

## let's add these: 

path2key="/home/daniel/Documents/teaching/functionalMicrobiomes/funmic2025admin/keys/"

## funmicStudent1
ssh-copy-id -p 30299 -i ${path2key}funmicStudent1.pub ubuntu@129.70.51.6  
## test login:
ssh -Xvi ${path2key}funmicStudent1.pub ubuntu@129.70.51.6 -p 30299 

## seems to work. repeat for the remaining: 

## funmicStudent2
ssh-copy-id -p 30361 -i ${path2key}funmicStudent2.pub ubuntu@129.70.51.6  
## test login:
ssh -Xvi ${path2key}funmicStudent2.pub ubuntu@129.70.51.6 -p 30361 

## funmicStudent3
ssh-copy-id -p 30311 -i ${path2key}funmicStudent3.pub ubuntu@129.70.51.6  
## test login:
ssh -Xvi ${path2key}funmicStudent3.pub ubuntu@129.70.51.6 -p 30311

## funmicStudent4
ssh-copy-id -p 30414 -i ${path2key}funmicStudent4.pub ubuntu@129.70.51.6  
## test login:
ssh -Xvi ${path2key}funmicStudent4.pub ubuntu@129.70.51.6 -p 30414

## funmicStudent5
ssh-copy-id -p 30425 -i ${path2key}funmicStudent5.pub ubuntu@129.70.51.6  
## test login:
ssh -Xvi ${path2key}funmicStudent5.pub ubuntu@129.70.51.6 -p 30425


## funmic student6 
ssh-copy-id -p 30494 -i ${path2key}funmicStudent6.pub ubuntu@129.70.51.6  
ssh -Xvi ${path2key}funmicStudent6.pub ubuntu@129.70.51.6 -p 30494


## funmicStudent07

ssh-copy-id -p 30344 -i ${path2key}funmicStudent7.pub ubuntu@129.70.51.6  

ssh -vi ${path2key}funmicStudent7.pub ubuntu@129.70.51.6 -p 30344


## funmicStudent08
ssh-copy-id -p 30337 -i ${path2key}funmicStudent8.pub ubuntu@129.70.51.6  
ssh -Xvi ${path2key}funmicStudent8.pub ubuntu@129.70.51.6 -p 30337


## funmicStudent09
ssh-copy-id -p 30349 -i ${path2key}funmicStudent9.pub ubuntu@129.70.51.6  
ssh -Xvi ${path2key}funmicStudent9.pub ubuntu@129.70.51.6 -p 30349

## funmicStudent10
ssh-copy-id -p 30362 -i ${path2key}funmicStudent10.pub ubuntu@129.70.51.6  
ssh -Xvi ${path2key}funmicStudent10.pub ubuntu@129.70.51.6 -p 30362

## funmicStudent11
ssh-copy-id -p 30463 -i ${path2key}funmicStudent11.pub ubuntu@129.70.51.6  
ssh -Xvi ${path2key}funmicStudent11.pub ubuntu@129.70.51.6 -p 30463

## funmicStudent12
ssh-copy-id -p 30370 -i ${path2key}funmicStudent12.pub ubuntu@129.70.51.6  
ssh -Xvi ${path2key}funmicStudent12.pub ubuntu@129.70.51.6 -p 30370

## check these by removing .ssh:
https://apple.stackexchange.com/questions/166970/ssh-key-persists-even-after-i-delete-the-private-key-from-ssh-why
ssh-add -D

## I think dimitri's key should be in these already, because we cloned the teaching computer.

## these are the key file assignments:
Alica Heid, funmicStudent1
Gabriela Goscinska Twigg, funmicStudent2
Johannes Feldmeier, funmicStudent3
Tobias Lehmann, funmicStudent4
Sascha Neuhaus, funmicStudent5
Nelly Ramser, funmicStudent6
Rika Stute, funmicStudent7
Hanna Whr, funmicStudent8
Silena Rausch, funmicStudent9
Floyd Betz, funmicStudent10
Carolina Steger, funmicStudent11
Ana-Lena Knll, funmicStudent12

## for the login slide
Your name,VM name,Username,Remote host,SSH port
Alica Heid, funmicStudent01, ubuntu, 129.70.51.6, 30299
Gabriela Goscinska Twigg, funmicStudent02, ubuntu, 129.70.51.6, 30361
Johannes Feldmeier, funmicStudent03, ubuntu, 129.70.51.6, 30311
Tobias Lehmann, funmicStudent04, ubuntu, 129.70.51.6, 30414
Sascha Neuhaus, funmicStudent05, ubuntu, 129.70.51.6, 30425
Nelly Ramser, funmicStudent06, ubuntu, 129.70.51.6, 30494
Rika Stute, funmicStudent07, ubuntu, 129.70.51.6, 30344
Hanna Whr, funmicStudent08, ubuntu, 129.70.51.6, 30337
Silena Rausch, funmicStudent09, ubuntu, 129.70.51.6, 30349
Floyd Betz, funmicStudent10, ubuntu, 129.70.51.6, 30362
Carolina Steger, funmicStudent11, ubuntu, 129.70.51.6, 30463
Ana-Lena Knll, funmicStudent12, ubuntu, 129.70.51.6, 30370

## so my ssh commands to access these computers would be:

ssh -X ubuntu@129.70.51.6 -p 30299 ## Alica 

ssh -X ubuntu@129.70.51.6 -p 30361 ## Gabriela 


ssh -X ubuntu@129.70.51.6 -p 30311 ## Johannes 
ssh -X ubuntu@129.70.51.6 -p 30414 ## Tobias 
ssh -X ubuntu@129.70.51.6 -p 30425 ## Sascha 
ssh -X ubuntu@129.70.51.6 -p 30494 ## Nelly 
ssh -X ubuntu@129.70.51.6 -p 30344 ## Rika 
ssh -X ubuntu@129.70.51.6 -p 30337 ## Hanna 
ssh -X ubuntu@129.70.51.6 -p 30349 ## Silena 
ssh -X ubuntu@129.70.51.6 -p 30362 ## Floyd 
ssh -X ubuntu@129.70.51.6 -p 30463 ## Carolina 
ssh -X ubuntu@129.70.51.6 -p 30370 ## Ana-Lena 

## for some reason, the teaching computer won't mount

## using alica's computer for now:

ssh -X ubuntu@129.70.51.6 -p 30299

## does the problem lie with the volume or the VM?

## try attaching it to Alica's machine. 

ssh -X ubuntu@129.70.51.6 -p 30299 ## alica

ssh -X ubuntu@129.70.51.6 -p 30370 ## Ana-Lena

sudo mount vde /media/testVol

## fails, with same error

## inverse - let's see if the teaching machine will accept a different volume

sudo mount vdc /vol/funmic

## also fails?


lsblk -o NAME,SIZE,MOUNTPOINT,FSTYPE,TYPE,UUID | egrep -v "^loop"

sudo mount 861ebc07-e98a-4c2f-aacd-f9a96bfeac6f /vol/funmic

## great, now I fucked up the student computer. 
## can we restore the teaching computer using the fstab?

lsblk -o NAME,SIZE,MOUNTPOINT,FSTYPE,TYPE,UUID | egrep -v "^loop"

UUID=861ebc07-e98a-4c2f-aacd-f9a96bfeac6f ## why is this exactly the same as the other? weird.

## adding this vol back into the fstab brings things back to 
## life. 

## and what about the student VM/vol?

## can't remount the drive 
## anyway

## try putting it in the fstab? as above, since the UUID is same:
UUID=861ebc07-e98a-4c2f-aacd-f9a96bfeac6f       /vol/funmic      auto    defaults        0       2

## this seems to restore function this vm (funmic01).

## just checking, how would we modify the metagenome script to include nohup?:



