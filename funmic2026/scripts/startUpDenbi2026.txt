## no microtraits this year, drop it. We may use "s-cyc-db" 
## which should just be a diamond-searchable db

######## set up VM ##########

## let's begin the process of setting up de.NBI for our practical:

## let's get the log-in working

ssh -i /path/to/your/ssh/private/key ubuntu@129.70.51.6 -p 31993

## need to update keys for denbi....

## let's make a new key pair for our "new" desktop

ssh-keygen -t rsa -f officeComp_denbi26

## starting a new repo:
git clone https://github.com/danchurch/FunctionalMicrobiomePractical.git
git config --global user.email "danchurchthomas@gmail.com"
git config --global user.name "danchurch"
git remote set-url origin git@github.com:danchurch/FunctionalMicrobiomePractical.git

 
ssh ubuntu@129.70.51.6 -p 31993 ## login works from desktop. We're in business. 

## if we want to transfer with rsync, instead of scp, any problems there?
## https://stackoverflow.com/questions/4549945/is-it-possible-to-specify-a-different-ssh-port-when-using-rsync
## something like?: rsync -auv daniel@132.180.112.24:/home/daniel/.ssh/officeComp_denbi26.pub ~/

## let's just try rerun our installation scripts from last year:

## mount the drive, with fstab so it survives reboots:

sudo mount /dev/vdc /vol/funmic

blkid /dev/vda 

## funny, volume is not listed as mounted, but seems to be working 
## and rebooting doesn't really break it.  I don't understand.
## but I guess if it isn't broken, don't fix it....

## on the simpleVM de.NBI site, they mention the volumes may not
## be formatted. Let's check.

df -Th ## not listed there. I'm confused. How am I allowed to write to it?

## something doesn't smell right.
## let's try formatting the file system before we do any work. 
## Might save us problems later:

#sudo mkfs.ext4 /dev/vdc
sudo mount /dev/vdc /vol/funmic ## now this works 

lsblk -o NAME,SIZE,MOUNTPOINT,FSTYPE,TYPE,UUID | egrep -v "^loop"

## works. UUID = 1c547cd8-ce37-4efc-a49e-be6d967a72fa

## but change the owner for this path away from Root to normal user:
sudo chown ubuntu:ubuntu /vol/funmic

## let's wait to add it to the fstab. I think this could cause
## problems when we clone the whole setup. 

## some housekeeping software...

sudo apt install unzip

## last year we had do some extra installs for port-forwarding
## do we need to do this again?

## not run ##
apt search xorg

apt search xauth

sudo apt-get install xauth xorg
## not run ^^ ##

## of course, need conda:

mkdir -p /vol/funmic/miniconda

wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /vol/funmic/miniconda/miniconda.sh
bash /vol/funmic/miniconda/miniconda.sh -b -u -p /vol/funmic/miniconda/miniconda3
rm /vol/funmic/miniconda/miniconda.sh
source /vol/funmic/miniconda/miniconda3/bin/activate
export PATH=/vol/funmic/miniconda3/miniconda3/bin:$PATH
conda init

## get the main channels that we use:
conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge



## now start building the software environments...
## logging in and out, conda is stable. 
## but of course reboot kills it, because of the fstab issue. 
## for now remount and restart conda each time,

## I think that means each time we reboot we need to do this:

sudo mount /dev/vdc /vol/funmic
conda init ## needed?
source ~/.bashrc

## fix the fstabs individually when we have the other VMs. 


####### sra toolkit #######
wget https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/current/sratoolkit.current-ubuntu64.tar.gz
tar -xzf sratoolkit.current-ubuntu64.tar.gz

cd /home/ubuntu/sratoolkit.3.3.0-ubuntu64/bin

## set the working directory and temp file location to somewhere on the volume, then
## seems to work, add to path

## adding this to bashrc
export PATH=/home/ubuntu/sratoolkit.3.3.0-ubuntu64/bin:$PATH

####### Dimitri's data #######

## from last year we have this script for getting his 
## data. Does it work?

####### getKelpReads.sh ############

names=(
"ERR3801502"
"ERR3801542"
"ERR3801603"
)

for i in ${names[@]}; do
  prefetch $i
  fasterq-dump --split-files $i
done

####################################

## run this on the volume where we have plenty of room:

mkdir -p /vol/funmic/datasets/kelpBiofilm

cd /vol/funmic/datasets/kelpBiofilm

nohup bash getKelpReads.sh &


######## science software installs  ##########

### cutadapt ###

conda create -n qualityControlRawSequences -c bioconda cutadapt fastqc

### phyloflash ###

conda create -n communityComposition -c bioconda phyloflash

## not working, maybe try an older version:

#conda create -n phyloflash_3.4.1 bioconda::phyloflash==3.4.1 ## nope

## last supported version for linux on bioconda:
conda create -n phyloflash_3.3b1 bioconda::phyloflash==3.3b1
## bioconda/linux-64::bbmap-39.52-he5f24ec_0

##  formatted silva database for phyloFlash
## looks like they have updated from last year to silva 138.2:

cd /vol/funmic/databases

wget https://zenodo.org/records/18158647/files/138.2.tar.gz

tar -xzf 138.2.tar.gz

mv 138.2/ phyloflashSilvaDB/

conda activate communityComposition # 

## test 
phyloFlash.pl -dbhome /vol/funmic/databases/phyloflashSilvaDB -lib TEST -CPUs 8 \
 -read1 ${CONDA_PREFIX}/lib/phyloFlash/test_files/test_F.fq.gz \
 -read2 ${CONDA_PREFIX}/lib/phyloFlash/test_files/test_R.fq.gz \
 -almosteverything

## seems to work.
## but not working for Dimitris reads

## dimitris code from last time :
phyloDB=/vol/funmic/databases/phyloflashSilvaDB
READ_DIRECTORY=/vol/funmic/datasets/kelpBiofilm
OUTPUTDIRECTORY=/vol/funmic/Kelp/phyloFlashOut/

cd /vol/funmic/databases/phyloflashSilvaDB

## break his code up into one command:

mkdir /vol/funmic/danTesting

cd /vol/funmic/danTesting

conda activate phyloflash_3.3b1 
#conda activate phyloflash_3.4.1  ## same error
#conda activate communityComposition # 


## just checking, if we reorder our path variable?

echo $PATH

which bbmap.sh

export PATH="/vol/funmic/.metabat/bin:/home/ubuntu/sratoolkit.3.3.0-ubuntu64/bin:/vol/funmic/miniconda/miniconda3/envs/phyloflash_3.3b1/bin:/vol/funmic/miniconda/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/vol/funmic/.bbmap"

which bbmap.sh ## goes to the conda-installed bbmap

READ_DIRECTORY=/vol/funmic/datasets/kelpBiofilm
phyloDB=/vol/funmic/databases/phyloflashSilvaDB/138.2
SAMPLE=ERR3801502
phyloFlash.pl \
    -lib PhyloFlash_Kelp_${SAMPLE} \
    -read1 ${READ_DIRECTORY}/${SAMPLE}_1.fastq.gz \
    -read2 ${READ_DIRECTORY}/${SAMPLE}_2.fastq.gz \
    -readlength 250 \
    -clusterid 98 \
    -taxlevel 7 \
    -dbhome $phyloDB \
    -CPUs 10 \
    -everything


## could the issue be with bbmap? Try with sortmerna instead


conda activate pf_sortmerna 

phyloDB=/vol/funmic/databases/phyloflashSilvaDB/138.2
phyloFlash.pl -dbhome $phyloDB -lib TEST -CPUs 4 \
 -read1 ${CONDA_PREFIX}/lib/phyloFlash/test_files/test_F.fq.gz \
 -read2 ${CONDA_PREFIX}/lib/phyloFlash/test_files/test_R.fq.gz \
 -sortmerna \
 -almosteverything

## meh, looks like we need to muck around a lot with sortmerrna to 
## make that work..


## since the test files are running well, this seems funny. 

## let's try repairing the illumina reads:

READ_DIRECTORY=/vol/funmic/datasets/kelpBiofilm
repair.sh in=${READ_DIRECTORY}/ERR3801502_1.fastq.gz \
          in2=${READ_DIRECTORY}/ERR3801502_2.fastq.gz \
          out=test_1.fastq \
          out2=test_2.fastq \
          outs=singles.fastq

## nope, don't seem to be any issues there.

ls -l ${READ_DIRECTORY}/ERR3801502_1.fastq.gz 

ls -l ${READ_DIRECTORY}/ERR3801502_2.fastq.gz 

wc -l ${READ_DIRECTORY}/ERR3801502_1.fastq.gz 

wc -l test_1.fastq

head test_1.fastq

wc -l ${READ_DIRECTORY}/ERR3801502_2.fastq.gz 
wc -l test_2.fastq

## not sure what is going on...


## 

### megahit ###
conda create -n assembly -c bioconda megahit

### quast ###
conda create -n assemblyQC -c bioconda quast

### minimap ###

## the repo for minimap (https://github.com/lh3/minimap2?tab=readme-ov-file#install)
## says just download binaries. so:

mkdir /vol/funmic/.minimap2

cd /vol/funmic/.minimap2

curl -L https://github.com/lh3/minimap2/releases/download/v2.28/minimap2-2.28_x64-linux.tar.bz2 | tar -jxvf -

## link to somewhere we can find it
sudo ln -s /vol/funmic/.minimap2/minimap2-2.28_x64-linux/minimap2 /usr/local/bin/minimap2

cd /vol/funmic

wget https://netix.dl.sourceforge.net/project/bbmap/BBMap_39.62.tar.gz

tar -xvzf BBMap_39.62.tar.gz

mv bbmap /vol/funmic/.bbmap/

## needs java:

sudo apt install default-jre

##### binning software #####

## last year installing maxbin2 and concoct via conda worked
## but had to compile metabat from source.

### maxbin2 ###
### concoct ###
conda create -n binning -c bioconda maxbin2 concoct

### metabat ###

## have to install from source (https://bitbucket.org/berkeleylab/metabat/src/master/)

apt search libboost-all-dev
sudo apt install libboost-all-dev
sudo apt install cmake
sudo apt install g++ ## already installed 
apt search autoconf ## already installed 

cd /vol/funmic
git clone https://bitbucket.org/berkeleylab/metabat.git metabat
cd metabat
mkdir build 
cd build
cmake -DCMAKE_INSTALL_PREFIX=/vol/funmic/.metabat ..
make
make install
cd ..
rm -rf build

## add to path 
export PATH="/vol/funmic/.metabat/bin:$PATH"

##### refining bins #####
### das_tool ###

conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge
conda create -n refine -c bioconda das_tool

##### depreplication #####
### deprep ###

conda create -n dereplication -c bioconda  drep

## however, this had a problem. The version of pandas installed 
## automatically was too new, Dimitri re-installed with a 
## pandas version from 2024

##### assign taxonomy to bins #####
### gtdb-tk ###

conda create -n assignTaxonomy -c conda-forge -c bioconda gtdbtk=2.6.1


conda activate assignTaxonomy

## try their script for downloading the GTDB:

download-db.sh

## they say to move this here:
/vol/funmic/miniconda/miniconda3/envs/assignTaxonomy/share/assignTaxonomy/db/

gtdbtk check_install

## while that is running...

##### metabarcoding #####

### R ####
sudo apt install r-base-core

sudo R

## of course some dependencies, variously in R and terminal
install.packages("BiocManager", repos = "https://cloud.r-project.org")

## from last year we were missing the following packages/libraries
sudo apt install libcurl4-openssl-dev
sudo apt install libssl-dev
sudo apt install libblas-dev liblapack-dev
sudo apt install libpng-dev
sudo apt install libjpeg-dev
sudo apt install liblzma-dev
sudo apt install libbz2-dev

BiocManager::install("dada2")

## get a dada2-compatible silva:

mkdir /vol/funmic/databases/metabarcodingSilvaDB 
cd /vol/funmic/databases/metabarcodingSilvaDB

wget https://zenodo.org/records/14169026/files/silva_nr99_v138.2_toGenus_trainset.fa.gz

### vegan ###
install.packages('vegan')

### phyloseq ###
BiocManager::install("phyloseq")

##### annotation software #####

## as before, try one big environment:

conda create -n annotation -c bioconda -c conda-forge prodigal diamond eggnog-mapper 

## not including interproscan because the conda version is pretty old.

## I'm confused a bit, not sure if the databases downloaded with that?

conda activate annotation 

emapper.py ## nope



## try their script for getting the database:

download_eggnog_data.py

## it wants a directory at: 
cd /vol/funmic/miniconda/miniconda3/envs/annotation/lib/python3.11/site-packages/data
## which doesn't exist. Not sure if I should make it.

## last year I changed the shell variable for this environment to force it to check 
## some where else. Try this again:

conda activate annotation; echo $CONDA_PREFIX

cd $CONDA_PREFIX

mkdir -p ./etc/conda/activate.d
mkdir -p ./etc/conda/deactivate.d
mkdir /vol/funmic/databases/eggnog-mapper-data

touch ./etc/conda/activate.d/env_vars.sh
touch ./etc/conda/deactivate.d/env_vars.sh

vim ./etc/conda/activate.d/env_vars.sh ## to say:

#############
#!/bin/sh

export EGGNOG_DATA_DIR=/vol/funmic/databases/EggNOG
###############

## and the deactivation script:
vim ./etc/conda/deactivate.d/env_vars.sh

##############
#!/bin/sh

unset EGGNOG_DATA_DIR
##############

## try again:
download_eggnog_data.py

## not finding the embl website...why?

vim -M /vol/funmic/miniconda/miniconda3/envs/annotation/bin/download_eggnog_data.py

download_eggnog_data.py

## from this script...
http://eggnogdb.embl.de/download/emapperdb-{__DB_VERSION__}

vim -M /vol/funmic/miniconda/miniconda3/envs/annotation/lib/python3.11/site-packages/eggnogmapper/version.py ## we need db version 5.0.2


## yeah, the website for the eggnog mapper databases is down and apparently has
## been for a couple months. I have added my name to the list of users on the issue.

## let's try the version that Dimitri has locally on the nanopore computer:

## on my desktop:
rsync -auv test@132.180.112.115:/media/vol2/Databases/EggNOG /media/vol/denbiFileTransfers/

## and then onto deNBI. 

rsync -auv \
  --progress \
  -e "ssh -p 31993" \
  /media/vol/denbiFileTransfers/EggNOG ubuntu@129.70.51.6:/vol/funmic/databases/

## test again:
emapper.py 

## no error about databases. Test it deeper later. 

### interproscan ###

## there is a repo... are the binaries and the database in there?

cd /vol/funmic
#git clone https://github.com/ebi-pf-team/interproscan.git

## nope, not the way to do that...
## as per: https://interproscan-docs.readthedocs.io/en/v5/UserDocs.html#obtaining-a-copy-of-interproscan

mkdir /vol/funmic/databases/interproscan-5.77-108.0
cd /vol/funmic/databases/interproscan-5.77-108.0
wget https://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.77-108.0/interproscan-5.77-108.0-64-bit.tar.gz
wget https://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.77-108.0/interproscan-5.77-108.0-64-bit.tar.gz.md5
md5sum -c interproscan-5.77-108.0-64-bit.tar.gz.md5 ## looks good
tar -pxvzf interproscan-5.77-108.0-*-bit.tar.gz

cd /vol/funmic/databases/interproscan-5.77-108.0

./interproscan.sh -version ## seems to work.


### metabarcoding data ###

## somehow felix made the metabarcode data from last year available
## publically:

## on my compuer: 
cd /media/vol/funmicBackups/Barcode2025

wget -O oemik2025.zip https://cloud.bayceer.uni-bayreuth.de/index.php/s/Om31TQtpVSDj4SV/download?path=%2F&files=

unzip oemik2025.zip

## last year we had 3 substrates for analysis:
## acetate, methanol, glucose
## in the lab we processed glycerol

## this year we add glycerol to the analysis

## where are the sample data for these? wait for instructions from Tillmann/Felix.
## notes from felix about data:

7 ~ 12C - glycerol
Fractions 3-8
=> 7.3 - 7.8

8 ~ 13C - glycerol
Fractions 2-7
8.2 - 8.7

## tillmann will supply the buoyant densities. 
## in the meantime, we need a sample table, and to update the 

## merge files from last year and the new substrate (glycerol):

## still on my computer:
ln -s /media/vol/funmicBackups/Barcode2024/raw_reads/* /media/vol/funmicBackups/Barcode2025/raw_reads

## last year, file names were: letter for substrate + number of isotope + number of fraction
## can we do the renaming of these files to match last year, and also generate our 
## sample data spreadsheet at the same time?

cd /media/vol/funmicBackups/Barcode2026

#sudo pip3 install openpyxl

python3
import pandas as pd
import re, os
from pathlib import Path

os.listdir()

aa = pd.read_excel('OeMik-Exp032_Proben_v2.xlsx', sheet_name="Proben")[['Genomics Id', 'Probe Id', 'ID']]
probefilter = aa['Probe Id'].isin(list(range(75,87)))
bb = aa[probefilter]
p = re.compile("\.|--")
cc = bb['ID'].str.split(p,expand=True)
dd = pd.concat([bb,cc], axis=1)
dd.columns = ['Genomics Id', 'Probe Id', 'ID', 'isoCode', 'Fraction', 'PCRcycles']
numberConvert = {"7":12, "8":13}
dd['Isotope'] = dd['isoCode'].apply(lambda x: numberConvert[x])
dd['sampleName'] = "Y" + dd['Isotope'].astype("string") + "-" + dd['Fraction'].str.zfill(2).astype("string")
dd['newFileName'] = "/media/vol/funmicBackups/Barcode2026/raw_reads/" + dd['sampleName'] + r".fastq.gz"
dd['oldFileName'] = "/media/vol/funmicBackups/OemikExp032/" + dd["Genomics Id"] + "_S" + dd["Probe Id"].astype('string') + "_L001_R1_001.fastq.gz"

## test for real files:
dd['oldFileName'].apply(lambda x: Path(x).exists()) ## looks okay

## now rename them. 
## will symbolic links work for us?
for row in dd.itertuples():
  os.symlink(row.oldFileName, row.newFileName)

## let's create our datasheet. We need to merge our new sample data with 
## the old, which is found here:

sip2024 = pd.read_csv('/home/daniel/Documents/teaching/funmic/FunctionalMicrobiomePractical/funmic2025/metabarcodeData/sip2024.csv')
sip2024.rename({'Unnamed: 0': 'sampleName'}, axis=1, inplace=True)

ee = dd.copy()[['Probe Id','Fraction','Isotope', 'sampleName']]
ee['Substrate'] = 'Y'
ee['BD'] = 0.000000001 ### fix this with Tillmann data when he sends it !!!!!!!!!!
ee = ee[['sampleName','Substrate','Isotope','Fraction','BD' ]]

sip2026 = pd.concat([sip2024,ee], axis=0)

metabarcodeDataDir="/home/daniel/Documents/teaching/funmic/FunctionalMicrobiomePractical/funmic2026/metabarcodeData/"
sip2026.to_csv(metabarcodeDataDir + "sip2026.csv", index=False)

## some minor differences from last year, named rowname column
## all values quoted last year...both seem trivial.
## Just need tillmann's BD data. Update when we get it.

## we want to put these files on our denbi machine, with the soft link names 
## not the original names... back in bash... note the "L" argument with rsync

cd /media/vol/funmicBackups/Barcode2026/raw_reads

getFile=/media/vol/funmicBackups/Barcode2026/raw_reads/
putHere=/vol/funmic/datasets/Barcode2026/raw_reads/
rsync -auvL \
  --progress \
  -e "ssh -p 31993" \
  $getFile ubuntu@129.70.51.6:$putHere

#### revisit metabarcoding pipeline ####

## tillmann got us the buoyant densities
## add these into our data:

/home/daniel/Documents/teaching/funmic/FunctionalMicrobiomePractical/funmic2026/metabarcodeData/sip2026.csv 

## bd_glycerol.csv ##
"Fraction","12C","13C"
1, 1.76, 1.761
2, 1.747, 1.749
3, 1.731, 1.735
4, 1.72, 1.722
5, 1.708, 1.712
6, 1.697, 1.699
7, 1.686, 1.688
8, 1.674, 1.676
9, 1.662, 1.662

python3
import pandas as pd
import re, os
from pathlib import Path

## our metadat so far is:
sipMD = pd.read_csv('sip2026.csv')
sipMD.head()

aa = pd.read_csv("bd_glycerol.csv")
C12 = aa[["Fraction","12C"]]
C12["Isotope"] = 12
C12.rename({"12C":"BD"}, axis="columns", inplace=True)
C13 = aa[["Fraction","13C"]]
C13["Isotope"] = 13
C13.rename({"13C":"BD"}, axis="columns", inplace=True)
glycerolOnly = pd.concat([C12, C13])
glycerolOnly['sampleName'] = "Y"+ glycerolOnly['Isotope'].astype('str') + "-" + glycerolOnly['Fraction'].astype('str').str.zfill(2)
glycerolOnly['Substrate'] = "Y"
glycerolOnly = glycerolOnly[['sampleName','Substrate','Isotope','Fraction','BD']]
hasReads = glycerolOnly['sampleName'].isin(sipMD['sampleName'])
glycerolOnly = glycerolOnly[hasReads]
sipMD = sipMD[sipMD['Substrate'] != "Y"]

sipMDnew = (pd.concat([glycerolOnly, sipMD]).
              sort_values(by=["Substrate","Isotope","Fraction"]).
              reset_index(drop=True)
            )

metabarcodeDataDir="/home/daniel/Documents/teaching/funmic/FunctionalMicrobiomePractical/funmic2026/metabarcodeData/"
sipMDnew.to_csv(metabarcodeDataDir + "sip2026.csv", index=False)

## put it on denbi

getFile=/home/daniel/Documents/teaching/funmic/FunctionalMicrobiomePractical/funmic2026/metabarcodeData/sip2026.csv
putHere=/vol/funmic/datasets/Barcode2026/
rsync -auv \
  --progress \
  -e "ssh -p 31993" \
  $getFile ubuntu@129.70.51.6:$putHere


###### get Tillmann his spreadsheets #########

## this starts around line 2500 from last year's startup script:

## genus level:

aa <- as.data.frame(otu_table(ps))
bb <- as.data.frame(tax_table(ps))
cc <- paste("d__",bb$Kingdom,";p__",bb$Phylum,";c__",bb$Class,";o__",bb$Order,";f__",bb$Family,";g__",bb$Genus, sep="")
all(rownames(tax_table(ps)) == colnames(aa))
colnames(aa) <- cc
dd <- cbind(aa,sample_data(ps))
level6df <- dd
rm(aa,bb,cc,dd)

## family level
ps.family <- tax_glom( ps, taxrank = "Family")
aa <- as.data.frame(otu_table(ps.family))
bb <- as.data.frame(tax_table(ps.family)[,1:5])
cc <- paste("d__",bb$Kingdom,";p__",bb$Phylum,";c__",bb$Class,";o__",bb$Order,";f__",bb$Family, sep="")
## do the rownames of our OTUtable match the order of our taxonomic names?
all(rownames(tax_table(ps.family)) == colnames(aa)) ## yes
## so use these as our colnames:
colnames(aa) <- cc
## and he wants the sample info as the last columns:
dd <- cbind(aa,sample_data(ps.family))
level5df <- dd
rm(aa,bb,cc,dd)

write.csv(level6df, "level6.csv")
write.csv(level5df, "level5.csv")

getFile=/vol/funmic/metabarcoding/level*.csv
putDir=/home/daniel/Documents/teaching/funmic/FunctionalMicrobiomePractical/funmic2026/metabarcodeData/
rsync -auv \
  --progress \
  -e "ssh -p 31993" \
  ubuntu@129.70.51.6:$getFile $putDir


### generating keys for students. We have eight students. 

cd /home/daniel/Documents/teaching/funmic/2025-2026/practicalCompAdmin

cut -f2 -d, studentList.csv

## to be continued. check out line 2080 from last years startup.
## first need to spin up VMs. 

