## okay, time to start setting up the 2023 funmic class


## mounting hard drive

## find it:
lsblk

## get uuid
blkid
## mount

sudo mount -U "c1feef14-014f-4cb0-9ee0-4db329308eab" /vol/danBot

## or
sudo mount /dev/vdc /vol/danBot


## to make sure everything is kosher for github:
find . -type f -size +40M

## or newer than 

find . -type f -newermt '5 minutes ago'

## also, conda broke when we updated a cloud config file.

## reinstall:

wget https://repo.anaconda.com/archive/Anaconda3-2022.10-Linux-x86_64.sh

bash ./Anaconda3-2022.10-Linux-x86_64.sh

## put anaconda here:
/vol/testNotSoBig/anaconda

## first steps, today:

## 0 - clean out the old folders, github repo
## done
## 1 - setup the instance for development
## instance initiated, I think? Not showing on my dashboard...

## 2 - get data sets

## maybe let's check out the Sereika/Albertson data and pipeline:
https://www.nature.com/articles/s41592-022-01539-7#data-availability

## their repo is here:
https://github.com/Serka-M/Digester-MultiSequencing

## their pipeline is really similar to the one we used last year,
## though no biobakery tools were used, just gtdb-tk. 

## I like the biobakery tools, let's decide when we get there...

## they have both mock and environmental data

## the mock DNA is from the zymogen mock community. 
## the eDNA is from activated sludge from an anaerobic sewage
## treatment plant. I don't really understand, because I
## thought "activated" implied the injection of oxygen
## into sewage material...
## intentionally anaerobic conditions implies they wanted
## methane production.
## not sure. Anyway...

## They have illumina reads, pacbio, and nanopore sequence data
## The nanopore data is of two flowcell generations, R10.4 and R9.4.1

## data for the mock community are here:
https://www.ebi.ac.uk/ena/browser/view/PRJEB48692

## in both cases, there are two nanopore platforms on there...
## do we want their minion data?

## quote: 
## "Anaerobic digester and Zymo R.9.4.1 datasets were generated on a MinION Mk1B (Oxford Nanopore Technologies) device"

## so we want the minion datasets

## in the case of the zymo data, I think the nanopore would be:
## sample name = SAMEA10644976 , library name = LIB-Zymo_HMW_R941 
## found here:
https://www.ebi.ac.uk/ena/browser/view/ERR7255742

## note there are promethion data for the R10.4 in the next entry, 

## 
## we won't use the promethion R10.4 data

## I think we are going to need a way to do this without a gui...

## for instance, does this work for the Nanopore Zymo data?

wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR725/002/ERR7255742/ERR7255742.fastq.gz
## looks like it works...so theoretically these are the files we
## need:

## zymo files (illumina miseq F+R, Nanopore minion):
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR725/009/ERR7255689/ERR7255689_1.fastq.gz
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR725/009/ERR7255689/ERR7255689_2.fastq.gz
#wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR725/002/ERR7255742/ERR7255742.fastq.gz ## that's 9.4 flowcell
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR728/008/ERR7287988/ERR7287988.fastq.gz ## 10.4, but promethion!

## biodigestor files (illumina miseq F+R, Nanopore minion):
## here they have two minion runs. One seems to be for a R10.3 cell.
## So again, stick with the R9.4.1 cell..
## also, there are two sets of illumina miseq files, one set from 2018,
## and one from 2020. The 2020 set doesn't have a finished "generated" fastq
## from the ENA folks. Supp. table 4 seems important here...

## all the nanopore data in this folder is from minions...

wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR701/004/ERR7014844/ERR7014844.fastq.gz ## 10.4
#wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR701/006/ERR7014876/ERR7014876.fastq.gz ## 9.4 flowcell
wget ftp://ftp.sra.ebi.ac.uk/vol1/run/ERR701/ERR7015307/IL-202001-1.fastq.gz
wget ftp://ftp.sra.ebi.ac.uk/vol1/run/ERR701/ERR7015307/IL-202001-2.fastq.gz

## can't find any further data. These last two are the submitted 2020 miseq 
## files, maybe they will work, if not try the 2018 files

## data for the digester are here:
https://www.ebi.ac.uk/ena/browser/view/PRJEB48021

## let's use the R9.4.1 data for the nanopore data
## this is the one that benefitted from polishing 
## with illumina data

## We'll try assembly with illumina alone, nanopore alone, and then 
## nanopore polished by illumina

## weird, though, they used 9 years of illumina data to 
## assist binning...hmm... wonder if we can skip that in the class.

## run through these for the zymogen together, then set them loose on
## the bioreactor data

## they don't have fast5 files, I don't think...
## should we practise basecalling with another dataset?

## tomorrow, start running through the datasets.
## this paper is a bit misleading. The title should actually say something
## like "Near perfect nanopore MAGs without illumina polishing, but with nine years
## of illumina data to help binning"....

## oh well. Let's see how we do. If we have to, go back to mbarc and chu. 


##### set up VM #####

## de.NBI big VM is not working, everytime I try to set up a full, GPU+ 128g RAM, 28 core
## machine, it stalls out.

## So I started up a smaller machine, no GPU, 64g, 28 cores. 

## seems to be running. Do I need to give it a new key?
## they have some on file for me...

ssh -vp 30481  -i /home/daniel/.ssh/ubuntu_e ubuntu@129.70.51.6

## looks like I deleted this key?

grep -R "3NAYJHfS2K3z5DrOM"

## yup. Can we sync up the keys that we used in the spruce project?
## that also seems to not be working...

## let's start over. We are not storing keys on the nanocomp,
## the only old key we need is the one for the emic instance
## that zhe is using.
## also github...

## it looks like our "ubuntu" keys are neither...
## take a chance and delete them.

## can we still use githhub?...yes
## emikAdmin? nope. Just killed it. 

## fix:
chmod 600 ubuntu_e

## plain old emik? yeah, still works

## okay, so those keys are only good for emik stuff.
## can we use our old id_ed keys? I think these are 
## what we use for github...

## so let's make some new ones...

man ssh-keygen

ssh-keygen -f funmic2023 

chmod 600 funmic2023

ssh -p 30427  -i /home/daniel/.ssh/funmic2023 ubuntu@129.70.51.6

## okay, that works. remember not to change keys anymore.
## de.nbi won't update the keys on VMs - they are stuck with 
## the original user-profile public keys when the VM is started

## great, now get our data:

cd /vol/testNotSoBig

sudo chown ubuntu: /vol/testNotSoBig

mkdir datasets

mkdir zymoMC

cd /vol/testNotSoBig/zymoMC
## zymo files (illumina miseq F+R, Nanopore 10.4):
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR725/009/ERR7255689/ERR7255689_1.fastq.gz
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR725/009/ERR7255689/ERR7255689_2.fastq.gz
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR728/008/ERR7287988/ERR7287988.fastq.gz


cd /vol/testNotSoBig
mkdir sludge

cd /vol/testNotSoBig/sludge
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR701/006/ERR7014876/ERR7014876.fastq.gz &
wget ftp://ftp.sra.ebi.ac.uk/vol1/run/ERR701/ERR7015307/IL-202001-1.fastq.gz &
wget ftp://ftp.sra.ebi.ac.uk/vol1/run/ERR701/ERR7015307/IL-202001-2.fastq.gz &

## next step check out the reads:

## everyone, including the authors of this paper, used porechop
## which is pretty much dead. 

## this package was pretty 

## for the illumina reads, we just use old fashioned fastqc

## which means we need to get anaconda going... 

wget https://repo.anaconda.com/archive/Anaconda3-2022.10-Linux-x86_64.sh

## installed

#### set up conda packages ###

## add bioconda
conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge
conda config --set channel_priority strict

## for updates
conda update -n base -c defaults conda

## readQC
conda create -n readQC_env -c bioconda cutadapt fastqc bbmap seqtk

### vsearch ###
conda create -n vsearch -c bioconda vsearch

### nanoplot/nanostat ###
## got to relax things a bit
conda config --set channel_priority flexible
conda create -n nanoplot -c bioconda nanoplot
conda config --set channel_priority strict

### megahit ###
conda create -n megahit -c bioconda megahit

### Flye ###
conda create -n flye -c bioconda flye

### (meta)quast ###
conda config --set channel_priority flexible
conda create -n metaquast -c bioconda quast
conda config --set channel_priority strict

### polishing ###
## and should probably put medaka and racon in the same 
#conda config --set channel_priority flexible
#conda create -n polishNanopore -c bioconda racon minimap2 medaka
#conda config --set channel_priority strict
### racon ###
#conda create -n racon -c bioconda medaka minimap2 

## that's not working. Break it up.

### medaka ###
#conda config --set channel_priority flexible
#conda create -n medaka -c bioconda medaka 

# ## recommended by the nanopore github site:
# conda create -n medaka -c conda-forge -c bioconda medaka
# #conda config --set channel_priority strict
# 
# ## that's run for ~24 hours
# 
# ## try pip in a virtual env
# python3 -m venv /path/to/new/virtual/environment
# virtualenv medaka --python=python3 --prompt "(medaka) "
# . medaka/bin/activate
# pip install medaka
# ## doesn't work, some sort of issue with compiler
# 
# ## still problems with virtualenv, won't install...jeezus
# 
# git clone https://github.com/nanoporetech/medaka.git
# cd medaka
# make install
# . ./venv/bin/activate
# ## also doesn't work
#
## maybe just a straight pip install.

conda create -n medaka 
conda activate medaka 
sudo apt install python3-pip

pip install medaka

## errors:
checking for bzlib.h... no
checking for lzma.h... no

## bzlib.h
sudo apt install libbz2-dev
## lzma.h
sudo apt install liblzma-dev
## think it works now...

### racon ###
conda create -n racon -c bioconda racon 
## quick

### samtools, bedtools, bedtools ###
conda create -n alignmentTools -c bioconda samtools=1.9 bedtools bowtie2 minimap2

conda activate alignmentTools

conda install seqtk

## can we add minimap2 to this?
## rename this to something nicer for the classk

### binning environments ###

## metaBat2 ##
conda create -n metabat2 -c bioconda metabat2
## quick

# ### maxbin2 ###
# #conda create -n maxbin2 -c bioconda maxbin2
# ## takes a long,long time
# 
# conda create -n maxbin2 -c bioconda maxbin2
# 
# ## try downloading the source from sourceforge:
# https://sourceforge.net/projects/maxbin2/
# 
# ## had to get it local
# file=/home/daniel/Documents/teaching/functionalMicrobiomes/MaxBin-2.2.7.tar.gz
# scp -i /home/daniel/.ssh/funmic2023 -P 30500 -r $file ubuntu@129.70.51.6:/vol/danBot/
# ## and back on danBot
# 
# tar -xvf MaxBin-2.2.7.tar.gz
# 
# conda create -n maxbin2 
# 
# conda activate maxbin2
# 
# conda install -c conda-forge perl
# 
# ## quick install in the readme says:
# ## 1. Download MaxBin and unzip it
# ## 2. Enter src directory under MaxBin and "make" it.
# ## 3. Run "./autobuild_auxiliary" at MaxBin directory to download, compile,
# ##    and setup the auxiliary software packages
# ## 4. MaxBin should be ready to go.
# 
# ## hopefully we can follow 
# ## https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#setting-environment-variables
# ## to add these settings
# ## https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#setting-environment-variables
# ## need perl in this environment
# 
# ## ugh, this is fucked...try concoct instead

### concoct ###

# ## following their github instructions:
# conda config --add channels defaults
# conda config --add channels bioconda
# conda config --add channels conda-forge
# conda create -n concoct python=3 concoct
# 
# ## is this necessary?
# conda install -c conda-forge scikit-learn

## this install isn't working. documentation here:
https://github.com/BinPro/CONCOCT/issues/321

## their yaml for concoct looks like thisL
###############
name: concoct
channels:
  - conda-forge
  - bioconda
dependencies:
  - concoct=1.1.0
  - libopenblas=*=openmp*
  - mkl
  - python>=3
  - samtools>=1.9
  - scikit-learn=1.1.*
variables:
  USE_OPENMP: 1
######

## let's try it:

conda deactivate
conda env remove -n concoct
conda env create -f concoct.yml



## then reset to bioconda priorities above

## VAMB ##
conda create -n vamb -c bioconda vamb
## looks like GPU support is available:
conda activate vamb
conda install -c pytorch pytorch torchvision cudatoolkit=10.2
## even with the cuda install, it's quick

## also needs minimap:
conda install -c bioconda minimap2

## das tools ##

conda config --set channel_priority flexible
conda create -n das_tool -c bioconda das_tool

conda config --set channel_priority strict
## that seems to install ok, but won't work 

## try a manual install?:
conda create -n das_tool

conda activate das_tool
cd /vol/danBot/refining/
git clone https://github.com/cmks/DAS_Tool.git
cd DAS_Tool
unzip db.zip

conda install r-data.table r-magrittr r-docopt
conda install -c bioconda prodigal diamond pullseq ruby

### checkm ###
conda create -n checkm -c bioconda checkm-genome

### MetaPhlan ###
conda create --n metaphlan -c conda-forge -c bioconda python=3.7 metaphlan
conda activate metaphlan
## put the db on the volumne, so we can clone later
metaphlan --install --bowtie2db /vol/danBot/metaphlanDB

## oh, we want to try phylphlan
conda activate metaphlan
conda install -c biobakery graphlan


### phyloPhlan ###
conda create --name phylophlan -c bioconda phylophlan

### check out the zymo data: ###

mkdir /vol/testNotSoBig/datasets/zymoMC/illumina
mkdir /vol/testNotSoBig/datasets/zymoMC/nanopore

gunzip ERR7255689_1.fastq.gz &
gunzip ERR7255689_2.fastq.gz &
gunzip ERR7255742.fastq.gz &

## the 1+2 files probably = illumina

cd /vol/testNotSoBig/datasets/zymoMC/illumina

head ERR7255689_1.fastq

tail ERR7255689_1.fastq

grep "10003:13052/" ERR7255689_1.fastq

grep "10003:13052/" ERR7255689_2.fastq

seq="GGTGACCACTCGCCCGCCCGCCGCGTTCTCTTCGCTCACGGCCAGGGCGAACATGTTAATCCAGTCGATGACGTTCATCGGATCGCGGGAGAGATTATCGCTGGAGACCAGCAGACGCCGTAACGCTACCGCGCGGCGTGGAACGTTCAGC"
seq="TTTGCATCAAAAGAAGCCCTATTTTTAGAAGTTTATCAAGATAGTATTCAGATGGAATTAACAGAACTAGGGAAAGTAGCAGAGCGAGATGATTTGGTTGGGGAAAAGAAGCTACAATCTATTTTCTTTGTAGCGACAGATTTTTCTAGCA"
seq="ACAATGCGATCAATAATGATTTCAATAGAATGCTTTTTATTCTTCTCGATTTCAATTTCGTCATTGATATCATAAATTTCTCCATCAACACGAATTCGAACATAGCCTTCTTTTTTTATTTCCTCAATAGTTTTCTTATGGGTTCCTTTTT"

echo $seq | wc -c ## seems like they are all 152 bp long. 

## why are these sequences so short, if they are miseq?

## seq numbers line up:
head ERR7255689_1.fastq 
head ERR7255689_2.fastq 

tail ERR7255689_1.fastq 
tail ERR7255689_2.fastq 

## these are really short. Did they already do some sort of trimming on them?
## fastqc

conda activate readQC_env

cd /vol/testNotSoBig/datasets/zymoMC/zymoQC

fastqc -o /vol/testNotSoBig/datasets/zymoMC/zymoQC/ \
    /vol/testNotSoBig/datasets/zymoMC/illumina/ERR7255689_*

scp -i /home/daniel/.ssh/funmic2023 -P 30427 \
    ubuntu@129.70.51.6:/vol/testNotSoBig/datasets/zymoMC/zymoQC/ERR7255689_1_fastqc.html .

(firefox ERR7255689_1_fastqc.html &) &

scp -i /home/daniel/.ssh/funmic2023 -P 30427 \
    ubuntu@129.70.51.6:/vol/testNotSoBig/datasets/zymoMC/zymoQC/ERR7255689_2_fastqc.html .

(firefox ERR7255689_2_fastqc.html &) &

## these look very good. A few illumina adapters in there...

## can these be aligned if they are so short?

conda activate readQC_env

## and the big one probably nanopore:
cd /vol/testNotSoBig/datasets/zymoMC/nanopore/

head ERR7255689_1.fastq

tail -n 200 ERR7255742.fastq | less

## how many reads?
grep -c ^@ ERR7255689_1.fastq ## 24887493
## 24,887,493 reads... interesting. let's see how many mags we 
## get out of that...

## how do we find which is the forward? reverse?
## meh, who cares. With so little overlap,
## seems best to just use them unpaired. 

conda activate vsearch 

vsearch --fastq_mergepairs ERR7255689_1.fastq --reverse ERR7255689_2.fastq --threads 20 --fastqout ERR7255689_paired.fasta 

## what does that look like?

conda activate readQC_env

fastqc -o /vol/testNotSoBig/datasets/zymoMC/zymoQC/ \
    /vol/testNotSoBig/datasets/zymoMC/illumina/ERR7255689_paired.fasta

file=/vol/testNotSoBig/datasets/zymoMC/zymoQC/ERR7255689_paired.fasta_fastqc.html

scp -i /home/daniel/.ssh/funmic2023 -P 30427 \
    ubuntu@129.70.51.6:$file .

(firefox ERR7255689_paired.fasta_fastqc.html &) &

head ERR7255689_paired.fasta

## only 8 million reads successfully merged? I think we have to not merge...
## the qualities of both R1 and R2 are high, can we use them without 
## merging?

## let's do it, for now. 

## how about visualizing the nanopore reads?

## we don't have a read report for the zymoMC nanopore reads, so don't 
## think we can use minionQC.

## so just run fastqc again...

conda activate readQC_env

cd /vol/danBot/datasets/zymoMC/zymoQC/nanopore/

less /vol/danBot/datasets/zymoMC/zymoQC/nanopore/nohup.out

nohup fastqc -t 20 -o /vol/danBot/datasets/zymoMC/zymoQC/nanopore/ \
    /vol/danBot/datasets/zymoMC/nanopore/ERR7287988.fastq  &

## get it local
file=/vol/danBot/datasets/zymoMC/zymoQC/nanopore/ERR7287988_fastqc.html
scp -i /home/daniel/.ssh/funmic2023 -P 30500 \
    ubuntu@129.70.51.6:$file .

(firefox ERR7287988_fastqc.html &) &

## wow, the 10.4 data is way way better.

## let's check the sludge nanopore:
cd /vol/danBot/datasets/sludge/nanopore
nohup fastqc -t 20 -o /vol/danBot/datasets/sludge/sludgeQC/nanopore \
    /vol/danBot/datasets/sludge/nanopore/ERR7014844.fastq  &> fastqc_sludgeNano.log

## get it local
file=/vol/danBot/datasets/sludge/sludgeQC/nanopore/ERR7014844_fastqc.html
scp -i /home/daniel/.ssh/funmic2023 -P 30500 \
    ubuntu@129.70.51.6:$file .

(firefox ERR7287988_fastqc.html &) &



## finding mention of nanoplot and nanostat
## what are these?

## before we go, see if we can get a quick output from nanoplot:

conda activate nanoplot

cd /vol/testNotSoBig/datasets/zymoMC/zymoQC/nanoplotOut

inFastq="/vol/testNotSoBig/datasets/zymoMC/nanopore/ERR7255742.fastq"
outDir="/vol/testNotSoBig/datasets/zymoMC/zymoQC/nanoplotOut/"

nohup NanoPlot -t 6 --verbose --store --huge -o $outDir --fastq $inFastq --format 'png' &

## maybe next time don't save the pickle, too big for github

## check it out:

tar -zcvf nanoplot.tar.gz /vol/testNotSoBig/datasets/zymoMC/zymoQC/nanoplotOut/

file=/vol/testNotSoBig/datasets/zymoMC/nanoplot.tar.gz
scp -i /home/daniel/.ssh/funmic2023 -P 30427 \
    ubuntu@129.70.51.6:$file .

## looks like we need to chop the adapters. Does porechop still work?

## for the barcode data I used cutadapt, previously. 

## tomorrow - do we need to chop adapters? try pore chop, and/or NanoFilt,  and/or cutadapt

## for now, since this is just teaching, let's use cutadapt.

conda activate readQC_env

cd /vol/testNotSoBig/datasets/zymoMC/nanopore/

## the simplest thing is to cut the first nine bp off these.

############################################################################
## debug time
## we know that cutadapt introduces empty reads some where in our pipe
conda deactivate
conda activate readQC_env
head -n 1000000 "/vol/testNotSoBig/datasets/zymoMC/nanopore/ERR7255742.fastq" > npHead.fastq
cutadapt -o ZymoNP_trimmed.fastq --cores 20 --cut 9 npHead.fastq 
grep "^$" npHead.fastq
cutadapt -o ZymoNP_trimmed_filteredEnds.fastq --cores 20 --cut 9 ZymoNP_trimmed.fastq
grep "^$" npHead.fastq

## not found? so where did the empty reads come from?

## meh, leave it alone
## flye claims they do not need qc on the nanopore reads,
## https://github.com/fenderglass/Flye/blob/flye/docs/USAGE.md

## so let's try it without:

conda deactivate

conda activate flye

## let's redo this, with the zymoMC 10.4 data
reads=/vol/danBot/datasets/zymoMC/nanopore/ERR7287988.fastq

cd /vol/danBot/assemblies/zymoMC/flyeNanopore

#nohup flye --meta \
#           --threads 25 \
#           --out-dir . \
#           --nano-hq $reads &> howlongdidIflye.log &

## and that blows out the memory. wow.
## the fly github repo says the following two settings 
## can be changed to lower memory requirements...but with not much further explananation.

--asm-coverage
--genome-size

## they say that 40x is usually enough coverage to make good initial "disjointig" assemblies
## so set to asm to 40?
## estimated genome size, I wonder if this applies to metagenomes?
## but we can't run without a genome size estimate
## do they want a metagenome size? or mag sizes?
## try metagenome size. Base estimate on our illumina metagenome size:

cont="/vol/danBot/assemblies/zymoMC/megahitZymoIllumina/final.contigs.fa"
wc -c $cont ## 41,514,822 characters, so maybe 41 million bp?

## ugh, asm not compatible with meta, So:

nohup flye --meta \
           --genome-size 41m \
           --threads 25 \
           --out-dir . \
           --nano-hq $reads &> howlongdidIflye.log &

## this will prbably break, but give it a shot

## if it doesn't work, try either random subsetting of reads
## or removal of small reads


wc -l $reads & ## fails. too big?

## taking a look at those reads, I see no justification in terms
## of quality for eleminating short reads.
## but they contain less info, I guess.

## how about a random subset 

## and, doesn't work. Time to subset. The first reads are presumably the best, 
## so...
## we know a file about the size of our r9.4 files worked with our cluster
## so if we can get down ~28 gb 

(28.2 / 48.1 ) 
## if read length is distributed randomly (it's not), 
## then we want to reduce this file by about half

## we know the R9.4 files had 8,851,918 reads; 31,995,546,765 basepairs; and 28.2 GB
## 10.4 -   18,831,686 reads; 53,221,766,826 basepairs; 48.1 GB

## wait, 48 gig is the listed file size on the ENA website for this file,
## but this is compressed size. I've deleted the r9.4 data, so I don't know 
## know what its uncompressed size was exactly.

## but it looks like the compression and the #bps scale about 1:1 here, so 
## assume we had ~1/2 the uncompressed file size.

## in general, it looks like the 10.4 data is about twice as large. 

## check the md5 -- 

cd /vol/danBot/datasets/zymoMC/nanopore

md5sum ERR7287988.fastq > nanoporechecksum.txt & ## 2c457962887c03afd89537e0a6cd4502

## this is different than the published md5 on the ena site, maybe they meant the compressed file?
## ena md5 = f40091ab244d98e9da48b561992a371e

md5sum ERR7287988.fastq.gz > nanoporechecksum.txt & ## yup, matches

## but we don't yet know how many reads are in the 10.4 nanopore dataset

## we can make the guess that if we subset to the same number of 
## reads that are in the 9.4 dataset, flye should be able to handle it. 
## We also want as much data

reads=/vol/danBot/datasets/zymoMC/nanopore/ERR7287988.fastq

cd /vol/danBot/datasets/zymoMC/nanopore/

head -n 35000000 $reads > nanopore10subsetted.fastq 

subsetReads=/vol/danBot/datasets/zymoMC/nanopore/nanopore10subsetted.fastq

## that seems to go right to sleep. Not sure why. 
## try split:

mkdir splitUpZymoNanopore

cd /vol/danBot/datasets/zymoMC/nanopore/splitUpZymoNanopore

nohup split -l 4000000 $reads &

## 19 files, 
## each should start with "@" and header info

## and end with qscores...

for i in *; do
  echo $i
  head -n 1 $i
  tail -n 1 $i
  echo "##################"
done

## looks good, and the last line of the last file looks like the last of our nanopore reads

## now try to combine. We want three combinations

## 1 - equal size to the last nanopore data that worked: ~50 gig
## 2 - halfway between this and the full size that failed, ~75 gig
## 3 - size that equals the illumina datasets, to make a fair comparison ~17.4 gig 

## not sure if we'll have time to test all three...

## to make the largest, 38 gig...
## I think I remember that quality drops over the course of a run, 
## so let's take from the oldest to newest files

4.1G xaa
4.8G xab
5.6G xac
5.8G xad
6.2G xae
6.2G xaf
6.3G xag
6.4G xah
6.5G xai
6.1G xaj
6.2G xak
5.9G xal
5.8G xam
5.5G xan
4.9G xao
4.4G xap
3.9G xaq
3.8G xar
2.8G xas


python3
import pandas as pd

filename=["xaa", "xab", "xac", "xad", "xae", "xaf", "xag", "xah", "xai", "xaj", "xak", "xal", "xam", "xan", "xao", "xap", "xaq", "xar", "xas"]
sizes = [4.1, 4.8, 5.6, 5.8, 6.2, 6.2, 6.3, 6.4, 6.5, 6.1, 6.2, 5.9, 5.8, 5.5, 4.9, 4.4, 3.9, 3.8, 2.8]
aa = pd.DataFrame({'file':filename, 'sizes':sizes})

aa.sizes.sum()

## we want the sizes to sum to ~50g, as a first attempt. This is about what a single minion flowcell can do, anway


aa.iloc[0:9,1].sum()  ## taking the first nine files gives us ~52 gig. Any less and we drop to 45 gig.


aa.iloc[0:9,0]

## so we want these files:
xaa
xab
xac
xad
xae
xaf
xag
xah
xai

cat xaa xab xac xad xae xaf xag xah xai > ERR7287988_shortened.fastq &

## flye command above edited to work with this file


############################################################################

conda deactivate
conda activate readQC_env

cd /vol/testNotSoBig/datasets/zymoMC/nanopore/

npData="/vol/testNotSoBig/datasets/zymoMC/nanopore/ERR7255742.fastq"
nohup cutadapt -o ZymoNP_trimmed.fastq --cores 25 --cut 9 $npData 

grep "^$" ZymoNP_trimmed.fastq ## check for empties


grep "^$" ZymoNP_trimmed_filteredEnds.fastq ## check for empties

## does flye like it?
conda deactivate
conda activate flye
reads=ZymoNP_trimmed.fastq
flye --meta \
     --threads 25 \
     --out-dir . \
     --nano-hq $reads 
## seems okay

conda deactivate
conda activate readQC_env

## cutadapt can also remove low quality ends, try this, also 

nohup cutadapt -q 10 \
  -o ZymoNP_trimmed_filteredEnds.fastq \
  --cores 25 \
  ZymoNP_trimmed.fastq &

grep -B 2 -A 2 "^$" ZymoNP_trimmed_filteredEnds.fastq ## check for empties
## there they are

conda deactivate
conda activate flye
reads=ZymoNP_trimmed_filteredEnds.fastq
flye --meta \
     --threads 25 \
     --out-dir . \
     --nano-hq $reads 

## yup. 
## fuck it, let's keep the "ragged edges".

## run fastqc on both
file=ZymoNP_trimmed.fastq
fastqc -t 10 \
  -o /vol/testNotSoBig/datasets/zymoMC/ \
  $file &> fastqcNPtrimmedLog.txt &


file=ZymoNP_trimmed_filteredEnds.fastq
fastqc -t 15 \
  -o /vol/testNotSoBig/datasets/zymoMC/ \
  $file &> fastqcNPtrimmedFilteredLog.txt &

## get them

file=/vol/testNotSoBig/datasets/zymoMC/zymoQC/nanopore/ZymoNP_trimmed_fastqc.html
scp -i /home/daniel/.ssh/funmic2023 -P 30427 \
    ubuntu@129.70.51.6:$file .

file=/vol/testNotSoBig/datasets/zymoMC/ZymoNP_trimmed_filteredEnds_fastqc.html
scp -i /home/daniel/.ssh/funmic2023 -P 30427 \
    ubuntu@129.70.51.6:$file .

(firefox ZymoNP_trimmed_fastqc.html &) &

(firefox ZymoNP_trimmed_filteredEnds_fastqc.html &) &

find . ZymoNP_trimmed_filteredEnds.fastq_fastqc.html

## that takes a long time, the fastqc generation.
## but both steps really helped. 

## just do fastqc once before on primary inspection of the nanopore reads
## and once after trimming and filtering.

## actually, just trimming. Filtering is screwing up our fasta 

## the illumina data seems find to me, except it doesn't pair well.
## can we still use it for assembly

### run assemblers! ###

## for the nanopore data, we'll use metaFlye 

## https://github.com/fenderglass/Flye

## for the illumina, we can choose between megahit and spades..

## both we (last year) and the Sereika project use megahit, so let's 
## stay with it.

conda activate megahit

fast1=/vol/danBot/datasets/zymoMC/illumina/ERR7255689_1.fastq
fast2=/vol/danBot/datasets/zymoMC/illumina/ERR7255689_2.fastq
outdir=/vol/danBot/assemblies/zymoMC/megahitZymoIllumina
\time -v megahit -1 $fast1 \
          -2 $fast2 \
          -t 25 \
          -o $outdir &> megahit.log &

## oops, forgot to nohup that

## not sure how well it went, but let's 
## that was really quick - N50 is 103233, not bad

## try to start the flye assembler, might take longer...

conda activate flye

ls /vol/testNotSoBig/datasets/zymoMC/nanopore/

cd /vol/testNotSoBig/assemblies/zymoMC/flyeNanopore/

#reads=/vol/testNotSoBig/datasets/zymoMC/nanopore/ZymoNP_trimmed_filteredEnds.fastq

reads=/vol/testNotSoBig/datasets/zymoMC/nanopore/ZymoNP_trimmed.fastq
outdir=/vol/testNotSoBig/assemblies/zymoMC/flyeNanopore/
\time -v nohup flye --meta \
          --threads 25 \
          --out-dir $outdir \
          --nano-hq $reads &> howlongDidIflye.log &

## oh crap. ran out of memory. Time to start a fat node.

##file=/vol/testNotSoBig/datasets/zymoMC/nanopore/ERR7255742.fastq
file=/vol/testNotSoBig/datasets/zymoMC/nanopore/ZymoNP_trimmed_filteredEnds.fastq

grep -B 6 -A 2 "@ERR7255742.1631 " $file

## are there others?
head -n 10000 $file | grep -B 2 -A 2 "^$"

## find these errors using empty lines
## can't acutally look for linbreaks with grep
grep -n -B 2 -A 2 "^$" $file > empties.txt &

grep -n -B 2 -A 2 "^$" $file ## not found when we look in the pre-cutadapt

## yeah, looks like a lot of empty reads after 

## just curious, do we get the same result from the raw file?
reads=/vol/testNotSoBig/datasets/zymoMC/nanopore/ZymoNP_trimmed_filteredEnds.fastq

### check out the sludge data ### 

cd /vol/testNotSoBig/datasets/sludge

gunzip IL-202001-1.fastq.gz &
gunzip IL-202001-2.fastq.gz &
gunzip ERR7014876.fastq.gz &

## sludge illumina

cd /vol/testNotSoBig/datasets/sludge/illumina

head -n 1 IL-202001-1.fastq

tail IL-202001-1.fastq

head -n 1 IL-202001-2.fastq

tail IL-202001-2.fastq

## why are these different?
tail -n 4 IL-202001-1.fastq | head -n 1
tail -n 4 IL-202001-2.fastq | head -n 1

## these do not look like paired read files to me

head -n 1000 IL-202001-1.fastq | less

## for instance:
grep -A 2 "16026:1992" IL-202001-1.fastq ## gives us:
@M00878:365:000000000-CVP5H:1:1102:16026:1992_1
@M00878:365:000000000-CVP5H:1:1102:16026:1992_2
## both in the same file


grep -A 2 "16026:1992" IL-202001-2.fastq ## nothing is found. 

grep -n "1102:16026:1992_" IL-202001-1.fastq &> findreadpairplace.txt ## gives us:
997:@M00878:365:000000000-CVP5H:1:1102:16026:1992_1
47092901:@M00878:365:000000000-CVP5H:1:1102:16026:1992_2

## how many reads are there?

grep -c ^@M00 IL-202001-1.fastq &> zoop ## 23545952 23,545,952

grep -c ^@M00 IL-202001-2.fastq &> zoop ## 23545952 23,545,952

## ugh, this is a fucking nature paper and I still have to clean
## data. 

tail IL-202001-1.fastq 

head -n 1000 IL-202001-2.fastq | less

## so looks like paired reads, but each file contains its own paired data?

## then why are they the exact same number of reads? That is weird.

## good explanation on illumina fastq sequence identifiers here: 
## https://support.illumina.com/help/BaseSpace_OLH_009008/Content/Source/Informatics/BS/FileFormat_FASTQ-files_swBS.htm

seq="CCCATTGCTTTTCATCACTTCAATTGTGTACCTTTGCTCTACGGTTAAATGGCTCATATTTTGCAACTTTTGGACGAGGACACAAAGTAAAGAGAATTTATCCATTTCAGGCGGGGGGACATTTTTGTCCCTTCGCTTGAAAAAAATTAATTCATGCCCTCAAAAAGTTGCATTTATTAGTT"
echo $seq | wc -c ## 183 BP, still short for miseq

## so if these are not paired, what are we looking at? 

## goals - fastqc all illumina, 

###### start over on new instance ####

## denbi has come through with the heavier (128 gb ram), GPU-equipped VMs.
## also, our medium instance (64gb) can't handle the full nanopore dataset
## assembly. So time to try the big guns.

## ssh for new instance:

ssh -p 30500 -i /home/daniel/.ssh/ubuntu_e ubuntu@129.70.51.6

## or just

danBot

## get anaconda on there:
wget https://repo.anaconda.com/archive/Anaconda3-2022.10-Linux-x86_64.sh

bash ./Anaconda3-2022.10-Linux-x86_64.sh

/vol/danBot/

## let's make it so danbot and funcomp can talk.

## on danbot, make a keypair:
ssh-keygen -f danbotFuncomp

chmod 600 danbotFuncomp
chmod 444 danbotFuncomp.pub

## put the pub key on funcomp authorized_keys
cat danbotFuncomp.pub

## try login:
ssh -vp 30427  -i /home/ubuntu/.ssh/danbotFuncomp ubuntu@129.70.51.6
## works

## what do we need?
## the raw data, get it all:

file=/vol/testNotSoBig/datasets/
nohup scp -i /home/ubuntu/.ssh/danbotFuncomp -P 30427 -r ubuntu@129.70.51.6:$file . &

## meh, something failed.
## in the end, detach old volume and attach here. 
## let's try our flye assembly again:

conda activate flye

cd /vol/danBot/assemblies/zymoMC/flyeNanopore/

## edited to use the shortened zymoMC reads, started 12:55
reads=/vol/danBot/datasets/zymoMC/nanopore/ERR7287988_shortened.fastq
outdir=/vol/danBot/assemblies/zymoMC/flyeNanopore/
\time -v nohup flye --meta \
          --threads 25 \
          --out-dir $outdir \
          --nano-hq $reads &> howlongDidIflye.log &
## still going (but mostly done?) at 17:00

## and let's get the assembly going for our nanopore sludge reads

conda deactivate 
conda activate flye

reads=/vol/danBot/datasets/sludge/nanopore/ERR7014844.fastq
outdir=/vol/danBot/assemblies/sludge/nanopore/
\time -v nohup flye --meta \
          --threads 25 \
          --out-dir $outdir \
          --nano-hq $reads &> howlongDidIflye.log &



########## assembly qc #############


## how is our illumina assembly
/vol/danBot/assemblies/zymoMC/flyeNanopore/assembly.fasta

## it's pretty small - 40M. 40,000,000 bytes

wc -c assembly.fasta &

## ecoli has  5.11174 mbp. This is a small metagenome.

## let's check them with metaquast?  

illuminaZymoAssembly=/vol/danBot/assemblies/zymoMC/megahitZymoIllumina/final.contigs.fa
illuminaZymoMetaquastOut=/vol/danBot/datasets/zymoMC/zymoQC/illumina/metaquast
nanoporeZymoAssembly=/vol/danBot/assemblies/zymoMC/flyeNanopore/assembly.fasta
nanoporeZymoMetaquastOut=/vol/danBot/datasets/zymoMC/zymoQC/nanopore/metaquast

conda activate metaquast

metaquast --help

metaquast -t 13 \
          -o $illuminaZymoMetaquastOut \
          $illuminaZymoAssembly 

## not much memory usage, but the downloads from ncbi take a while

metaquast -t 13 \
          -o $nanoporeZymoMetaquastOut \
          $nanoporeZymoAssembly &> nanoporeMetaquast.log &

## zip them up

cd /vol/danBot/datasets/zymoMC/zymoQC/illumina/
tar -czvf metaquastOutZymoIllumina.tar.gz metaquast/

cd /vol/danBot/datasets/zymoMC/zymoQC/nanopore

tar -czvf metaquastOutZymoNanopore.tar.gz metaquast/

## get these local
## put these somewhere not in the repo, they are big.

cd /home/daniel/Documents/teaching/functionalMicrobiomes/readQC/zymoQC/illumina/metaquast
file=/vol/danBot/datasets/zymoMC/zymoQC/illumina/metaquastOutZymoIllumina.tar.gz
scp -i /home/daniel/.ssh/funmic2023 -P 30500 -r ubuntu@129.70.51.6:$file . 

cd /home/daniel/Documents/teaching/functionalMicrobiomes/readQC/zymoQC/nanoporeQC/metaquast
file=/vol/danBot/datasets/zymoMC/zymoQC/nanopore/metaquastOutZymoNanopore.tar.gz
scp -i /home/daniel/.ssh/funmic2023 -P 30500 -r ubuntu@129.70.51.6:$file . 

cd /home/daniel/Documents/teaching/functionalMicrobiomes/readQC/zymoQC/illumina/metaquast


### polishing with Racon ###

## de.nbi has a working example here:
## https://denbi-nanopore-training-course.readthedocs.io/en/latest/polishing/medaka/Racon_1.html

conda activate racon

cd /vol/danBot/polish/zymoMCnano

## make alignment of reads to assembly

### binning ###

## the sereika paper used two of the same binning software packages
## as we used last year: metabat2 and maxbin2

## they polished the metagenome with Racon before binning.
## which means we need to learn about polishing...

## while we are waiting on medaka and maxbin2 installations,
## can we try our other to binning software out on the 
## illumina datasets?

## metabat

## we did this last year...

## metabat uses contig abundances
## they had a wrapper script to generate these
## from your raw reads, 

## I think we first had to do an alignment with bowtie and samtools

cd /vol/danBot/binning/zymoMC/illumina

conda activate alignmentTools 

cont="/vol/danBot/assemblies/zymoMC/megahitZymoIllumina/final.contigs.fa"

## do our raw reads for bowtie have to be in a single file?
## looks like we use -1 and -2 options

## step 1 make the bowtie index of the illimina read assembly

binWD="/vol/danBot/binning/zymoMC/illumina/illuminaBowTieIndex"
cont="/vol/danBot/assemblies/zymoMC/megahitZymoIllumina/final.contigs.fa"
reads1="/vol/danBot/datasets/zymoMC/illumina/ERR7255689_1.fastq"
reads2="/vol/danBot/datasets/zymoMC/illumina/ERR7255689_2.fastq"

bowtie2-build $cont $binWD/zymoIlluminaAssembly

## do the alignment
bowtie2 \
  -x $binWD/zymoIlluminaAssembly \
  -1 $reads1 -2 $reads2 \
  -S rawReads2Contigs.sam \
  --threads 25 \
  --local 

## started 9:45
## this takes a very long time, maybe 0.5 hour or more
## with 25 cores, 20 min

## sort it

samtools sort -l 1 \
    -@15 \
    -o rawReads2ContigsSorted.bam \
    -O BAM \
    rawReads2Contigs.sam
## 5 mins

    #-T /tmp/sortRR2C \ ## do we need this? doesn't seem like it

## now what? this is needed for metabat and concoct, I think 
## vamb uses minimap in their examples, though we could 
## probably figure out how to use this alignment file, I'm sure.

## try metabat...

### metabat ###

conda deactivate

conda activate metabat2


cd /vol/danBot/binning/zymoMC/illumina/metabat2

cont="/vol/danBot/assemblies/zymoMC/megahitZymoIllumina/final.contigs.fa"
bam="/vol/danBot/binning/zymoMC/illumina/illuminaBowTieIndex/rawReads2ContigsSorted.bam"

runMetaBat.sh $cont $bam ## quick, 5 min

## 14 bins... that seems about right. Since we're not inject the hiseq data into this.

### vamb ###

## this is new this year - how does vamb work? 

conda deactivate

### vamb ###

conda activate vamb 

cd /vol/danBot/binning/zymoMC/illumina/vamb

## they run a cataloging program on their assemblies. Not sure if I need to do this but...

cont="/vol/danBot/assemblies/zymoMC/megahitZymoIllumina/final.contigs.fa"
reads1="/vol/danBot/datasets/zymoMC/illumina/ERR7255689_1.fastq"
reads2="/vol/danBot/datasets/zymoMC/illumina/ERR7255689_2.fastq"

concatenate.py illumcatalogue.fna.gz $cont

## they use minimap2 in their examples

conda activate alignmentTools

minimap2 -d illumcatalogue.mmi illumcatalogue.fna.gz # make index, quick

## align reads to metagenome
minimap2 -t 8 -N 5 -ax sr illumcatalogue.mmi --split-prefix mmsplit $reads1 $reads2 | samtools view -F 3584 -b --threads 18 > illumReadsAligned2Contigs.bam
## maybe ten minutes
## use more cores.

## I wonder if we could use minimap instead of bowtie above? seems much faster, and samtools seems to 
## handle the outputs...

## run the binner:
conda deactivate 
conda activate vamb

#vamb --outdir vambOut --fasta illumcatalogue.fna.gz --bamfiles illumReadsAligned2Contigs.bam -p 16 -o C --minfasta 200000
## doesn't work...

#vamb --outdir vambOut --fasta illumcatalogue.fna.gz --bamfiles illumReadsAligned2Contigs.bam -p 16 -t 64 -o C --minfasta 200000
## also doesn't work...

## had to really drop the batch size down to far below what they recommend:
vamb --outdir vambOut --fasta illumcatalogue.fna.gz --bamfiles illumReadsAligned2Contigs.bam -p 16 -t 32 -o C --minfasta 200000
## 6 mins

ls /vol/danBot/binning/zymoMC/illumina/vamb/vambOut/bins | wc -l  ## 8 bins

## see here:
https://github.com/RasmussenLab/vamb/issues/59

## we may need to revert to a different binning software
## but it does claim to have found ~8 bins...

## conda maxbin really doesn't seem to be happening.
## can we install from source?
## nope.

## new strategy - use concoct, VAMB, metabat
##              - use the 10.4 data instead?
##              - which means rerun flye

### concoct ###

## indepth documentation here, for example:
## https://concoct.readthedocs.io/en/latest/scripts/cut_up_fasta.html

## cut_up_fasta.py original_contigs.fa -c 10000 -o 0 --merge_last -b contigs_10K.bed > contigs_10K.fa
## concoct_coverage_table.py contigs_10K.bed mapping/Sample*.sorted.bam > coverage_table.tsv
## concoct --composition_file contigs_10K.fa --coverage_file coverage_table.tsv -b concoct_output/
## merge_cutup_clustering.py concoct_output/clustering_gt1000.csv > concoct_output/clustering_merged.csv
## mkdir concoct_output/fasta_bins
## extract_fasta_bins.py original_contigs.fa concoct_output/clustering_merged.csv --output_path concoct_output/fasta_bins


## we'll work in our alignments folder
cd /vol/danBot/binning/zymoMC/illumina/illuminaBowTieIndex/

## necessary for step 2, generating the coverage table
conda deactivate
conda activate alignmentTools

samtools index rawReads2ContigsSorted.bam

conda activate concoct

cont="/vol/danBot/assemblies/zymoMC/megahitZymoIllumina/final.contigs.fa"
outputDir="/vol/danBot/binning/zymoMC/illumina/concoct"

cut_up_fasta.py $cont -c 10000 -o 0 --merge_last -b concoctContigs_10K.bed > concoctContigs_10K.fa
concoct_coverage_table.py concoctContigs_10K.bed rawReads2ContigsSorted.bam > coverage_table.tsv

## this takes maybe ten min?, started ~12:00
## the meat of it is here:

concoct \
  --composition_file concoctContigs_10K.fa \
  --coverage_file coverage_table.tsv \
  -t 25 \
  -b $outputDir


## get illumina reads through the binning and refining steps
## get nanopore polished?
## then bin/refine

cd $outputDir
merge_cutup_clustering.py clustering_gt1000.csv > clustering_merged.csv
mkdir fasta_bins
extract_fasta_bins.py $cont clustering_merged.csv --output_path fasta_bins/

## let's rename these, I think the raw numbers are causing problems
for i in *; do 
mv $i ${i/\.fa/_concat\.fa}
done

## this is here:
cd /vol/danBot/binning/zymoMC/illumina/concoct/fasta_bins_renamed


## 15 bins
## metabat had 14
## and VAMB had 8

## okay, the illumina data is ready for das tools

## but let's back up a bit. 
## don't really want to do the nanopore polishing right now,
## because we are running out of time and the class
## is full anyway. 

## which means we should use the 10.4 flow cell data, not the 9.4
## and we need to rerun flye, which takes forever. 

## before lunch.... 

## step one edit the data fetch above 
## restart flye on the zymo data

############# refining ###########

## try dastools on our illumina assembly

## das tools needs a contig-bin map of alignments
## one for each of the binning softwares we used

conda activate das_tool

Fasta_to_Contigs2Bin.sh

## odd, that file is not found in the conda installation
## can't find it anywhere in the anaconda environment directories, at least

## can we grab it out of the github repo?

cd /vol/danBot/refining/zymoMC

wget https://raw.githubusercontent.com/cmks/DAS_Tool/master/src/Fasta_to_Contig2Bin.sh
chmod 777 Fasta_to_Contig2Bin.sh

## our bins are here:
#concoctBins=/vol/danBot/binning/zymoMC/illumina/concoct/fasta_bins_renamed/ 
concoctBins=/vol/danBot/binning/zymoMC/illumina/concoct/fasta_bins/ 
metabatBins=/vol/danBot/binning/zymoMC/illumina/metabat2/final.contigs.fa.metabat-bins-20230222_091706/
vambBins=/vol/danBot/binning/zymoMC/illumina/vamb/vambOut/bins/

ls $concoctBins
ls $metabatBins
ls $vambBins

## concoct:
./Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $concoctBins \
    > concoct.contigs2bin.tsv

## I don't think that quite worked...
## let's keep just the first and last columns of that

paste <(cut -d " " -f 1 concoct.contigs2bin.tsv) <(cut -f 2 concoct.contigs2bin.tsv) > concoct.contigs2bin_edited.tsv

## move to the directory:
#mv concoct.contigs2bin.tsv $concoctBins

## metabat
./Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $metabatBins \
    > metabat.contigs2bin.tsv
#mv metabat.contigs2bin.tsv $metabatBins

## vamb
./Fasta_to_Contig2Bin.sh \
    -e fna \
    -i $vambBins \
    > vamb.contigs2bin.tsv
#mv vamb.contigs2bin.tsv $vambBins

## we need to cut the first three letters or so...
cut --complement -c 1-3 vamb.contigs2bin.tsv > vamb.contigs2bin_edited.tsv

cd /vol/danBot/refining/zymoMC

conda deactivate 

conda activate das_tool

## typos, probably

## adapted for conda

conda activate das_tool

DAS_Tool  -i concoct.contigs2bin_edited.tsv,metabat.contigs2bin.tsv,vamb.contigs2bin_edited.tsv \
    -l concoct,metabat,vamb \
    -c $cont \
    -t 25 \
    --write_bins \
    --write_bin_evals \
    -o DASToolRun1

## put them here:
ls /vol/danBot/refining/zymoMC/illumina/DASToolRun1_DASTool_bins

## and we're down to 5 mags. Check them out with checkM
## but first, we have to catch up the nanopore data...


### checkM ###

conda activate checkm

cd /vol/danBot/refining/zymoMC/illumina

## let's run checkm on our mags:

## first step - find the right branch of tol:
zymoIllBins=/vol/danBot/refining/zymoMC/illumina/DASToolRun1_DASTool_bins

checkm lineage_wf -t 22 -x fa $zymoIllBins checkMout

checkMout="/vol/danBot/refining/zymoMC/illumina/checkMout"

## this also generated the markers we need to do some quality analyses.
## we can look at this with checkm's qa function:

zymoIlluminaMarkers=/vol/danBot/refining/zymoMC/illumina/checkMout/lineage.ms

checkm qa $zymoIlluminaMarkers $checkMout > zymoIllCheckMout.txt

## great, get the nanopore data back on track, then try phylophlan on our MAGs.


### metaphlan ###

## let's look at our community using metaphlan, as a contrast to the metaquast
## tool, whihc is dependent on 16s recoovery, I think.

conda activate metaphlan

cd /vol/danBot/communityProfiles/zymoMC/illumina/metaphlan

metaphlanMarkerDB=/vol/danBot/metaphlanDB
seqs1=/vol/danBot/datasets/zymoMC/illumina/ERR7255689_1.fastq
seqs2=/vol/danBot/datasets/zymoMC/illumina/ERR7255689_2.fastq

## run metaphlan. Save the bowtie alignment in case we want to do it again..

metaphlan $seqs1,$seqs2 \
    --bowtie2db $metaphlanMarkerDB \
    --bowtie2out metaphlanBowtie2.ZymoIllu.bz2 \
    --nproc 20 \
    --input_type fastq \
    -o MetaPhlanProfiled_zymoIllu.txt 

## is there any way to quickly? we have this old script:


mphlanprofile=MetaPhlanProfiled_zymoIllu.txt 

head $mphlanprofile

#grep "s__" $mphlanprofile | sed  's/\([0-9]*[0-9]\.[0-9]*\).*$/\1/g' | sed 's/^.*s__//g' | cut -f 1,3
## too complicated but its neat, so save it... 

## this works better. 
mphlanprofile=MetaPhlanProfiled_zymoIllu.txt 
grep -v "t__" $mphlanprofile | grep "s__" | sed 's/^.*s__//g' | cut -f1,3 > zymoIlluMetaphlanAbundances.tsv

## if there is time, see visualizations for this? they use GraPhlan
## here:

https://github.com/biobakery/biobakery/wiki/metaphlan2#create-a-cladogram-with-graphlan


## they edit their abundance table in a similar way:

mphlanprofile=MetaPhlanProfiled_zymoIllu.txt 
grep -E "(s__)|(^ID)" $mphlanprofile | grep -v "t__" | sed 's/^.*s__//g' > merged_abundance_table_species.txt

## they use this script. I remember this not working last year:
export2graphlan.py --skip_rows 1,2 \
  -i merged_abundance_table.txt \
  --tree merged_abundance.tree.txt \
  --annotation merged_abundance.annot.txt \
  --most_abundant 100 \
  --abundance_threshold 1 \
  --least_biomarkers 10 \
  --annotations 5,6 \
  --external_annotations 7 \
  --min_clade_size 1

## but it's not installing. Wait to try this out, I think it is a dead end, and that I probably
## need to build my pipeline for tree visualization, don't have time for this.

### phylophlan ###

cd /vol/danBot/assignTax/zymoMC/illumina

conda activate phylophlan

MAGs=/vol/danBot/refining/zymoMC/illumina/DASToolRun1_DASTool_bins

## the newest database as implied by name is: CMG2122
## but that database sucks, I think is some project or tutorial specific thing.

phylophlan_metagenomic

\time -v phylophlan_metagenomic \
    -i $MAGs \
    -e fa \
    -d SGB.Jul20 \
    -o phylophlanOut_ZymoIllu \
    --nproc 2 \
    -n 1 \
    --verbose &> phylophlan23.log &

## and works like a charm. all five mags go right where they are supposed to.

## this takes a long time, just to download the databases..
## so, maybe add the database for the class ahead of time. 

## add processors if not running an assembly

## looks like they have a vis script:

## if the nanopore assembly doesn't give more mags, what do we do?

## well, need to start on the sludge data...we need to know that 
## they will have at least a few MAGs to work with.

## best to get the other nanopore assembly going tonight, so we can 
## run it through the pipeline quickly and make sure it is a viable 
## dataset

## so get download going when we have a core or two. 
## if the R10 data isn't enough, we'll have to try the R9 data

## or sick them on the mbarc data?

## looks like we have the right data. So set up the code for another flye assembly.
## also metabat?

## we have to understand the miseq data from the sludge - is it any good?
## or is it totally dependent on the hiseq reads?

## then we can run all three through the same pipeline?

## tonight, start the sludge nanopore assembly and fastqc

## this is repeated on line 612 above. delete when done:
cd /vol/danBot/datasets/sludge/nanopore
nohup fastqc -t 23 -o /vol/danBot/datasets/sludge/sludgeQC/nanopore \
    /vol/danBot/datasets/sludge/nanopore/ERR7014844.fastq  &> fastqc_sludgeNano.log

## get it local
file=/vol/danBot/datasets/sludge/sludgeQC/nanopore/ERR7014844_fastqc.html
scp -i /home/daniel/.ssh/funmic2023 -P 30500 \
    ubuntu@129.70.51.6:$file .


## this is repeated on line 1144 above. delete when done:
conda deactivate 
conda activate flye

reads=/vol/danBot/datasets/sludge/nanopore/ERR7014844.fastq
outdir=/vol/danBot/assemblies/sludge/nanopore/
\time -v nohup flye --meta \
          --threads 25 \
          --out-dir $outdir \
          --nano-hq $reads &> howlongDidIflye.log &

