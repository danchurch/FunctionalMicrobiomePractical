## okay, time to start setting up the 2023 funmic class


## mounting hard drive

## find it:
lsblk

## get uuid
blkid
## mount

sudo mount -U "c1feef14-014f-4cb0-9ee0-4db329308eab" /vol/danBot

## or
sudo mount /dev/vdc /vol/danBot


## to make sure everything is kosher for github:
find . -type f -size +40M

## or newer than 

find . -type f -newermt '5 minutes ago'

## also, conda broke when we updated a cloud config file.

## reinstall:

wget https://repo.anaconda.com/archive/Anaconda3-2022.10-Linux-x86_64.sh

bash ./Anaconda3-2022.10-Linux-x86_64.sh

## put anaconda here:
/vol/testNotSoBig/anaconda

## first steps, today:

## 0 - clean out the old folders, github repo
## done
## 1 - setup the instance for development
## instance initiated, I think? Not showing on my dashboard...

## 2 - get data sets

## maybe let's check out the Sereika/Albertson data and pipeline:
https://www.nature.com/articles/s41592-022-01539-7#data-availability

## their repo is here:
https://github.com/Serka-M/Digester-MultiSequencing

## their pipeline is really similar to the one we used last year,
## though no biobakery tools were used, just gtdb-tk. 

## I like the biobakery tools, let's decide when we get there...

## they have both mock and environmental data

## the mock DNA is from the zymogen mock community. 
## the eDNA is from activated sludge from an anaerobic sewage
## treatment plant. I don't really understand, because I
## thought "activated" implied the injection of oxygen
## into sewage material...
## intentionally anaerobic conditions implies they wanted
## methane production.
## not sure. Anyway...

## They have illumina reads, pacbio, and nanopore sequence data
## The nanopore data is of two flowcell generations, R10.4 and R9.4.1

## data for the mock community are here:
https://www.ebi.ac.uk/ena/browser/view/PRJEB48692

## in both cases, there are two nanopore platforms on there...
## do we want their minion data?

## quote: 
## "Anaerobic digester and Zymo R.9.4.1 datasets were generated on a MinION Mk1B (Oxford Nanopore Technologies) device"

## so we want the minion datasets

## in the case of the zymo data, I think the nanopore would be:
## sample name = SAMEA10644976 , library name = LIB-Zymo_HMW_R941 
## found here:
https://www.ebi.ac.uk/ena/browser/view/ERR7255742

## note there are promethion data for the R10.4 in the next entry, 

## 
## we won't use the promethion R10.4 data

## I think we are going to need a way to do this without a gui...

## for instance, does this work for the Nanopore Zymo data?

wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR725/002/ERR7255742/ERR7255742.fastq.gz
## looks like it works...so theoretically these are the files we
## need:

## zymo files (illumina miseq F+R, Nanopore minion):
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR725/009/ERR7255689/ERR7255689_1.fastq.gz
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR725/009/ERR7255689/ERR7255689_2.fastq.gz
#wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR725/002/ERR7255742/ERR7255742.fastq.gz ## that's 9.4 flowcell
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR728/008/ERR7287988/ERR7287988.fastq.gz ## 10.4, but promethion!

## biodigestor files (illumina miseq F+R, Nanopore minion):
## here they have two minion runs. One seems to be for a R10.3 cell.
## So again, stick with the R9.4.1 cell..
## also, there are two sets of illumina miseq files, one set from 2018,
## and one from 2020. The 2020 set doesn't have a finished "generated" fastq
## from the ENA folks. Supp. table 4 seems important here...

## all the nanopore data in this folder is from minions...

wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR701/004/ERR7014844/ERR7014844.fastq.gz ## 10.4
#wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR701/006/ERR7014876/ERR7014876.fastq.gz ## 9.4 flowcell
wget ftp://ftp.sra.ebi.ac.uk/vol1/run/ERR701/ERR7015307/IL-202001-1.fastq.gz
wget ftp://ftp.sra.ebi.ac.uk/vol1/run/ERR701/ERR7015307/IL-202001-2.fastq.gz

## can't find any further data. These last two are the submitted 2020 miseq 
## files, maybe they will work, if not try the 2018 files

## data for the digester are here:
https://www.ebi.ac.uk/ena/browser/view/PRJEB48021

## let's use the R9.4.1 data for the nanopore data
## this is the one that benefitted from polishing 
## with illumina data

## We'll try assembly with illumina alone, nanopore alone, and then 
## nanopore polished by illumina

## weird, though, they used 9 years of illumina data to 
## assist binning...hmm... wonder if we can skip that in the class.

## run through these for the zymogen together, then set them loose on
## the bioreactor data

## they don't have fast5 files, I don't think...
## should we practise basecalling with another dataset?

## tomorrow, start running through the datasets.
## this paper is a bit misleading. The title should actually say something
## like "Near perfect nanopore MAGs without illumina polishing, but with nine years
## of illumina data to help binning"....

## oh well. Let's see how we do. If we have to, go back to mbarc and chu. 


##### set up VM #####

## de.NBI big VM is not working, everytime I try to set up a full, GPU+ 128g RAM, 28 core
## machine, it stalls out.

## So I started up a smaller machine, no GPU, 64g, 28 cores. 

## seems to be running. Do I need to give it a new key?
## they have some on file for me...

ssh -vp 30481  -i /home/daniel/.ssh/ubuntu_e ubuntu@129.70.51.6

## looks like I deleted this key?

grep -R "3NAYJHfS2K3z5DrOM"

## yup. Can we sync up the keys that we used in the spruce project?
## that also seems to not be working...

## let's start over. We are not storing keys on the nanocomp,
## the only old key we need is the one for the emic instance
## that zhe is using.
## also github...

## it looks like our "ubuntu" keys are neither...
## take a chance and delete them.

## can we still use githhub?...yes
## emikAdmin? nope. Just killed it. 

## fix:
chmod 600 ubuntu_e

## plain old emik? yeah, still works

## okay, so those keys are only good for emik stuff.
## can we use our old id_ed keys? I think these are 
## what we use for github...

## so let's make some new ones...

man ssh-keygen

ssh-keygen -f funmic2023 

chmod 600 funmic2023

ssh -p 30427  -i /home/daniel/.ssh/funmic2023 ubuntu@129.70.51.6

## okay, that works. remember not to change keys anymore.
## de.nbi won't update the keys on VMs - they are stuck with 
## the original user-profile public keys when the VM is started

## great, now get our data:

cd /vol/testNotSoBig

sudo chown ubuntu: /vol/testNotSoBig

mkdir datasets

mkdir zymoMC

cd /vol/testNotSoBig/zymoMC
## zymo files (illumina miseq F+R, Nanopore minion):
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR725/009/ERR7255689/ERR7255689_1.fastq.gz
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR725/009/ERR7255689/ERR7255689_2.fastq.gz
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR725/002/ERR7255742/ERR7255742.fastq.gz

cd /vol/testNotSoBig
mkdir sludge

cd /vol/testNotSoBig/sludge
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR701/006/ERR7014876/ERR7014876.fastq.gz &
wget ftp://ftp.sra.ebi.ac.uk/vol1/run/ERR701/ERR7015307/IL-202001-1.fastq.gz &
wget ftp://ftp.sra.ebi.ac.uk/vol1/run/ERR701/ERR7015307/IL-202001-2.fastq.gz &

## next step check out the reads:

## everyone, including the authors of this paper, used porechop
## which is pretty much dead. 

## this package was pretty 

## for the illumina reads, we just use old fashioned fastqc

## which means we need to get anaconda going... 

wget https://repo.anaconda.com/archive/Anaconda3-2022.10-Linux-x86_64.sh

## installed

#### set up conda packages ###

## add bioconda
conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge
conda config --set channel_priority strict

## for updates
conda update -n base -c defaults conda

## readQC
conda create -n readQC_env -c bioconda cutadapt fastqc bbmap seqtk

### vsearch ###
conda create -n vsearch -c bioconda vsearch

### nanoplot/nanostat ###
## got to relax things a bit
conda config --set channel_priority flexible
conda create -n nanoplot -c bioconda nanoplot
conda config --set channel_priority strict

### megahit ###
conda create -n megahit -c bioconda megahit

### Flye ###
conda create -n flye -c bioconda flye

### (meta)quast ###
conda config --set channel_priority flexible
conda create -n metaquast -c bioconda quast
conda config --set channel_priority strict

### polishing ###
## and should probably put medaka and racon in the same 
#conda config --set channel_priority flexible
#conda create -n polishNanopore -c bioconda racon minimap2 medaka
#conda config --set channel_priority strict
### racon ###
#conda create -n racon -c bioconda medaka minimap2 

## that's not working. Break it up.

### medaka ###
#conda config --set channel_priority flexible
#conda create -n medaka -c bioconda medaka 

# ## recommended by the nanopore github site:
# conda create -n medaka -c conda-forge -c bioconda medaka
# #conda config --set channel_priority strict
# 
# ## that's run for ~24 hours
# 
# ## try pip in a virtual env
# python3 -m venv /path/to/new/virtual/environment
# virtualenv medaka --python=python3 --prompt "(medaka) "
# . medaka/bin/activate
# pip install medaka
# ## doesn't work, some sort of issue with compiler
# 
# ## still problems with virtualenv, won't install...jeezus
# 
# git clone https://github.com/nanoporetech/medaka.git
# cd medaka
# make install
# . ./venv/bin/activate
# ## also doesn't work
#
## maybe just a straight pip install.

conda create -n medaka 
conda activate medaka 
sudo apt install python3-pip

pip install medaka

## errors:
checking for bzlib.h... no
checking for lzma.h... no

## bzlib.h
sudo apt install libbz2-dev
## lzma.h
sudo apt install liblzma-dev
## think it works now...

### racon ###
conda create -n racon -c bioconda racon 
## quick

### samtools, bedtools, bedtools ###
conda create -n alignmentTools -c bioconda samtools=1.9 bedtools bowtie2 minimap2

conda activate alignmentTools

conda install seqtk

## can we add minimap2 to this?
## rename this to something nicer for the classk

### binning environments ###

## metaBat2 ##
conda create -n metabat2 -c bioconda metabat2
## quick

# ### maxbin2 ###
# #conda create -n maxbin2 -c bioconda maxbin2
# ## takes a long,long time
# 
# conda create -n maxbin2 -c bioconda maxbin2
# 
# ## try downloading the source from sourceforge:
# https://sourceforge.net/projects/maxbin2/
# 
# ## had to get it local
# file=/home/daniel/Documents/teaching/functionalMicrobiomes/MaxBin-2.2.7.tar.gz
# scp -i /home/daniel/.ssh/funmic2023 -P 30500 -r $file ubuntu@129.70.51.6:/vol/danBot/
# ## and back on danBot
# 
# tar -xvf MaxBin-2.2.7.tar.gz
# 
# conda create -n maxbin2 
# 
# conda activate maxbin2
# 
# conda install -c conda-forge perl
# 
# ## quick install in the readme says:
# ## 1. Download MaxBin and unzip it
# ## 2. Enter src directory under MaxBin and "make" it.
# ## 3. Run "./autobuild_auxiliary" at MaxBin directory to download, compile,
# ##    and setup the auxiliary software packages
# ## 4. MaxBin should be ready to go.
# 
# ## hopefully we can follow 
# ## https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#setting-environment-variables
# ## to add these settings
# ## https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#setting-environment-variables
# ## need perl in this environment
# 
# ## ugh, this is fucked...try concoct instead

### concoct ###

# ## following their github instructions:
# conda config --add channels defaults
# conda config --add channels bioconda
# conda config --add channels conda-forge
# conda create -n concoct python=3 concoct
# 
# ## is this necessary?
# conda install -c conda-forge scikit-learn

## this install isn't working. documentation here:
https://github.com/BinPro/CONCOCT/issues/321

## their yaml for concoct looks like thisL
###############
name: concoct
channels:
  - conda-forge
  - bioconda
dependencies:
  - concoct=1.1.0
  - libopenblas=*=openmp*
  - mkl
  - python>=3
  - samtools>=1.9
  - scikit-learn=1.1.*
variables:
  USE_OPENMP: 1
######

## let's try it:

conda deactivate
conda env remove -n concoct
conda env create -f concoct.yml



## then reset to bioconda priorities above

## VAMB ##
conda create -n vamb -c bioconda vamb
## looks like GPU support is available:
conda activate vamb
conda install -c pytorch pytorch torchvision cudatoolkit=10.2
## even with the cuda install, it's quick

## also needs minimap:
conda install -c bioconda minimap2

## das tools ##

conda config --set channel_priority flexible
conda create -n das_tool -c bioconda das_tool

conda config --set channel_priority strict
## that seems to install ok, but won't work 

## try a manual install?:
conda create -n das_tool

conda activate das_tool
cd /vol/danBot/refining/
git clone https://github.com/cmks/DAS_Tool.git
cd DAS_Tool
unzip db.zip

conda install r-data.table r-magrittr r-docopt
conda install -c bioconda prodigal diamond pullseq ruby

### checkm ###
conda create -n checkm -c bioconda checkm-genome

### MetaPhlan ###
conda create --n metaphlan -c conda-forge -c bioconda python=3.7 metaphlan
conda activate metaphlan
## put the db on the volumne, so we can clone later
metaphlan --install --bowtie2db /vol/danBot/metaphlanDB

### phyloPhlan ###
conda create --name phylophlan -c bioconda phylophlan


### check out the zymo data: ###

mkdir /vol/testNotSoBig/datasets/zymoMC/illumina
mkdir /vol/testNotSoBig/datasets/zymoMC/nanopore

gunzip ERR7255689_1.fastq.gz &
gunzip ERR7255689_2.fastq.gz &
gunzip ERR7255742.fastq.gz &

## the 1+2 files probably = illumina

cd /vol/testNotSoBig/datasets/zymoMC/illumina

head ERR7255689_1.fastq

tail ERR7255689_1.fastq

grep "10003:13052/" ERR7255689_1.fastq

grep "10003:13052/" ERR7255689_2.fastq

seq="GGTGACCACTCGCCCGCCCGCCGCGTTCTCTTCGCTCACGGCCAGGGCGAACATGTTAATCCAGTCGATGACGTTCATCGGATCGCGGGAGAGATTATCGCTGGAGACCAGCAGACGCCGTAACGCTACCGCGCGGCGTGGAACGTTCAGC"
seq="TTTGCATCAAAAGAAGCCCTATTTTTAGAAGTTTATCAAGATAGTATTCAGATGGAATTAACAGAACTAGGGAAAGTAGCAGAGCGAGATGATTTGGTTGGGGAAAAGAAGCTACAATCTATTTTCTTTGTAGCGACAGATTTTTCTAGCA"
seq="ACAATGCGATCAATAATGATTTCAATAGAATGCTTTTTATTCTTCTCGATTTCAATTTCGTCATTGATATCATAAATTTCTCCATCAACACGAATTCGAACATAGCCTTCTTTTTTTATTTCCTCAATAGTTTTCTTATGGGTTCCTTTTT"

echo $seq | wc -c ## seems like they are all 152 bp long. 

## why are these sequences so short, if they are miseq?

## seq numbers line up:
head ERR7255689_1.fastq 
head ERR7255689_2.fastq 

tail ERR7255689_1.fastq 
tail ERR7255689_2.fastq 

## these are really short. Did they already do some sort of trimming on them?
## fastqc

conda activate readQC_env

cd /vol/testNotSoBig/datasets/zymoMC/zymoQC

fastqc -o /vol/testNotSoBig/datasets/zymoMC/zymoQC/ \
    /vol/testNotSoBig/datasets/zymoMC/illumina/ERR7255689_*

scp -i /home/daniel/.ssh/funmic2023 -P 30427 \
    ubuntu@129.70.51.6:/vol/testNotSoBig/datasets/zymoMC/zymoQC/ERR7255689_1_fastqc.html .

(firefox ERR7255689_1_fastqc.html &) &

scp -i /home/daniel/.ssh/funmic2023 -P 30427 \
    ubuntu@129.70.51.6:/vol/testNotSoBig/datasets/zymoMC/zymoQC/ERR7255689_2_fastqc.html .

(firefox ERR7255689_2_fastqc.html &) &

## these look very good. A few illumina adapters in there...

## can these be aligned if they are so short?

conda activate readQC_env

## and the big one probably nanopore:
cd /vol/testNotSoBig/datasets/zymoMC/nanopore/

head ERR7255689_1.fastq

tail -n 200 ERR7255742.fastq | less

## how many reads?
grep -c ^@ ERR7255689_1.fastq ## 24887493
## 24,887,493 reads... interesting. let's see how many mags we 
## get out of that...

## how do we find which is the forward? reverse?
## meh, who cares. With so little overlap,
## seems best to just use them unpaired. 

conda activate vsearch 

vsearch --fastq_mergepairs ERR7255689_1.fastq --reverse ERR7255689_2.fastq --threads 20 --fastqout ERR7255689_paired.fasta 

## what does that look like?

conda activate readQC_env

fastqc -o /vol/testNotSoBig/datasets/zymoMC/zymoQC/ \
    /vol/testNotSoBig/datasets/zymoMC/illumina/ERR7255689_paired.fasta

file=/vol/testNotSoBig/datasets/zymoMC/zymoQC/ERR7255689_paired.fasta_fastqc.html

scp -i /home/daniel/.ssh/funmic2023 -P 30427 \
    ubuntu@129.70.51.6:$file .

(firefox ERR7255689_paired.fasta_fastqc.html &) &

head ERR7255689_paired.fasta

## only 8 million reads successfully merged? I think we have to not merge...
## the qualities of both R1 and R2 are high, can we use them without 
## merging?

## let's do it, for now. 

## how about visualizing the nanopore reads?

## we don't have a read report for the nanopore reads, so don't 
## think we can use minionQC.

## so just run fastqc again...

conda activate readQC_env

cd /vol/danBot/datasets/zymoMC/zymoQC/nanopore/

less /vol/danBot/datasets/zymoMC/zymoQC/nanopore/nohup.out

nohup fastqc -t 20 -o /vol/danBot/datasets/zymoMC/zymoQC/nanopore/ \
    /vol/danBot/datasets/zymoMC/nanopore/ERR7287988.fastq  &

file=/vol/danBot/datasets/zymoMC/zymoQC/nanopore/ERR7287988_fastqc.html
scp -i /home/daniel/.ssh/funmic2023 -P 30427 \
    ubuntu@129.70.51.6:$file .

(firefox ERR7255689_paired.fasta_fastqc.html &) &

## finding mention of nanoplot and nanostat
## what are these?

## before we go, see if we can get a quick output from nanoplot:

conda activate nanoplot

cd /vol/testNotSoBig/datasets/zymoMC/zymoQC/nanoplotOut

inFastq="/vol/testNotSoBig/datasets/zymoMC/nanopore/ERR7255742.fastq"
outDir="/vol/testNotSoBig/datasets/zymoMC/zymoQC/nanoplotOut/"

nohup NanoPlot -t 6 --verbose --store --huge -o $outDir --fastq $inFastq --format 'png' &

## maybe next time don't save the pickle, too big for github

## check it out:

tar -zcvf nanoplot.tar.gz /vol/testNotSoBig/datasets/zymoMC/zymoQC/nanoplotOut/

file=/vol/testNotSoBig/datasets/zymoMC/nanoplot.tar.gz
scp -i /home/daniel/.ssh/funmic2023 -P 30427 \
    ubuntu@129.70.51.6:$file .

## looks like we need to chop the adapters. Does porechop still work?

## for the barcode data I used cutadapt, previously. 

## tomorrow - do we need to chop adapters? try pore chop, and/or NanoFilt,  and/or cutadapt

## for now, since this is just teaching, let's use cutadapt.

conda activate readQC_env

cd /vol/testNotSoBig/datasets/zymoMC/nanopore/

## the simplest thing is to cut the first nine bp off these.

############################################################################
## debug time
## we know that cutadapt introduces empty reads some where in our pipe
conda deactivate
conda activate readQC_env
head -n 1000000 "/vol/testNotSoBig/datasets/zymoMC/nanopore/ERR7255742.fastq" > npHead.fastq
cutadapt -o ZymoNP_trimmed.fastq --cores 20 --cut 9 npHead.fastq 
grep "^$" npHead.fastq
cutadapt -o ZymoNP_trimmed_filteredEnds.fastq --cores 20 --cut 9 ZymoNP_trimmed.fastq
grep "^$" npHead.fastq

## not found? so where did the empty reads come from?

## meh, leave it alone
## flye claims they do not need qc on the nanopore reads,
## https://github.com/fenderglass/Flye/blob/flye/docs/USAGE.md

## so let's try it without:

conda deactivate

conda activate flye

## let's redo this, with the zymoMC 10.4 data
reads=/vol/danBot/datasets/zymoMC/nanopore/ERR7287988.fastq

cd /vol/danBot/assemblies/zymoMC/flyeNanopore

#nohup flye --meta \
#           --threads 25 \
#           --out-dir . \
#           --nano-hq $reads &> howlongdidIflye.log &

## and that blows out the memory. wow.
## the fly github repo says the following two settings 
## can be changed to lower memory requirements...but with not much further explananation.

--asm-coverage
--genome-size

## they say that 40x is usually enough coverage to make good initial "disjointig" assemblies
## so set to asm to 40?
## estimated genome size, I wonder if this applies to metagenomes?
## but we can't run without a genome size estimate
## do they want a metagenome size? or mag sizes?
## try metagenome size. Base estimate on our illumina metagenome size:

cont="/vol/danBot/assemblies/zymoMC/megahitZymoIllumina/final.contigs.fa"
wc -c $cont ## 41,514,822 characters, so maybe 41 million bp?

## ugh, asm not compatible with meta, So:

nohup flye --meta \
           --genome-size 41m \
           --threads 25 \
           --out-dir . \
           --nano-hq $reads &> howlongdidIflye.log &

## this will prbably break, but give it a shot

## if it doesn't work, try either random subsetting of reads
## or removal of small reads


wc -l $reads & ## fails. too big?

## taking a look at those reads, I see no justification in terms
## of quality for eleminating short reads.
## but they contain less info, I guess.

## how about a random subset 

## and, doesn't work. Time to subset. The first reads are presumably the best, 
## so...
## we know a file about the size of our r9.4 files worked with our cluster
## so if we can get down ~28 gb 

(28.2 / 48.1 ) 
## if read length is distributed randomly (it's not), 
## then we want to reduce this file by about half

## we know the R9.4 files had 8,851,918 reads; 31,995,546,765 basepairs; and 28.2 GB
## 10.4 - ??? reads; 53,221,766,826 basepairs; 48.1 GB

## but we don't yet know how many reads are in the 10.4 nanopore dataset

## we can make the guess that if we subset to the same number of 
## reads that are in the 9.4 dataset, flye should be able to handle it. 
## We also want as much data

reads=/vol/danBot/datasets/zymoMC/nanopore/ERR7287988.fastq

cd /vol/danBot/datasets/zymoMC/nanopore/

head -n 35000000 $reads > nanopore10subsetted.fastq 

subsetReads=/vol/danBot/datasets/zymoMC/nanopore/nanopore10subsetted.fastq

## that seems to go right to sleep. Not sure why. 
## try split:

mkdir splitUpZymoNanopore

cd /vol/danBot/datasets/zymoMC/nanopore/splitUpZymoNanopore

nohup split -l 4000000 $reads &

nohup flye --meta \
           --genome-size 41m \
           --threads 25 \
           --out-dir . \
           --nano-hq $reads &> howlongdidIflye.log &

## if this work, maybe try a higher number of reads?
############################################################################

conda deactivate
conda activate readQC_env

cd /vol/testNotSoBig/datasets/zymoMC/nanopore/

npData="/vol/testNotSoBig/datasets/zymoMC/nanopore/ERR7255742.fastq"
nohup cutadapt -o ZymoNP_trimmed.fastq --cores 25 --cut 9 $npData 

grep "^$" ZymoNP_trimmed.fastq ## check for empties


grep "^$" ZymoNP_trimmed_filteredEnds.fastq ## check for empties

## does flye like it?
conda deactivate
conda activate flye
reads=ZymoNP_trimmed.fastq
flye --meta \
     --threads 25 \
     --out-dir . \
     --nano-hq $reads 
## seems okay

conda deactivate
conda activate readQC_env

## cutadapt can also remove low quality ends, try this, also 

nohup cutadapt -q 10 \
  -o ZymoNP_trimmed_filteredEnds.fastq \
  --cores 25 \
  ZymoNP_trimmed.fastq &

grep -B 2 -A 2 "^$" ZymoNP_trimmed_filteredEnds.fastq ## check for empties
## there they are

conda deactivate
conda activate flye
reads=ZymoNP_trimmed_filteredEnds.fastq
flye --meta \
     --threads 25 \
     --out-dir . \
     --nano-hq $reads 

## yup. 
## fuck it, let's keep the "ragged edges".

## run fastqc on both
file=ZymoNP_trimmed.fastq
fastqc -t 10 \
  -o /vol/testNotSoBig/datasets/zymoMC/ \
  $file &> fastqcNPtrimmedLog.txt &


file=ZymoNP_trimmed_filteredEnds.fastq
fastqc -t 15 \
  -o /vol/testNotSoBig/datasets/zymoMC/ \
  $file &> fastqcNPtrimmedFilteredLog.txt &

## get them

file=/vol/testNotSoBig/datasets/zymoMC/zymoQC/nanopore/ZymoNP_trimmed_fastqc.html
scp -i /home/daniel/.ssh/funmic2023 -P 30427 \
    ubuntu@129.70.51.6:$file .

file=/vol/testNotSoBig/datasets/zymoMC/ZymoNP_trimmed_filteredEnds_fastqc.html
scp -i /home/daniel/.ssh/funmic2023 -P 30427 \
    ubuntu@129.70.51.6:$file .

(firefox ZymoNP_trimmed_fastqc.html &) &

(firefox ZymoNP_trimmed_filteredEnds_fastqc.html &) &

find . ZymoNP_trimmed_filteredEnds.fastq_fastqc.html

## that takes a long time, the fastqc generation.
## but both steps really helped. 

## just do fastqc once before on primary inspection of the nanopore reads
## and once after trimming and filtering.

## actually, just trimming. Filtering is screwing up our fasta 

## the illumina data seems find to me, except it doesn't pair well.
## can we still use it for assembly

### run assemblers! ###

## for the nanopore data, we'll use metaFlye 

## https://github.com/fenderglass/Flye

## for the illumina, we can choose between megahit and spades..

## both we (last year) and the Sereika project use megahit, so let's 
## stay with it.

conda activate megahit

fast1=/vol/danBot/datasets/zymoMC/illumina/ERR7255689_1.fastq
fast2=/vol/danBot/datasets/zymoMC/illumina/ERR7255689_2.fastq
outdir=/vol/danBot/assemblies/zymoMC/megahitZymoIllumina
\time -v megahit -1 $fast1 \
          -2 $fast2 \
          -t 25 \
          -o $outdir &> megahit.log &

## oops, forgot to nohup that

## not sure how well it went, but let's 
## that was really quick - N50 is 103233, not bad

## try to start the flye assembler, might take longer...

conda activate flye

ls /vol/testNotSoBig/datasets/zymoMC/nanopore/

cd /vol/testNotSoBig/assemblies/zymoMC/flyeNanopore/

#reads=/vol/testNotSoBig/datasets/zymoMC/nanopore/ZymoNP_trimmed_filteredEnds.fastq

reads=/vol/testNotSoBig/datasets/zymoMC/nanopore/ZymoNP_trimmed.fastq
outdir=/vol/testNotSoBig/assemblies/zymoMC/flyeNanopore/
\time -v nohup flye --meta \
          --threads 25 \
          --out-dir $outdir \
          --nano-hq $reads &> howlongDidIflye.log &

## oh crap. ran out of memory. Time to start a fat node.

##file=/vol/testNotSoBig/datasets/zymoMC/nanopore/ERR7255742.fastq
file=/vol/testNotSoBig/datasets/zymoMC/nanopore/ZymoNP_trimmed_filteredEnds.fastq

grep -B 6 -A 2 "@ERR7255742.1631 " $file

## are there others?
head -n 10000 $file | grep -B 2 -A 2 "^$"

## find these errors using empty lines
## can't acutally look for linbreaks with grep
grep -n -B 2 -A 2 "^$" $file > empties.txt &

grep -n -B 2 -A 2 "^$" $file ## not found when we look in the pre-cutadapt

## yeah, looks like a lot of empty reads after 

## just curious, do we get the same result from the raw file?
reads=/vol/testNotSoBig/datasets/zymoMC/nanopore/ZymoNP_trimmed_filteredEnds.fastq

### check out the sludge data ### 

cd /vol/testNotSoBig/datasets/sludge

gunzip IL-202001-1.fastq.gz &
gunzip IL-202001-2.fastq.gz &
gunzip ERR7014876.fastq.gz &

## sludge illumina

cd /vol/testNotSoBig/datasets/sludge/illumina

head -n 1 IL-202001-1.fastq

tail IL-202001-1.fastq

head -n 1 IL-202001-2.fastq

tail IL-202001-2.fastq

## why are these different?
tail -n 4 IL-202001-1.fastq | head -n 1
tail -n 4 IL-202001-2.fastq | head -n 1

## these do not look like paired read files to me

head -n 1000 IL-202001-1.fastq | less

## for instance:
grep -A 2 "16026:1992" IL-202001-1.fastq ## gives us:
@M00878:365:000000000-CVP5H:1:1102:16026:1992_1
@M00878:365:000000000-CVP5H:1:1102:16026:1992_2
## both in the same file


grep -A 2 "16026:1992" IL-202001-2.fastq ## nothing is found. 

grep -n "1102:16026:1992_" IL-202001-1.fastq &> findreadpairplace.txt ## gives us:
997:@M00878:365:000000000-CVP5H:1:1102:16026:1992_1
47092901:@M00878:365:000000000-CVP5H:1:1102:16026:1992_2

## how many reads are there?

grep -c ^@M00 IL-202001-1.fastq &> zoop ## 23545952 23,545,952

grep -c ^@M00 IL-202001-2.fastq &> zoop ## 23545952 23,545,952

## ugh, this is a fucking nature paper and I still have to clean
## data. 

tail IL-202001-1.fastq 

head -n 1000 IL-202001-2.fastq | less

## so looks like paired reads, but each file contains its own paired data?

## then why are they the exact same number of reads? That is weird.

## good explanation on illumina fastq sequence identifiers here: 
## https://support.illumina.com/help/BaseSpace_OLH_009008/Content/Source/Informatics/BS/FileFormat_FASTQ-files_swBS.htm

seq="CCCATTGCTTTTCATCACTTCAATTGTGTACCTTTGCTCTACGGTTAAATGGCTCATATTTTGCAACTTTTGGACGAGGACACAAAGTAAAGAGAATTTATCCATTTCAGGCGGGGGGACATTTTTGTCCCTTCGCTTGAAAAAAATTAATTCATGCCCTCAAAAAGTTGCATTTATTAGTT"
echo $seq | wc -c ## 183 BP, still short for miseq

## so if these are not paired, what are we looking at? 

## goals - fastqc all illumina, 

###### start over on new instance ####

## denbi has come through with the heavier (128 gb ram), GPU-equipped VMs.
## also, our medium instance (64gb) can't handle the full nanopore dataset
## assembly. So time to try the big guns.

## ssh for new instance:

ssh -p 30500 -i /home/daniel/.ssh/ubuntu_e ubuntu@129.70.51.6

## or just

danBot

## get anaconda on there:
wget https://repo.anaconda.com/archive/Anaconda3-2022.10-Linux-x86_64.sh

bash ./Anaconda3-2022.10-Linux-x86_64.sh

/vol/danBot/

## let's make it so danbot and funcomp can talk.

## on danbot, make a keypair:
ssh-keygen -f danbotFuncomp

chmod 600 danbotFuncomp
chmod 444 danbotFuncomp.pub

## put the pub key on funcomp authorized_keys
cat danbotFuncomp.pub

## try login:
ssh -vp 30427  -i /home/ubuntu/.ssh/danbotFuncomp ubuntu@129.70.51.6
## works

## what do we need?
## the raw data, get it all:

file=/vol/testNotSoBig/datasets/
nohup scp -i /home/ubuntu/.ssh/danbotFuncomp -P 30427 -r ubuntu@129.70.51.6:$file . &

## meh, something failed.
## in the end, detach old volume and attach here. 
## let's try our flye assembly again:

conda activate flye

reads=/vol/danBot/datasets/zymoMC/nanopore/ZymoNP_trimmed.fastq
outdir=/vol/danBot/assemblies/zymoMC/flyeNanopore/
\time -v nohup flye --meta \
          --threads 25 \
          --out-dir $outdir \
          --nano-hq $reads &> howlongDidIflye.log &

## anything else we can do right now?

## we can try the binning software, but this requires a lot of memory...think we are stuck for a bit

## 

## how is our illumina assembly
/vol/danBot/assemblies/zymoMC/flyeNanopore/assembly.fasta

## it's pretty small - 40M. 40,000,000 bytes

wc -c assembly.fasta &

## ecoli has  5.11174 mbp. This is a small metagenome.

## let's check them with metaquast?  


illuminaZymoAssembly=/vol/danBot/assemblies/zymoMC/megahitZymoIllumina/final.contigs.fa
illuminaZymoMetaquastOut=/vol/danBot/datasets/zymoMC/zymoQC/illumina/metaquast
nanoporeZymoAssembly=/vol/danBot/assemblies/zymoMC/flyeNanopore/assembly.fasta
nanoporeZymoMetaquastOut=/vol/danBot/datasets/zymoMC/zymoQC/nanopore/metaquast

conda activate metaquast

metaquast --help

metaquast -t 13 \
          -o $illuminaZymoMetaquastOut \
          $illuminaZymoAssembly 

## not much memory usage, but the downloads from ncbi take a while

metaquast -t 13 \
          -o $nanoporeZymoMetaquastOut \
          $nanoporeZymoAssembly &> nanoporeMetaquast.log &

## zip them up

cd /vol/danBot/datasets/zymoMC/zymoQC/illumina/
tar -czvf metaquastOutZymoIllumina.tar.gz metaquast/

cd /vol/danBot/datasets/zymoMC/zymoQC/nanopore

tar -czvf metaquastOutZymoNanopore.tar.gz metaquast/

## get these local
## put these somewhere not in the repo, they are big.

cd /home/daniel/Documents/teaching/functionalMicrobiomes/readQC/zymoQC/illumina/metaquast
file=/vol/danBot/datasets/zymoMC/zymoQC/illumina/metaquastOutZymoIllumina.tar.gz
scp -i /home/daniel/.ssh/funmic2023 -P 30500 -r ubuntu@129.70.51.6:$file . 

cd /home/daniel/Documents/teaching/functionalMicrobiomes/readQC/zymoQC/nanoporeQC/metaquast
file=/vol/danBot/datasets/zymoMC/zymoQC/nanopore/metaquastOutZymoNanopore.tar.gz
scp -i /home/daniel/.ssh/funmic2023 -P 30500 -r ubuntu@129.70.51.6:$file . 

cd /home/daniel/Documents/teaching/functionalMicrobiomes/readQC/zymoQC/illumina/metaquast


### polishing with Racon ###

## de.nbi has a working example here:
## https://denbi-nanopore-training-course.readthedocs.io/en/latest/polishing/medaka/Racon_1.html

conda activate racon

cd /vol/danBot/polish/zymoMCnano

## make alignment of reads to assembly

### binning ###

## the sereika paper used two of the same binning software packages
## as we used last year: metabat2 and maxbin2

## they polished the metagenome with Racon before binning.
## which means we need to learn about polishing...

## while we are waiting on medaka and maxbin2 installations,
## can we try our other to binning software out on the 
## illumina datasets?

## metabat

## we did this last year...

## metabat uses contig abundances
## they had a wrapper script to generate these
## from your raw reads, 

## I think we first had to do an alignment with bowtie and samtools

cd /vol/danBot/binning/zymoMC/illumina

conda activate alignmentTools 

cont="/vol/danBot/assemblies/zymoMC/megahitZymoIllumina/final.contigs.fa"

## do our raw reads for bowtie have to be in a single file?
## looks like we use -1 and -2 options

## step 1 make the bowtie index of the illimina read assembly

binWD="/vol/danBot/binning/zymoMC/illumina/illuminaBowTieIndex"
cont="/vol/danBot/assemblies/zymoMC/megahitZymoIllumina/final.contigs.fa"
reads1="/vol/danBot/datasets/zymoMC/illumina/ERR7255689_1.fastq"
reads2="/vol/danBot/datasets/zymoMC/illumina/ERR7255689_2.fastq"

bowtie2-build $cont $binWD/zymoIlluminaAssembly

## do the alignment
bowtie2 \
  -x $binWD/zymoIlluminaAssembly \
  -1 $reads1 -2 $reads2 \
  -S rawReads2Contigs.sam \
  --threads 25 \
  --local 

## started 9:45
## this takes a very long time, maybe 0.5 hour or more
## with 25 cores, 20 min

## sort it

samtools sort -l 1 \
    -@15 \
    -o rawReads2ContigsSorted.bam \
    -O BAM \
    rawReads2Contigs.sam
## 5 mins

    #-T /tmp/sortRR2C \ ## do we need this? doesn't seem like it

## now what? this is needed for metabat and concoct, I think 
## vamb uses minimap in their examples, though we could 
## probably figure out how to use this alignment file, I'm sure.

## try metabat...

### metabat ###

conda deactivate

conda activate metabat2


cd /vol/danBot/binning/zymoMC/illumina/metabat2

cont="/vol/danBot/assemblies/zymoMC/megahitZymoIllumina/final.contigs.fa"
bam="/vol/danBot/binning/zymoMC/illumina/illuminaBowTieIndex/rawReads2ContigsSorted.bam"

runMetaBat.sh $cont $bam ## quick, 5 min

## 14 bins... that seems about right. Since we're not inject the hiseq data into this.

### vamb ###

## this is new this year - how does vamb work? 

conda deactivate

### vamb ###

conda activate vamb 

cd /vol/danBot/binning/zymoMC/illumina/vamb

## they run a cataloging program on their assemblies. Not sure if I need to do this but...

cont="/vol/danBot/assemblies/zymoMC/megahitZymoIllumina/final.contigs.fa"
reads1="/vol/danBot/datasets/zymoMC/illumina/ERR7255689_1.fastq"
reads2="/vol/danBot/datasets/zymoMC/illumina/ERR7255689_2.fastq"

concatenate.py illumcatalogue.fna.gz $cont

## they use minimap2 in their examples

conda activate alignmentTools

minimap2 -d illumcatalogue.mmi illumcatalogue.fna.gz # make index, quick

## align reads to metagenome
minimap2 -t 8 -N 5 -ax sr illumcatalogue.mmi --split-prefix mmsplit $reads1 $reads2 | samtools view -F 3584 -b --threads 18 > illumReadsAligned2Contigs.bam
## maybe ten minutes
## use more cores.

## I wonder if we could use minimap instead of bowtie above? seems much faster, and samtools seems to 
## handle the outputs...

## run the binner:
conda deactivate 
conda activate vamb

#vamb --outdir vambOut --fasta illumcatalogue.fna.gz --bamfiles illumReadsAligned2Contigs.bam -p 16 -o C --minfasta 200000
## doesn't work...

#vamb --outdir vambOut --fasta illumcatalogue.fna.gz --bamfiles illumReadsAligned2Contigs.bam -p 16 -t 64 -o C --minfasta 200000
## also doesn't work...

## had to really drop the batch size down to far below what they recommend:
vamb --outdir vambOut --fasta illumcatalogue.fna.gz --bamfiles illumReadsAligned2Contigs.bam -p 16 -t 32 -o C --minfasta 200000
## 6 mins

ls /vol/danBot/binning/zymoMC/illumina/vamb/vambOut/bins | wc -l  ## 8 bins

## see here:
https://github.com/RasmussenLab/vamb/issues/59

## we may need to revert to a different binning software
## but it does claim to have found ~8 bins...

## conda maxbin really doesn't seem to be happening.
## can we install from source?
## nope.

## new strategy - use concoct, VAMB, metabat
##              - use the 10.4 data instead?
##              - which means rerun flye

### concoct ###

## indepth documentation here, for example:
## https://concoct.readthedocs.io/en/latest/scripts/cut_up_fasta.html

## cut_up_fasta.py original_contigs.fa -c 10000 -o 0 --merge_last -b contigs_10K.bed > contigs_10K.fa
## concoct_coverage_table.py contigs_10K.bed mapping/Sample*.sorted.bam > coverage_table.tsv
## concoct --composition_file contigs_10K.fa --coverage_file coverage_table.tsv -b concoct_output/
## merge_cutup_clustering.py concoct_output/clustering_gt1000.csv > concoct_output/clustering_merged.csv
## mkdir concoct_output/fasta_bins
## extract_fasta_bins.py original_contigs.fa concoct_output/clustering_merged.csv --output_path concoct_output/fasta_bins


## we'll work in our alignments folder
cd /vol/danBot/binning/zymoMC/illumina/illuminaBowTieIndex/

## necessary for step 2, generating the coverage table
conda deactivate
conda activate alignmentTools

samtools index rawReads2ContigsSorted.bam

conda activate concoct

cont="/vol/danBot/assemblies/zymoMC/megahitZymoIllumina/final.contigs.fa"
outputDir="/vol/danBot/binning/zymoMC/illumina/concoct"

cut_up_fasta.py $cont -c 10000 -o 0 --merge_last -b concoctContigs_10K.bed > concoctContigs_10K.fa
concoct_coverage_table.py concoctContigs_10K.bed rawReads2ContigsSorted.bam > coverage_table.tsv

## this takes maybe ten min?, started ~12:00
## the meat of it is here:

concoct \
  --composition_file concoctContigs_10K.fa \
  --coverage_file coverage_table.tsv \
  -t 25 \
  -b $outputDir


## get illumina reads through the binning and refining steps
## get nanopore polished?
## then bin/refine

cd $outputDir
merge_cutup_clustering.py clustering_gt1000.csv > clustering_merged.csv
mkdir fasta_bins
extract_fasta_bins.py $cont clustering_merged.csv --output_path fasta_bins/

## let's rename these, I think the raw numbers are causing problems
for i in *; do 
mv $i ${i/\.fa/_concat\.fa}
done

## this is here:
cd /vol/danBot/binning/zymoMC/illumina/concoct/fasta_bins_renamed


## 15 bins
## metabat had 14
## and VAMB had 8

## okay, the illumina data is ready for das tools

## but let's back up a bit. 
## don't really want to do the nanopore polishing right now,
## because we are running out of time and the class
## is full anyway. 

## which means we should use the 10.4 flow cell data, not the 9.4
## and we need to rerun flye, which takes forever. 

## before lunch.... 

## step one edit the data fetch above 
## restart flye on the zymo data

############# refining ###########

## try dastools on our illumina assembly

## das tools needs a contig-bin map of alignments
## one for each of the binning softwares we used

conda activate das_tool

Fasta_to_Contigs2Bin.sh

## odd, that file is not found in the conda installation
## can't find it anywhere in the anaconda environment directories, at least

## can we grab it out of the github repo?

cd /vol/danBot/refining/zymoMC

wget https://raw.githubusercontent.com/cmks/DAS_Tool/master/src/Fasta_to_Contig2Bin.sh
chmod 777 Fasta_to_Contig2Bin.sh

## our bins are here:
#concoctBins=/vol/danBot/binning/zymoMC/illumina/concoct/fasta_bins_renamed/ 
concoctBins=/vol/danBot/binning/zymoMC/illumina/concoct/fasta_bins/ 
metabatBins=/vol/danBot/binning/zymoMC/illumina/metabat2/final.contigs.fa.metabat-bins-20230222_091706/
vambBins=/vol/danBot/binning/zymoMC/illumina/vamb/vambOut/bins/

ls $concoctBins
ls $metabatBins
ls $vambBins

## concoct:
./Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $concoctBins \
    > concoct.contigs2bin.tsv

## I don't think that quite worked...
## let's keep just the first and last columns of that

paste <(cut -d " " -f 1 concoct.contigs2bin.tsv) <(cut -f 2 concoct.contigs2bin.tsv) > concoct.contigs2bin_edited.tsv

## move to the directory:
#mv concoct.contigs2bin.tsv $concoctBins

## metabat
./Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $metabatBins \
    > metabat.contigs2bin.tsv
#mv metabat.contigs2bin.tsv $metabatBins

## vamb
./Fasta_to_Contig2Bin.sh \
    -e fna \
    -i $vambBins \
    > vamb.contigs2bin.tsv
#mv vamb.contigs2bin.tsv $vambBins

## we need to cut the first three letters or so...
cut --complement -c 1-3 vamb.contigs2bin.tsv > vamb.contigs2bin_edited.tsv

cd /vol/danBot/refining/zymoMC

conda deactivate 

conda activate das_tool

## typos, probably

## adapted for conda

conda activate das_tool

DAS_Tool  -i concoct.contigs2bin_edited.tsv,metabat.contigs2bin.tsv,vamb.contigs2bin_edited.tsv \
    -l concoct,metabat,vamb \
    -c $cont \
    -t 25 \
    --write_bins \
    --write_bin_evals \
    -o DASToolRun1

## put them here:
ls /vol/danBot/refining/zymoMC/illumina/DASToolRun1_DASTool_bins

## and we're down to 5 mags. Check them out with checkM
## but first, we have to catch up the nanopore data...


### checkM ###

conda activate checkm

cd /vol/danBot/refining/zymoMC/illumina

## let's run checkm on our mags:

## first step - find the right branch of tol:
zymoIllBins=/vol/danBot/refining/zymoMC/illumina/DASToolRun1_DASTool_bins

checkm lineage_wf -t 22 -x fa $zymoIllBins checkMout

checkMout="/vol/danBot/refining/zymoMC/illumina/checkMout"

## this also generated the markers we need to do some quality analyses.
## we can look at this with checkm's qa function:

zymoIlluminaMarkers=/vol/danBot/refining/zymoMC/illumina/checkMout/lineage.ms

checkm qa $zymoIlluminaMarkers $checkMout > zymoIllCheckMout.txt

## great, get the nanopore data back on track, then try phylophlan on our MAGs.


### metaphlan ###

## let's look at our community using metaphlan, as a contrast to the metaquast
## tool, whihc is dependent on 16s recoovery, I think.

conda activate metaphlan

metaphlanMarkerDB=/vol/studentFunMic1Vol/metaphlanMarkerDB
MBARCraw=/vol/studentFunMic1Vol/sequenceData/mbarc/smallerMBARC_trimmed.fq

mkdir -p /vol/studentFunMic1Vol/metaphlanWD/mbarcMPA
cd /vol/studentFunMic1Vol/metaphlanWD/mbarcMPA

metaphlan $MBARCraw \
    --bowtie2db $metaphlanMarkerDB \
    --nproc 6 \
    --input_type fastq \
    -o mbarcMetaPhlanProfiled_metagenome.txt \



## what to do tonight, if possible:

## download the fastqc file of new nanopore
## if split worked, put a few chunks together and 
## try to start the nanopore assembly with this.
