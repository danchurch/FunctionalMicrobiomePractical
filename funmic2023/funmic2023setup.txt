## okay, time to start setting up the 2023 funmic class

### evaluation ###

## look for this email:
10.01.2023
## link is here:
https://eval.uni-bayreuth.de/unizensus/b/2SNX5DIRRZZ3I?0

## email title is:
Zugang zur Lehrevaluation Ihrer Veranstaltung Funktionelle Mikrobiomforschung

## mounting hard drive

## find it:
lsblk

## get uuid
blkid
## mount

sudo mount -U "c1feef14-014f-4cb0-9ee0-4db329308eab" /vol/danBot

## or
sudo mount /dev/vdc /vol/danBot


## to make sure everything is kosher for github:
find . -type f -size +40M

## or newer than 

find . -type f -newermt '5 minutes ago'

## also, conda broke when we updated a cloud config file.

## reinstall:

wget https://repo.anaconda.com/archive/Anaconda3-2022.10-Linux-x86_64.sh

bash ./Anaconda3-2022.10-Linux-x86_64.sh

## put anaconda here:
/vol/testNotSoBig/anaconda

## first steps, today:

## 0 - clean out the old folders, github repo
## done
## 1 - setup the instance for development
## instance initiated, I think? Not showing on my dashboard...

## 2 - get data sets

## maybe let's check out the Sereika/Albertson data and pipeline:
https://www.nature.com/articles/s41592-022-01539-7#data-availability

## their repo is here:
https://github.com/Serka-M/Digester-MultiSequencing

## their pipeline is really similar to the one we used last year,
## though no biobakery tools were used, just gtdb-tk. 

## I like the biobakery tools, let's decide when we get there...

## they have both mock and environmental data

## the mock DNA is from the zymogen mock community. 
## the eDNA is from activated sludge from an anaerobic sewage
## treatment plant. I don't really understand, because I
## thought "activated" implied the injection of oxygen
## into sewage material...
## intentionally anaerobic conditions implies they wanted
## methane production.
## not sure. Anyway...

## They have illumina reads, pacbio, and nanopore sequence data
## The nanopore data is of two flowcell generations, R10.4 and R9.4.1

## data for the mock community are here:
https://www.ebi.ac.uk/ena/browser/view/PRJEB48692

## in both cases, there are two nanopore platforms on there...
## do we want their minion data?

## quote: 
## "Anaerobic digester and Zymo R.9.4.1 datasets were generated on a MinION Mk1B (Oxford Nanopore Technologies) device"

## so we want the minion datasets

## in the case of the zymo data, I think the nanopore would be:
## sample name = SAMEA10644976 , library name = LIB-Zymo_HMW_R941 
## found here:
https://www.ebi.ac.uk/ena/browser/view/ERR7255742

## note there are promethion data for the R10.4 in the next entry, 

## 
## we won't use the promethion R10.4 data

## I think we are going to need a way to do this without a gui...

## for instance, does this work for the Nanopore Zymo data?

wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR725/002/ERR7255742/ERR7255742.fastq.gz
## looks like it works...so theoretically these are the files we
## need:

## zymo files (illumina miseq F+R, Nanopore minion):
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR725/009/ERR7255689/ERR7255689_1.fastq.gz
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR725/009/ERR7255689/ERR7255689_2.fastq.gz
#wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR725/002/ERR7255742/ERR7255742.fastq.gz ## that's 9.4 flowcell
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR728/008/ERR7287988/ERR7287988.fastq.gz ## 10.4, but promethion!

## biodigestor files (illumina miseq F+R, Nanopore minion):
## here they have two minion runs. One seems to be for a R10.3 cell.
## So again, stick with the R9.4.1 cell..
## also, there are two sets of illumina miseq files, one set from 2018,
## and one from 2020. The 2020 set doesn't have a finished "generated" fastq
## from the ENA folks. Supp. table 4 seems important here...

## all the nanopore data in this folder is from minions...

wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR701/004/ERR7014844/ERR7014844.fastq.gz ## 10.4
#wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR701/006/ERR7014876/ERR7014876.fastq.gz ## 9.4 flowcell

## illumina miseq data, sludge
wget ftp://ftp.sra.ebi.ac.uk/vol1/run/ERR701/ERR7015307/IL-202001-1.fastq.gz

wget ftp://ftp.sra.ebi.ac.uk/vol1/run/ERR701/ERR7015307/IL-202001-2.fastq.gz

## can't find any further data. These last two are the submitted 2020 miseq 
## files, maybe they will work, if not try the 2018 files

## data for the digester are here:
https://www.ebi.ac.uk/ena/browser/view/PRJEB48021

## let's use the R9.4.1 data for the nanopore data
## this is the one that benefitted from polishing 
## with illumina data

## We'll try assembly with illumina alone, nanopore alone, and then 
## nanopore polished by illumina

## weird, though, they used 9 years of illumina data to 
## assist binning...hmm... wonder if we can skip that in the class.

## run through these for the zymogen together, then set them loose on
## the bioreactor data

## they don't have fast5 files, I don't think...
## should we practise basecalling with another dataset?

## tomorrow, start running through the datasets.
## this paper is a bit misleading. The title should actually say something
## like "Near perfect nanopore MAGs without illumina polishing, but with nine years
## of illumina data to help binning"....

## oh well. Let's see how we do. If we have to, go back to mbarc and chu. 


##### set up VM #####

## de.NBI big VM is not working, everytime I try to set up a full, GPU+ 128g RAM, 28 core
## machine, it stalls out.

## So I started up a smaller machine, no GPU, 64g, 28 cores. 

## seems to be running. Do I need to give it a new key?
## they have some on file for me...

ssh -vp 30481  -i /home/daniel/.ssh/ubuntu_e ubuntu@129.70.51.6

## looks like I deleted this key?

grep -R "3NAYJHfS2K3z5DrOM"

## yup. Can we sync up the keys that we used in the spruce project?
## that also seems to not be working...

## let's start over. We are not storing keys on the nanocomp,
## the only old key we need is the one for the emic instance
## that zhe is using.
## also github...

## it looks like our "ubuntu" keys are neither...
## take a chance and delete them.

## can we still use githhub?...yes
## emikAdmin? nope. Just killed it. 

## fix:
chmod 600 ubuntu_e

## plain old emik? yeah, still works

## okay, so those keys are only good for emik stuff.
## can we use our old id_ed keys? I think these are 
## what we use for github...

## so let's make some new ones...

man ssh-keygen

ssh-keygen -f funmic2023 

chmod 600 funmic2023

ssh -p 30427  -i /home/daniel/.ssh/funmic2023 ubuntu@129.70.51.6

## okay, that works. remember not to change keys anymore.
## de.nbi won't update the keys on VMs - they are stuck with 
## the original user-profile public keys when the VM is started

## great, now get our data:

cd /vol/testNotSoBig

sudo chown ubuntu: /vol/testNotSoBig

mkdir datasets

mkdir zymoMC

cd /vol/testNotSoBig/zymoMC
## zymo files (illumina miseq F+R, Nanopore 10.4):
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR725/009/ERR7255689/ERR7255689_1.fastq.gz
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR725/009/ERR7255689/ERR7255689_2.fastq.gz
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR728/008/ERR7287988/ERR7287988.fastq.gz


cd /vol/testNotSoBig
mkdir sludge

cd /vol/testNotSoBig/sludge
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR701/006/ERR7014876/ERR7014876.fastq.gz &
wget ftp://ftp.sra.ebi.ac.uk/vol1/run/ERR701/ERR7015307/IL-202001-1.fastq.gz &
wget ftp://ftp.sra.ebi.ac.uk/vol1/run/ERR701/ERR7015307/IL-202001-2.fastq.gz &

## next step check out the reads:

## everyone, including the authors of this paper, used porechop
## which is pretty much dead. 

## this package was pretty 

## for the illumina reads, we just use old fashioned fastqc

## which means we need to get anaconda going... 

/vol/danBot

wget https://repo.anaconda.com/archive/Anaconda3-2022.10-Linux-x86_64.sh

## installed

#### set up conda packages ###

## add bioconda
conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge
conda config --set channel_priority strict

## for updates
conda update -n base -c defaults conda

## readQC
#conda create -n readQC_env -c bioconda cutadapt fastqc bbmap seqtk
## oops, this name is better
##conda rename -n readQC_env readQC

conda create -n readQC -c bioconda cutadapt fastqc bbmap seqtk

#conda create -n readQCnv -c bioconda cutadapt fastqc bbmap seqtk

### vsearch ###
conda create -n vsearch -c bioconda vsearch

### nanoplot/nanostat ###
## got to relax things a bit
conda config --set channel_priority flexible
conda create -n nanoplot -c bioconda nanoplot
conda config --set channel_priority strict

### megahit ###
conda create -n megahit -c bioconda megahit

### Flye ###
conda create -n flye -c bioconda flye

### (meta)quast ###
conda config --set channel_priority flexible
conda create -n metaquast -c bioconda quast
conda config --set channel_priority strict

### polishing ###
## and should probably put medaka and racon in the same 
#conda config --set channel_priority flexible
#conda create -n polishNanopore -c bioconda racon minimap2 medaka
#conda config --set channel_priority strict
### racon ###
#conda create -n racon -c bioconda medaka minimap2 

## that's not working. Break it up.

### medaka ###
#conda config --set channel_priority flexible
#conda create -n medaka -c bioconda medaka 

# ## recommended by the nanopore github site:
# conda create -n medaka -c conda-forge -c bioconda medaka
# #conda config --set channel_priority strict
# 
# ## that's run for ~24 hours
# 
# ## try pip in a virtual env
# python3 -m venv /path/to/new/virtual/environment
# virtualenv medaka --python=python3 --prompt "(medaka) "
# . medaka/bin/activate
# pip install medaka
# ## doesn't work, some sort of issue with compiler
# 
# ## still problems with virtualenv, won't install...jeezus
# 
# git clone https://github.com/nanoporetech/medaka.git
# cd medaka
# make install
# . ./venv/bin/activate
# ## also doesn't work
#
## maybe just a straight pip install.

conda create -n medaka 
conda activate medaka 
sudo apt install python3-pip

pip install medaka

## errors:
checking for bzlib.h... no
checking for lzma.h... no

## bzlib.h
sudo apt install libbz2-dev
## lzma.h
sudo apt install liblzma-dev
## think it works now...

### racon ###
conda create -n racon -c bioconda racon 
## quick

### samtools, bedtools, bedtools ###
conda create -n alignmentTools -c bioconda samtools=1.9 bedtools bowtie2 minimap2 seqtk

conda activate alignmentTools

#conda install seqtk

## can we add minimap2 to this?
## rename this to something nicer for the classk

### binning environments ###

## metaBat2 ##
conda create -n metabat2 -c bioconda metabat2

## quick

# ### maxbin2 ###
# #conda create -n maxbin2 -c bioconda maxbin2
# ## takes a long,long time
# 
# conda create -n maxbin2 -c bioconda maxbin2
# 
# ## try downloading the source from sourceforge:
# https://sourceforge.net/projects/maxbin2/
# 
# ## had to get it local
# file=/home/daniel/Documents/teaching/functionalMicrobiomes/MaxBin-2.2.7.tar.gz
# scp -i /home/daniel/.ssh/funmic2023 -P 30500 -r $file ubuntu@129.70.51.6:/vol/danBot/
# ## and back on danBot
# 
# tar -xvf MaxBin-2.2.7.tar.gz
# 
# conda create -n maxbin2 
# 
# conda activate maxbin2
# 
# conda install -c conda-forge perl
# 
# ## quick install in the readme says:
# ## 1. Download MaxBin and unzip it
# ## 2. Enter src directory under MaxBin and "make" it.
# ## 3. Run "./autobuild_auxiliary" at MaxBin directory to download, compile,
# ##    and setup the auxiliary software packages
# ## 4. MaxBin should be ready to go.
# 
# ## hopefully we can follow 
# ## https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#setting-environment-variables
# ## to add these settings
# ## https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#setting-environment-variables
# ## need perl in this environment
# 
# ## ugh, this is fucked...try concoct instead

### concoct ###

# ## following their github instructions:
# conda config --add channels defaults
# conda config --add channels bioconda
# conda config --add channels conda-forge
# conda create -n concoct python=3 concoct
# 
# ## is this necessary?
# conda install -c conda-forge scikit-learn

## this install isn't working. documentation here:
https://github.com/BinPro/CONCOCT/issues/321

## their yaml for concoct looks like thisL
###############
name: concoct
channels:
  - conda-forge
  - bioconda
dependencies:
  - concoct=1.1.0
  - libopenblas=*=openmp*
  - mkl
  - python>=3
  - samtools>=1.9
  - scikit-learn=1.1.*
variables:
  USE_OPENMP: 1
######

## let's try it:

conda deactivate

conda env remove -n concoct

conda env create -f concoct.yml


## then reset to bioconda priorities above

## VAMB ##

conda create -n vamb -c bioconda vamb minimap2

## looks like GPU support is available:
#conda activate vamb
#conda install -c pytorch pytorch torchvision cudatoolkit=10.2
## even with the cuda install, it's quick
## also needs minimap:
#conda install -c bioconda minimap2

## das tools ##


conda config --set channel_priority flexible
conda create -n das_tool -c bioconda das_tool
conda config --set channel_priority strict

## that seems to install ok, but won't work 

## try a manual install?:

conda create -n das_tool

conda activate das_tool

mkdir /vol/danBot/refining/
cd /vol/danBot/refining/

conda install -c conda-forge unzip

git clone https://github.com/cmks/DAS_Tool.git

cd DAS_Tool

unzip db.zip

conda install r-data.table r-magrittr r-docopt
conda install -c bioconda prodigal diamond pullseq ruby

## i think I remember we needed another script...
wget https://raw.githubusercontent.com/cmks/DAS_Tool/master/src/Fasta_to_Contig2Bin.sh
chmod 777 Fasta_to_Contig2Bin.sh

## we installed this last year around 1419. Try those scripts, 
## see if they work...

conda create -n das_env

conda activate das_env

conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge

conda install -c bioconda das_tool

## nope...gotta do the git install, I guess. Start over.

conda create -n unzip -c conda-forge unzip

conda create -n das_env

git clone https://github.com/cmks/DAS_Tool.git

conda deactivate
conda activate unzip

unzip db.zip

conda deactivate

conda activate das_env

conda install -c conda-forge r-base

conda install r-data.table r-magrittr r-docopt
## not working, try manual installation
## in R

repo='http://cran.us.r-project.org' #select a repository
install.packages('data.table', repos=repo, dependencies = T)
install.packages('magrittr', repos=repo, dependencies = T)
install.packages('docopt', repos=repo, dependencies = T)

conda install -c bioconda prodigal diamond pullseq ruby
## this is taking forever

## not run
## git the script
wget https://raw.githubusercontent.com/cmks/DAS_Tool/master/src/Fasta_to_Contig2Bin.sh

## make sure to check DAS_Tool ahead of time, I'm betting we accidentally
## destroyed it when we deleted the repo. 

## maybe they have some sample data to test in the github repo

## ugh.


### checkm ###

conda config --set channel_priority flexible
conda create -n checkm -c bioconda checkm-genome

conda config --set channel_priority strict

### MetaPhlan ###

conda create --name metaphlan -c conda-forge -c bioconda python=3.7 metaphlan

conda activate metaphlan

## put the db on the volumne, so we can clone later
metaphlan --install --bowtie2db /vol/danBot/metaphlanDB

### graphlan ###

#conda create -n graphlan -c biobakery graphlan ## doesn't work

conda create -n graphlan -c bioconda graphlan
conda activate graphlan ## looks like we need this also, has to be done separately?
conda install export2graphlan -c bioconda

## would this work?
conda create -n graphlan -c bioconda graphlan export2graphlan

## and didn't work


### phyloPhlan ###
conda config --set channel_priority flexible
conda create --name phylophlan -c bioconda phylophlan
conda config --set channel_priority strict

### qiime2 ###

## qiime2 folks recommend getting their yml and 
## installing from there:

wget https://data.qiime2.org/distro/core/qiime2-2022.11-py38-linux-conda.yml

conda env create -n qiime2 --file qiime2-2022.11-py38-linux-conda.yml

#conda rename -n qiime2-2022.11 qiime2



### check out the zymo data: ###

mkdir /vol/testNotSoBig/datasets/zymoMC/illumina
mkdir /vol/testNotSoBig/datasets/zymoMC/nanopore

gunzip ERR7255689_1.fastq.gz &
gunzip ERR7255689_2.fastq.gz &
gunzip ERR7255742.fastq.gz &

## the 1+2 files probably = illumina

cd /vol/testNotSoBig/datasets/zymoMC/illumina

head ERR7255689_1.fastq

tail ERR7255689_1.fastq

grep "10003:13052/" ERR7255689_1.fastq

grep "10003:13052/" ERR7255689_2.fastq

seq="GGTGACCACTCGCCCGCCCGCCGCGTTCTCTTCGCTCACGGCCAGGGCGAACATGTTAATCCAGTCGATGACGTTCATCGGATCGCGGGAGAGATTATCGCTGGAGACCAGCAGACGCCGTAACGCTACCGCGCGGCGTGGAACGTTCAGC"
seq="TTTGCATCAAAAGAAGCCCTATTTTTAGAAGTTTATCAAGATAGTATTCAGATGGAATTAACAGAACTAGGGAAAGTAGCAGAGCGAGATGATTTGGTTGGGGAAAAGAAGCTACAATCTATTTTCTTTGTAGCGACAGATTTTTCTAGCA"
seq="ACAATGCGATCAATAATGATTTCAATAGAATGCTTTTTATTCTTCTCGATTTCAATTTCGTCATTGATATCATAAATTTCTCCATCAACACGAATTCGAACATAGCCTTCTTTTTTTATTTCCTCAATAGTTTTCTTATGGGTTCCTTTTT"

echo $seq | wc -c ## seems like they are all 152 bp long. 

## why are these sequences so short, if they are miseq?

## seq numbers line up:
head ERR7255689_1.fastq 
head ERR7255689_2.fastq 

tail ERR7255689_1.fastq 
tail ERR7255689_2.fastq 

## these are really short. Did they already do some sort of trimming on them?
## fastqc

conda activate readQC_env

cd /vol/testNotSoBig/datasets/zymoMC/zymoQC

fastqc -o /vol/testNotSoBig/datasets/zymoMC/zymoQC/ \
    /vol/testNotSoBig/datasets/zymoMC/illumina/ERR7255689_*

scp -i /home/daniel/.ssh/funmic2023 -P 30427 \
    ubuntu@129.70.51.6:/vol/testNotSoBig/datasets/zymoMC/zymoQC/ERR7255689_1_fastqc.html .

(firefox ERR7255689_1_fastqc.html &) &

scp -i /home/daniel/.ssh/funmic2023 -P 30427 \
    ubuntu@129.70.51.6:/vol/testNotSoBig/datasets/zymoMC/zymoQC/ERR7255689_2_fastqc.html .

(firefox ERR7255689_2_fastqc.html &) &

## these look very good. A few illumina adapters in there...

## can these be aligned if they are so short?

conda activate readQC_env

## and the big one probably nanopore:
cd /vol/testNotSoBig/datasets/zymoMC/nanopore/

head ERR7255689_1.fastq

tail -n 200 ERR7255742.fastq | less

## how many reads?
grep -c ^@ ERR7255689_1.fastq ## 24887493
## 24,887,493 reads... interesting. let's see how many mags we 
## get out of that...

## how do we find which is the forward? reverse?
## meh, who cares. With so little overlap,
## seems best to just use them unpaired. 

conda activate vsearch 

vsearch --fastq_mergepairs ERR7255689_1.fastq --reverse ERR7255689_2.fastq --threads 20 --fastqout ERR7255689_paired.fasta 

## what does that look like?

conda activate readQC_env

fastqc -o /vol/testNotSoBig/datasets/zymoMC/zymoQC/ \
    /vol/testNotSoBig/datasets/zymoMC/illumina/ERR7255689_paired.fasta

file=/vol/testNotSoBig/datasets/zymoMC/zymoQC/ERR7255689_paired.fasta_fastqc.html

scp -i /home/daniel/.ssh/funmic2023 -P 30427 \
    ubuntu@129.70.51.6:$file .

(firefox ERR7255689_paired.fasta_fastqc.html &) &

head ERR7255689_paired.fasta

## only 8 million reads successfully merged? I think we have to not merge...
## the qualities of both R1 and R2 are high, can we use them without 
## merging?

## let's do it, for now. 

## how about visualizing the nanopore reads?

## we don't have a read report for the zymoMC nanopore reads, so don't 
## think we can use minionQC.

## so just run fastqc again...

conda activate readQC_env

cd /vol/danBot/datasets/zymoMC/zymoQC/nanopore/

less /vol/danBot/datasets/zymoMC/zymoQC/nanopore/nohup.out

nohup fastqc -t 20 -o /vol/danBot/datasets/zymoMC/zymoQC/nanopore/ \
    /vol/danBot/datasets/zymoMC/nanopore/ERR7287988.fastq  &

## get it local
file=/vol/danBot/datasets/zymoMC/zymoQC/nanopore/ERR7287988_fastqc.html
scp -i /home/daniel/.ssh/funmic2023 -P 30500 \
    ubuntu@129.70.51.6:$file .

(firefox ERR7287988_fastqc.html &) &

## wow, the 10.4 data is way way better.

## let's check the sludge nanopore:
cd /vol/danBot/datasets/sludge/nanopore
nohup fastqc -t 20 -o /vol/danBot/datasets/sludge/sludgeQC/nanopore \
    /vol/danBot/datasets/sludge/nanopore/ERR7014844.fastq  &> fastqc_sludgeNano.log

## get it local
file=/vol/danBot/datasets/sludge/sludgeQC/nanopore/ERR7014844_fastqc.html
scp -i /home/daniel/.ssh/funmic2023 -P 30500 \
    ubuntu@129.70.51.6:$file .

(firefox ERR7287988_fastqc.html &) &



## finding mention of nanoplot and nanostat
## what are these?

## before we go, see if we can get a quick output from nanoplot:

conda activate nanoplot

cd /vol/testNotSoBig/datasets/zymoMC/zymoQC/nanoplotOut

inFastq="/vol/testNotSoBig/datasets/zymoMC/nanopore/ERR7255742.fastq"
outDir="/vol/testNotSoBig/datasets/zymoMC/zymoQC/nanoplotOut/"

nohup NanoPlot -t 6 --verbose --store --huge -o $outDir --fastq $inFastq --format 'png' &

## maybe next time don't save the pickle, too big for github

## check it out:

tar -zcvf nanoplot.tar.gz /vol/testNotSoBig/datasets/zymoMC/zymoQC/nanoplotOut/

file=/vol/testNotSoBig/datasets/zymoMC/nanoplot.tar.gz
scp -i /home/daniel/.ssh/funmic2023 -P 30427 \
    ubuntu@129.70.51.6:$file .

## looks like we need to chop the adapters. Does porechop still work?

## for the barcode data I used cutadapt, previously. 

## tomorrow - do we need to chop adapters? try pore chop, and/or NanoFilt,  and/or cutadapt

## for now, since this is just teaching, let's use cutadapt.

conda activate readQC_env

cd /vol/testNotSoBig/datasets/zymoMC/nanopore/

## the simplest thing is to cut the first nine bp off these.

############################################################################
## debug time
## we know that cutadapt introduces empty reads some where in our pipe
conda deactivate
conda activate readQC_env
head -n 1000000 "/vol/testNotSoBig/datasets/zymoMC/nanopore/ERR7255742.fastq" > npHead.fastq
cutadapt -o ZymoNP_trimmed.fastq --cores 20 --cut 9 npHead.fastq 
grep "^$" npHead.fastq
cutadapt -o ZymoNP_trimmed_filteredEnds.fastq --cores 20 --cut 9 ZymoNP_trimmed.fastq
grep "^$" npHead.fastq

## not found? so where did the empty reads come from?

## meh, leave it alone
## flye claims they do not need qc on the nanopore reads,
## https://github.com/fenderglass/Flye/blob/flye/docs/USAGE.md

## so let's try it without:

conda deactivate

conda activate flye

## let's redo this, with the zymoMC 10.4 data
reads=/vol/danBot/datasets/zymoMC/nanopore/ERR7287988.fastq

cd /vol/danBot/assemblies/zymoMC/flyeNanopore

#nohup flye --meta \
#           --threads 25 \
#           --out-dir . \
#           --nano-hq $reads &> howlongdidIflye.log &

## and that blows out the memory. wow.
## the fly github repo says the following two settings 
## can be changed to lower memory requirements...but with not much further explananation.

--asm-coverage
--genome-size

## they say that 40x is usually enough coverage to make good initial "disjointig" assemblies
## so set to asm to 40?
## estimated genome size, I wonder if this applies to metagenomes?
## but we can't run without a genome size estimate
## do they want a metagenome size? or mag sizes?
## try metagenome size. Base estimate on our illumina metagenome size:

cont="/vol/danBot/assemblies/zymoMC/megahitZymoIllumina/final.contigs.fa"
wc -c $cont ## 41,514,822 characters, so maybe 41 million bp?

## ugh, asm not compatible with meta, So:

nohup flye --meta \
           --genome-size 41m \
           --threads 25 \
           --out-dir . \
           --nano-hq $reads &> howlongdidIflye.log &

## this will prbably break, but give it a shot

## if it doesn't work, try either random subsetting of reads
## or removal of small reads


wc -l $reads & ## fails. too big?

## taking a look at those reads, I see no justification in terms
## of quality for eleminating short reads.
## but they contain less info, I guess.

## how about a random subset 

## and, doesn't work. Time to subset. The first reads are presumably the best, 
## so...
## we know a file about the size of our r9.4 files worked with our cluster
## so if we can get down ~28 gb 

(28.2 / 48.1 ) 
## if read length is distributed randomly (it's not), 
## then we want to reduce this file by about half

## we know the R9.4 files had 8,851,918 reads; 31,995,546,765 basepairs; and 28.2 GB
## 10.4 -   18,831,686 reads; 53,221,766,826 basepairs; 48.1 GB

## wait, 48 gig is the listed file size on the ENA website for this file,
## but this is compressed size. I've deleted the r9.4 data, so I don't know 
## know what its uncompressed size was exactly.

## but it looks like the compression and the #bps scale about 1:1 here, so 
## assume we had ~1/2 the uncompressed file size.

## in general, it looks like the 10.4 data is about twice as large. 

## check the md5 -- 

cd /vol/danBot/datasets/zymoMC/nanopore

md5sum ERR7287988.fastq > nanoporechecksum.txt & ## 2c457962887c03afd89537e0a6cd4502

## this is different than the published md5 on the ena site, maybe they meant the compressed file?
## ena md5 = f40091ab244d98e9da48b561992a371e

md5sum ERR7287988.fastq.gz > nanoporechecksum.txt & ## yup, matches

## but we don't yet know how many reads are in the 10.4 nanopore dataset

## we can make the guess that if we subset to the same number of 
## reads that are in the 9.4 dataset, flye should be able to handle it. 
## We also want as much data

reads=/vol/danBot/datasets/zymoMC/nanopore/ERR7287988.fastq

cd /vol/danBot/datasets/zymoMC/nanopore/

head -n 35000000 $reads > nanopore10subsetted.fastq 

subsetReads=/vol/danBot/datasets/zymoMC/nanopore/nanopore10subsetted.fastq

## that seems to go right to sleep. Not sure why. 
## try split:

mkdir splitUpZymoNanopore

cd /vol/danBot/datasets/zymoMC/nanopore/splitUpZymoNanopore

nohup split -l 4000000 $reads &

## 19 files, 
## each should start with "@" and header info

## and end with qscores...

for i in *; do
  echo $i
  head -n 1 $i
  tail -n 1 $i
  echo "##################"
done

## looks good, and the last line of the last file looks like the last of our nanopore reads

## now try to combine. We want three combinations

## 1 - equal size to the last nanopore data that worked: ~50 gig
## 2 - halfway between this and the full size that failed, ~75 gig
## 3 - size that equals the illumina datasets, to make a fair comparison ~17.4 gig 

## not sure if we'll have time to test all three...

## to make the largest, 38 gig...
## I think I remember that quality drops over the course of a run, 
## so let's take from the oldest to newest files

4.1G xaa
4.8G xab
5.6G xac
5.8G xad
6.2G xae
6.2G xaf
6.3G xag
6.4G xah
6.5G xai
6.1G xaj
6.2G xak
5.9G xal
5.8G xam
5.5G xan
4.9G xao
4.4G xap
3.9G xaq
3.8G xar
2.8G xas


python3

import pandas as pd

filename=["xaa", "xab", "xac", "xad", "xae", "xaf", "xag", "xah", "xai", "xaj", "xak", "xal", "xam", "xan", "xao", "xap", "xaq", "xar", "xas"]
sizes = [4.1, 4.8, 5.6, 5.8, 6.2, 6.2, 6.3, 6.4, 6.5, 6.1, 6.2, 5.9, 5.8, 5.5, 4.9, 4.4, 3.9, 3.8, 2.8]
aa = pd.DataFrame({'file':filename, 'sizes':sizes})

aa.sizes.sum()

## we want the sizes to sum to ~50g, as a first attempt. This is about what a single minion flowcell can do, anway


aa.iloc[0:9,1].sum()  ## taking the first nine files gives us ~52 gig. Any less and we drop to 45 gig.

aa.iloc[0:5,1].sum()  ## 26 gig, this still crashed metaphlan

aa.iloc[0:4,1].sum()  ## 20 gig


aa.iloc[0:4,0]

## so we want these files:
xaa
xab
xac
xad
xae
xaf
xag
xah
xai

cat xaa xab xac xad xae xaf xag xah xai > ERR7287988_shortened.fastq &

## we need even shorter for metaphlan...

cd /vol/danBot/datasets/zymoMC/nanopore/splitUpZymoNanopore

cat xaa xab xac xad > ../ERR7287988_forZymoNanoMetaphlan.fastq &


## flye command above edited to work with this file


############################################################################

conda deactivate
conda activate readQC_env

cd /vol/testNotSoBig/datasets/zymoMC/nanopore/

npData="/vol/testNotSoBig/datasets/zymoMC/nanopore/ERR7255742.fastq"
nohup cutadapt -o ZymoNP_trimmed.fastq --cores 25 --cut 9 $npData 

grep "^$" ZymoNP_trimmed.fastq ## check for empties


grep "^$" ZymoNP_trimmed_filteredEnds.fastq ## check for empties

## does flye like it?
conda deactivate
conda activate flye
reads=ZymoNP_trimmed.fastq
flye --meta \
     --threads 25 \
     --out-dir . \
     --nano-hq $reads 
## seems okay

conda deactivate
conda activate readQC_env

## cutadapt can also remove low quality ends, try this, also 

nohup cutadapt -q 10 \
  -o ZymoNP_trimmed_filteredEnds.fastq \
  --cores 25 \
  ZymoNP_trimmed.fastq &

grep -B 2 -A 2 "^$" ZymoNP_trimmed_filteredEnds.fastq ## check for empties
## there they are

conda deactivate
conda activate flye
reads=ZymoNP_trimmed_filteredEnds.fastq
flye --meta \
     --threads 25 \
     --out-dir . \
     --nano-hq $reads 

## yup. 
## fuck it, let's keep the "ragged edges".

## run fastqc on both
file=ZymoNP_trimmed.fastq
fastqc -t 10 \
  -o /vol/testNotSoBig/datasets/zymoMC/ \
  $file &> fastqcNPtrimmedLog.txt &


file=ZymoNP_trimmed_filteredEnds.fastq
fastqc -t 15 \
  -o /vol/testNotSoBig/datasets/zymoMC/ \
  $file &> fastqcNPtrimmedFilteredLog.txt &

## get them

file=/vol/testNotSoBig/datasets/zymoMC/zymoQC/nanopore/ZymoNP_trimmed_fastqc.html
scp -i /home/daniel/.ssh/funmic2023 -P 30427 \
    ubuntu@129.70.51.6:$file .

file=/vol/testNotSoBig/datasets/zymoMC/ZymoNP_trimmed_filteredEnds_fastqc.html
scp -i /home/daniel/.ssh/funmic2023 -P 30427 \
    ubuntu@129.70.51.6:$file .

(firefox ZymoNP_trimmed_fastqc.html &) &

(firefox ZymoNP_trimmed_filteredEnds_fastqc.html &) &

find . ZymoNP_trimmed_filteredEnds.fastq_fastqc.html

## that takes a long time, the fastqc generation.
## but both steps really helped. 

## just do fastqc once before on primary inspection of the nanopore reads
## and once after trimming and filtering.

## actually, just trimming. Filtering is screwing up our fasta 

## the illumina data seems find to me, except it doesn't pair well.
## can we still use it for assembly

### run assemblers! ###

## for the nanopore data, we'll use metaFlye 

## https://github.com/fenderglass/Flye

## for the illumina, we can choose between megahit and spades..

## both we (last year) and the Sereika project use megahit, so let's 
## stay with it.

conda activate megahit

fast1=/vol/danBot/datasets/zymoMC/illumina/ERR7255689_1.fastq
fast2=/vol/danBot/datasets/zymoMC/illumina/ERR7255689_2.fastq
outdir=/vol/danBot/assemblies/zymoMC/megahitZymoIllumina
\time -v megahit -1 $fast1 \
          -2 $fast2 \
          -t 25 \
          -o $outdir &> megahit.log &

## oops, forgot to nohup that

## not sure how well it went, but let's 
## that was really quick - N50 is 103233, not bad

## try to start the flye assembler, might take longer...

conda activate flye

ls /vol/testNotSoBig/datasets/zymoMC/nanopore/

cd /vol/testNotSoBig/assemblies/zymoMC/flyeNanopore/

#reads=/vol/testNotSoBig/datasets/zymoMC/nanopore/ZymoNP_trimmed_filteredEnds.fastq

reads=/vol/testNotSoBig/datasets/zymoMC/nanopore/ZymoNP_trimmed.fastq
outdir=/vol/testNotSoBig/assemblies/zymoMC/flyeNanopore/
\time -v nohup flye --meta \
          --threads 25 \
          --out-dir $outdir \
          --nano-hq $reads &> howlongDidIflye.log &

## oh crap. ran out of memory. Time to start a fat node.

##file=/vol/testNotSoBig/datasets/zymoMC/nanopore/ERR7255742.fastq
file=/vol/testNotSoBig/datasets/zymoMC/nanopore/ZymoNP_trimmed_filteredEnds.fastq

grep -B 6 -A 2 "@ERR7255742.1631 " $file

## are there others?
head -n 10000 $file | grep -B 2 -A 2 "^$"

## find these errors using empty lines
## can't acutally look for linbreaks with grep
grep -n -B 2 -A 2 "^$" $file > empties.txt &

grep -n -B 2 -A 2 "^$" $file ## not found when we look in the pre-cutadapt

## yeah, looks like a lot of empty reads after 

## just curious, do we get the same result from the raw file?


reads=/vol/testNotSoBig/datasets/zymoMC/nanopore/ZymoNP_trimmed_filteredEnds.fastq

### check out the sludge data ### 

cd /vol/testNotSoBig/datasets/sludge

gunzip IL-202001-1.fastq.gz &
gunzip IL-202001-2.fastq.gz &
gunzip ERR7014876.fastq.gz &

## sludge illumina

cd /vol/testNotSoBig/datasets/sludge/illumina

head -n 1 IL-202001-1.fastq

tail IL-202001-1.fastq

head -n 1 IL-202001-2.fastq

tail IL-202001-2.fastq

## why are these different?
tail -n 4 IL-202001-1.fastq | head -n 1
tail -n 4 IL-202001-2.fastq | head -n 1

## these do not look like paired read files to me

head -n 1000 IL-202001-1.fastq | less

## for instance:
grep -A 2 "16026:1992" IL-202001-1.fastq ## gives us:
@M00878:365:000000000-CVP5H:1:1102:16026:1992_1
@M00878:365:000000000-CVP5H:1:1102:16026:1992_2
## both in the same file


grep -A 2 "16026:1992" IL-202001-2.fastq ## nothing is found. 

grep -n "1102:16026:1992_" IL-202001-1.fastq &> findreadpairplace.txt ## gives us:
997:@M00878:365:000000000-CVP5H:1:1102:16026:1992_1
47092901:@M00878:365:000000000-CVP5H:1:1102:16026:1992_2

## how many reads are there?

grep -c ^@M00 IL-202001-1.fastq &> zoop ## 23545952 23,545,952

grep -c ^@M00 IL-202001-2.fastq &> zoop ## 23545952 23,545,952

## ugh, this is a fucking nature paper and I still have to clean
## data. 

tail IL-202001-1.fastq 

head -n 1000 IL-202001-2.fastq | less

## so looks like paired reads, but each file contains its own paired data?

## then why are they the exact same number of reads? That is weird.

## good explanation on illumina fastq sequence identifiers here: 
## https://support.illumina.com/help/BaseSpace_OLH_009008/Content/Source/Informatics/BS/FileFormat_FASTQ-files_swBS.htm

seq="CCCATTGCTTTTCATCACTTCAATTGTGTACCTTTGCTCTACGGTTAAATGGCTCATATTTTGCAACTTTTGGACGAGGACACAAAGTAAAGAGAATTTATCCATTTCAGGCGGGGGGACATTTTTGTCCCTTCGCTTGAAAAAAATTAATTCATGCCCTCAAAAAGTTGCATTTATTAGTT"
echo $seq | wc -c ## 183 BP, still short for miseq

## so if these are not paired, what are we looking at? 

## goals - fastqc all illumina, 

###### start over on new instance ####

## denbi has come through with the heavier (128 gb ram), GPU-equipped VMs.
## also, our medium instance (64gb) can't handle the full nanopore dataset
## assembly. So time to try the big guns.

## ssh for new instance:

ssh -p 30500 -i /home/daniel/.ssh/ubuntu_e ubuntu@129.70.51.6

## or just

danBot

## get anaconda on there:
wget https://repo.anaconda.com/archive/Anaconda3-2022.10-Linux-x86_64.sh

bash ./Anaconda3-2022.10-Linux-x86_64.sh

/vol/danBot/

## let's make it so danbot and funcomp can talk.

## on danbot, make a keypair:
ssh-keygen -f danbotFuncomp

chmod 600 danbotFuncomp
chmod 444 danbotFuncomp.pub

## put the pub key on funcomp authorized_keys
cat danbotFuncomp.pub

## try login:
ssh -vp 30427  -i /home/ubuntu/.ssh/danbotFuncomp ubuntu@129.70.51.6
## works

## what do we need?
## the raw data, get it all:

file=/vol/testNotSoBig/datasets/
nohup scp -i /home/ubuntu/.ssh/danbotFuncomp -P 30427 -r ubuntu@129.70.51.6:$file . &

## meh, something failed.
## in the end, detach old volume and attach here. 
## let's try our flye assembly again:

conda activate flye

cd /vol/danBot/assemblies/zymoMC/flyeNanopore/

## edited to use the shortened zymoMC reads, started 12:55
reads=/vol/danBot/datasets/zymoMC/nanopore/ERR7287988_shortened.fastq
outdir=/vol/danBot/assemblies/zymoMC/flyeNanopore/
\time -v nohup flye --meta \
          --threads 25 \
          --out-dir $outdir \
          --nano-hq $reads &> howlongDidIflye.log &
## still going (but mostly done?) at 17:00

## and let's get the assembly going for our nanopore sludge reads

conda deactivate 
conda activate flye

reads=/vol/danBot/datasets/sludge/nanopore/ERR7014844.fastq
outdir=/vol/danBot/assemblies/sludge/nanopore/
\time -v nohup flye --meta \
          --threads 25 \
          --out-dir $outdir \
          --nano-hq $reads &> howlongDidIflye.log &


########## assembly qc #############


## how is our illumina assembly
/vol/danBot/assemblies/zymoMC/flyeNanopore/assembly.fasta

## it's pretty small - 40M. 40,000,000 bytes

wc -c assembly.fasta &

## ecoli has  5.11174 mbp. This is a small metagenome.

## let's check them with metaquast?  

illuminaZymoAssembly=/vol/danBot/assemblies/zymoMC/megahitZymoIllumina/final.contigs.fa
illuminaZymoMetaquastOut=/vol/danBot/datasets/zymoMC/zymoQC/illumina/metaquast
nanoporeZymoAssembly=/vol/danBot/assemblies/zymoMC/flyeNanopore/assembly.fasta
nanoporeZymoMetaquastOut=/vol/danBot/datasets/zymoMC/zymoQC/nanopore/metaquast

conda activate metaquast

metaquast --help

metaquast -t 13 \
          -o $illuminaZymoMetaquastOut \
          $illuminaZymoAssembly 

## not much memory usage, but the downloads from ncbi take a while

metaquast -t 13 \
          -o $nanoporeZymoMetaquastOut \
          $nanoporeZymoAssembly &> nanoporeMetaquast.log &

## zip them up

cd /vol/danBot/datasets/zymoMC/zymoQC/illumina/
tar -czvf metaquastOutZymoIllumina.tar.gz metaquast/

cd /vol/danBot/datasets/zymoMC/zymoQC/nanopore

tar -czvf metaquastOutZymoNanopore.tar.gz metaquast/


## get these local
## put these somewhere not in the repo, they are big.

cd /home/daniel/Documents/teaching/functionalMicrobiomes/readQC/zymoQC/illumina/metaquast
file=/vol/danBot/datasets/zymoMC/zymoQC/illumina/metaquastOutZymoIllumina.tar.gz
scp -i /home/daniel/.ssh/funmic2023 -P 30500 -r ubuntu@129.70.51.6:$file . 

cd /home/daniel/Documents/teaching/functionalMicrobiomes/readQC/zymoQC/nanoporeQC/metaquast
file=/vol/danBot/datasets/zymoMC/zymoQC/nanopore/metaquastOutZymoNanopore.tar.gz
scp -i /home/daniel/.ssh/funmic2023 -P 30500 -r ubuntu@129.70.51.6:$file . 

cd /home/daniel/Documents/teaching/functionalMicrobiomes/readQC/zymoQC/illumina/metaquast


### polishing with Racon ###

## de.nbi has a working example here:
## https://denbi-nanopore-training-course.readthedocs.io/en/latest/polishing/medaka/Racon_1.html

conda activate racon

cd /vol/danBot/polish/zymoMCnano

## make alignment of reads to assembly

### binning ###

## the sereika paper used two of the same binning software packages
## as we used last year: metabat2 and maxbin2

## they polished the metagenome with Racon before binning.
## which means we need to learn about polishing...

## while we are waiting on medaka and maxbin2 installations,
## can we try our other to binning software out on the 
## illumina datasets?

## metabat

## we did this last year...

## metabat uses contig abundances
## they had a wrapper script to generate these
## from your raw reads, 

## I think we first had to do an alignment with bowtie and samtools

cd /vol/danBot/binning/zymoMC/illumina

conda activate alignmentTools 

cont="/vol/danBot/assemblies/zymoMC/megahitZymoIllumina/final.contigs.fa"

## do our raw reads for bowtie have to be in a single file?
## looks like we use -1 and -2 options

## step 1 make the bowtie index of the illimina read assembly

binWD="/vol/danBot/binning/zymoMC/illumina/illuminaBowTieIndex"
cont="/vol/danBot/assemblies/zymoMC/megahitZymoIllumina/final.contigs.fa"
reads1="/vol/danBot/datasets/zymoMC/illumina/ERR7255689_1.fastq"
reads2="/vol/danBot/datasets/zymoMC/illumina/ERR7255689_2.fastq"

bowtie2-build $cont $binWD/zymoIlluminaAssembly

## do the alignment
bowtie2 \
  -x $binWD/zymoIlluminaAssembly \
  -1 $reads1 -2 $reads2 \
  -S rawReads2Contigs.sam \
  --threads 25 \
  --local 

## started 9:45
## this takes a very long time, maybe 0.5 hour or more
## with 25 cores, 20 min

## sort it

samtools sort -l 1 \
    -@15 \
    -o rawReads2ContigsSorted.bam \
    -O BAM \
    rawReads2Contigs.sam
## 5 mins

    #-T /tmp/sortRR2C \ ## do we need this? doesn't seem like it

## now what? this is needed for metabat and concoct, I think 
## vamb uses minimap in their examples, though we could 
## probably figure out how to use this alignment file, I'm sure.

## try metabat...

### metabat ###

conda deactivate

conda activate metabat2

runMetaBat.sh

cd /vol/danBot/binning/zymoMC/illumina/metabat2

cont="/vol/danBot/assemblies/zymoMC/megahitZymoIllumina/final.contigs.fa"
bam="/vol/danBot/binning/zymoMC/illumina/illuminaBowTieIndex/rawReads2ContigsSorted.bam"

runMetaBat.sh $cont $bam ## quick, 5 min

## 14 bins... that seems about right. Since we're not inject the hiseq data into this.

### vamb ###

## this is new this year - how does vamb work? 

conda deactivate

### vamb ###

conda activate vamb 

cd /vol/danBot/binning/zymoMC/illumina/vamb

## they run a cataloging program on their assemblies. Not sure if I need to do this but...

cont="/vol/danBot/assemblies/zymoMC/megahitZymoIllumina/final.contigs.fa"
reads1="/vol/danBot/datasets/zymoMC/illumina/ERR7255689_1.fastq"
reads2="/vol/danBot/datasets/zymoMC/illumina/ERR7255689_2.fastq"

concatenate.py illumcatalogue.fna.gz $cont

## they use minimap2 in their examples

conda activate alignmentTools

minimap2 -d illumcatalogue.mmi illumcatalogue.fna.gz # make index, quick

## align reads to metagenome
minimap2 -t 8 -N 5 -ax sr illumcatalogue.mmi --split-prefix mmsplit $reads1 $reads2 | samtools view -F 3584 -b --threads 18 > illumReadsAligned2Contigs.bam
## maybe ten minutes
## use more cores.

## I wonder if we could use minimap instead of bowtie above? seems much faster, and samtools seems to 
## handle the outputs...

## run the binner:
conda deactivate 
conda activate vamb

vamb --outdir vambOut --fasta sludgeIlluCatalogue.fna.gz --bamfiles sludgeIlluVamb_ReadsAligned2Contigs.bam -o C --minfasta 200000 


cd /vol/danBot/binning/sludge/illumina/vamb/vambOut/bins
ls | wc -l  ## 88 bins

## see here, btw
https://github.com/RasmussenLab/vamb/issues/59

## we may need to revert to a different binning software
## but it does claim to have found ~8 bins...

## conda maxbin really doesn't seem to be happening.
## can we install from source?
## nope.

## new strategy - use concoct, VAMB, metabat
##              - use the 10.4 data instead?
##              - which means rerun flye

### concoct ###

## indepth documentation here, for example:
## https://concoct.readthedocs.io/en/latest/scripts/cut_up_fasta.html

## cut_up_fasta.py original_contigs.fa -c 10000 -o 0 --merge_last -b contigs_10K.bed > contigs_10K.fa
## concoct_coverage_table.py contigs_10K.bed mapping/Sample*.sorted.bam > coverage_table.tsv
## concoct --composition_file contigs_10K.fa --coverage_file coverage_table.tsv -b concoct_output/
## merge_cutup_clustering.py concoct_output/clustering_gt1000.csv > concoct_output/clustering_merged.csv
## mkdir concoct_output/fasta_bins
## extract_fasta_bins.py original_contigs.fa concoct_output/clustering_merged.csv --output_path concoct_output/fasta_bins


## we'll work in our alignments folder
cd /vol/danBot/binning/zymoMC/illumina/illuminaBowTieIndex/

## necessary for step 2, generating the coverage table
conda deactivate
conda activate alignmentTools

samtools index rawReads2ContigsSorted.bam

conda activate concoct

cont="/vol/danBot/assemblies/zymoMC/megahitZymoIllumina/final.contigs.fa"
outputDir="/vol/danBot/binning/zymoMC/illumina/concoct"

cut_up_fasta.py $cont -c 10000 -o 0 --merge_last -b concoctContigs_10K.bed > concoctContigs_10K.fa
concoct_coverage_table.py concoctContigs_10K.bed rawReads2ContigsSorted.bam > coverage_table.tsv

## this takes maybe ten min?, started ~12:00
## the meat of it is here:

concoct \
  --composition_file concoctContigs_10K.fa \
  --coverage_file coverage_table.tsv \
  -t 25 \
  -b $outputDir


## get illumina reads through the binning and refining steps
## get nanopore polished?
## then bin/refine

cd $outputDir
merge_cutup_clustering.py clustering_gt1000.csv > clustering_merged.csv
mkdir fasta_bins
extract_fasta_bins.py $cont clustering_merged.csv --output_path fasta_bins/

## let's rename these, I think the raw numbers are causing problems
for i in *; do 
mv $i ${i/\.fa/_concat\.fa}
done

## this is here:
cd /vol/danBot/binning/zymoMC/illumina/concoct/fasta_bins_renamed


## 15 bins
## metabat had 14
## and VAMB had 8

## okay, the illumina data is ready for das tools

## but let's back up a bit. 
## don't really want to do the nanopore polishing right now,
## because we are running out of time and the class
## is full anyway. 

## which means we should use the 10.4 flow cell data, not the 9.4
## and we need to rerun flye, which takes forever. 

## before lunch.... 

## step one edit the data fetch above 
## restart flye on the zymo data

############# refining ###########

## try dastools on our illumina assembly

## das tools needs a contig-bin map of alignments
## one for each of the binning softwares we used

conda activate das_tool

Fasta_to_Contigs2Bin.sh

## odd, that file is not found in the conda installation
## can't find it anywhere in the anaconda environment directories, at least

## can we grab it out of the github repo?

cd /vol/danBot/refining/zymoMC

wget https://raw.githubusercontent.com/cmks/DAS_Tool/master/src/Fasta_to_Contig2Bin.sh
chmod 777 Fasta_to_Contig2Bin.sh

## our bins are here:
#concoctBins=/vol/danBot/binning/zymoMC/illumina/concoct/fasta_bins_renamed/ 
concoctBins=/vol/danBot/binning/zymoMC/illumina/concoct/fasta_bins/ 
metabatBins=/vol/danBot/binning/zymoMC/illumina/metabat2/final.contigs.fa.metabat-bins-20230222_091706/
vambBins=/vol/danBot/binning/zymoMC/illumina/vamb/vambOut/bins/

ls $concoctBins
ls $metabatBins
ls $vambBins

## concoct:
./Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $concoctBins \
    > concoct.contigs2bin.tsv

## I don't think that quite worked...
## let's keep just the first and last columns of that

paste <(cut -d " " -f 1 concoct.contigs2bin.tsv) <(cut -f 2 concoct.contigs2bin.tsv) > concoct.contigs2bin_edited.tsv

## move to the directory:
#mv concoct.contigs2bin.tsv $concoctBins

## metabat
./Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $metabatBins \
    > metabat.contigs2bin.tsv
#mv metabat.contigs2bin.tsv $metabatBins

## vamb
./Fasta_to_Contig2Bin.sh \
    -e fna \
    -i $vambBins \
    > vamb.contigs2bin.tsv
#mv vamb.contigs2bin.tsv $vambBins

## we need to cut the first three letters or so...
cut --complement -c 1-3 vamb.contigs2bin.tsv > vamb.contigs2bin_edited.tsv

cd /vol/danBot/refining/zymoMC

conda deactivate 

conda activate das_tool

## typos, probably

## adapted for conda

conda activate das_tool

DAS_Tool  -i concoct.contigs2bin_edited.tsv,metabat.contigs2bin.tsv,vamb.contigs2bin_edited.tsv \
    -l concoct,metabat,vamb \
    -c $cont \
    -t 25 \
    --write_bins \
    --write_bin_evals \
    -o DASToolRun1

## put them here:
ls /vol/danBot/refining/zymoMC/illumina/DASToolRun1_DASTool_bins

## and we're down to 5 mags. Check them out with checkM
## but first, we have to catch up the nanopore data...


### checkM ###

conda activate checkm

cd /vol/danBot/refining/zymoMC/illumina

## let's run checkm on our mags:

## first step - find the right branch of tol:
zymoIllBins=/vol/danBot/refining/zymoMC/illumina/DASToolRun1_DASTool_bins

checkm lineage_wf -t 22 -x fa $zymoIllBins checkMout

checkMout="/vol/danBot/refining/zymoMC/illumina/checkMout"

## this also generated the markers we need to do some quality analyses.
## we can look at this with checkm's qa function:

zymoIlluminaMarkers=/vol/danBot/refining/zymoMC/illumina/checkMout/lineage.ms

checkm qa $zymoIlluminaMarkers $checkMout > zymoIllCheckMout.txt

## great, get the nanopore data back on track, then try phylophlan on our MAGs.


### metaphlan ###

## let's look at our community using metaphlan, as a contrast to the metaquast
## tool, whihc is dependent on 16s recoovery, I think.

conda deactivate

conda activate metaphlan

cd /vol/danBot/communityProfiles/zymoMC/illumina/metaphlan

metaphlanMarkerDB=/vol/danBot/metaphlanDB
seqs1=/vol/danBot/datasets/zymoMC/illumina/ERR7255689_1.fastq
seqs2=/vol/danBot/datasets/zymoMC/illumina/ERR7255689_2.fastq

## run metaphlan. Save the bowtie alignment in case we want to do it again..

metaphlan $seqs1,$seqs2 \
    --bowtie2db $metaphlanMarkerDB \
    --bowtie2out metaphlanBowtie2.ZymoIllu.bz2 \
    --nproc 20 \
    --input_type fastq \
    -o MetaPhlanProfiled_zymoIllu.txt 

## is there any way to quickly? we have this old script:


mphlanprofile=MetaPhlanProfiled_zymoIllu.txt 

head $mphlanprofile

#grep "s__" $mphlanprofile | sed  's/\([0-9]*[0-9]\.[0-9]*\).*$/\1/g' | sed 's/^.*s__//g' | cut -f 1,3
## too complicated but its neat, so save it... 

## this works better. 
mphlanprofile=MetaPhlanProfiled_zymoIllu.txt 
grep -v "t__" $mphlanprofile | grep "s__" | sed 's/^.*s__//g' | cut -f1,3 > zymoIlluMetaphlanAbundances.tsv

### visualize metaphlan results with graphlan ###

## if there is time, see visualizations for this? they use GraPhlan
## here:
https://github.com/biobakery/biobakery/wiki/metaphlan2#create-a-cladogram-with-graphlan

conda deactivate

conda activate graphlan

cd /vol/danBot/communityProfiles/zymoMC/illumina/metaphlan


## they edit their abundance table in a similar way:

mphlanprofile=MetaPhlanProfiled_zymoIllu.txt 

grep -E "(s__)|(^ID)" $mphlanprofile | grep -v "t__" | sed 's/^.*s__//g' > zymoIlluMetaphlanAbundances.tsv

export2graphlan.py -h

## this works
export2graphlan.py -i MetaPhlanProfiled_zymoIllu.txt  -t merged_abundance.tree.txt -a  merged_abundance.annot.txt

## so try full suggested settings:
export2graphlan.py -i MetaPhlanProfiled_zymoIllu.txt \
  --skip_rows 1,2 \
  --tree merged_abundance.tree.txt \
  --annotation merged_abundance.annot.txt \
  --most_abundant 100 \
  --abundance_threshold 1 \
  --least_biomarkers 10 \
  --annotations 5,6 \
  --external_annotations 7 \
  --min_clade_size 1 

graphlan_annotate.py --annot merged_abundance.annot.txt merged_abundance.tree.txt merged_abundance.xml
graphlan.py --dpi 300 merged_abundance.xml merged_abundance.png --external_legends

## get it local
rm zymoIlluGraphlanOut/*
mv *.png zymoIlluGraphlanOut/ ## put a copy of our relative abundances in there
cp zymoIlluMetaphlanAbundances.tsv zymoIlluGraphlanOut/
tar -czvf zymoIlluGraphlanOut.tar.gz zymoIlluGraphlanOut/

## local
cd /home/daniel/Documents/teaching/functionalMicrobiomes/metaphlanGraphics
file=/vol/danBot/communityProfiles/zymoMC/illumina/metaphlan/zymoIlluGraphlanOut.tar.gz
scp -i /home/daniel/.ssh/funmic2023 -P 30500 -r ubuntu@129.70.51.6:$file .
tar -xzf zymoIlluGraphlanOut.tar.gz 
eog zymoIlluGraphlanOut/*png &

### phylophlan ###

cd /vol/danBot/assignTax/zymoMC/illumina

conda activate phylophlan

MAGs=/vol/danBot/refining/zymoMC/illumina/DASToolRun1_DASTool_bins

## the newest database as implied by name is: CMG2122
## but that database sucks, I think is some project or tutorial specific thing.

phylophlan_metagenomic

\time -v phylophlan_metagenomic \
    -i $MAGs \
    -e fa \
    -d SGB.Jul20 \
    -o phylophlanOut_ZymoIllu \
    --nproc 2 \
    -n 1 \
    --verbose &> phylophlan23.log &

## and works like a charm. all five mags go right where they are supposed to.

## this takes a long time, just to download the databases..
## so, maybe add the database for the class ahead of time. 

## add processors if not running an assembly

## looks like they have a vis script:

## if the nanopore assembly doesn't give more mags, what do we do?

## well, need to start on the sludge data...we need to know that 
## they will have at least a few MAGs to work with.

## best to get the other nanopore assembly going tonight, so we can 
## run it through the pipeline quickly and make sure it is a viable 
## dataset

## so get download going when we have a core or two. 
## if the R10 data isn't enough, we'll have to try the R9 data

## or sick them on the mbarc data?

## looks like we have the right data. So set up the code for another flye assembly.
## also metabat?

## we have to understand the miseq data from the sludge - is it any good?
## or is it totally dependent on the hiseq reads?

## then we can run all three through the same pipeline?

#### run MC nanopore through the above pipeline ###

## start over with with zymoNano assembly:

## run metaquast and metaphlan on the nanopore data. 
## can metaphlan handle nanopore data?
## I think it is largely based on bowtie, which isn't 
## optimized for nanopore, but let's see

conda deactivate 

conda activate metaphlan

cd /vol/danBot/communityProfiles/zymoMC/nanopore/metaphlan

metaphlanMarkerDB=/vol/danBot/metaphlanDB
#seqs=/vol/danBot/datasets/zymoMC/nanopore/ERR7287988_shortened.fastq ## too big, memory dump
seqs=/vol/danBot/datasets/zymoMC/nanopore/ERR7287988_forZymoNanoMetaphlan.fastq ## smaller, 27 gig


## started ~13:42 UTC
## lots of cores memory dumps, even with 5 cores. try 1
\time -v nohup metaphlan $seqs \
    --bowtie2db $metaphlanMarkerDB \
    --bowtie2out metaphlanBowtie2.ZymoNano.bz2 \
    --nproc 1 \
    --input_type fastq \
    -o MetaPhlanProfiled_zymoIllu.txt  &> metaphlanZymoNano.log &

## runs really slow. But can't speed up because of memory issues
## use nohup next time

### zymoNano metaquast ###

## as with metaphlan, not sure how metaquast will behave with nanopore reads
## should be fine, this uses the assembly, not the reads. 
## but who knows. Try it out.

cd /vol/danBot/communityProfiles/zymoMC/nanopore/metaquast

conda deactivate 

conda activate metaquast

illuminaNanoAssembly=/vol/danBot/assemblies/zymoMC/flyeNanopore/assembly.fasta
illuminaZymoMetaquastOut=/vol/danBot/communityProfiles/zymoMC/nanopore/metaquast
## not much memory usage, but the downloads from ncbi take a while
metaquast -t 13 \
          -o $illuminaZymoMetaquastOut \
          $illuminaNanoAssembly &> nanoporeMetaquast.log &

## zip it up 

cd /vol/danBot/communityProfiles/zymoMC/nanopore
tar -czvf metaquastOutZymoNanopore.tar.gz metaquast/

## and get it local:
cd /home/daniel/Documents/teaching/functionalMicrobiomes/readQC/zymoQC/nanoporeQC/nano10.4/metaquast
file=/vol/danBot/communityProfiles/zymoMC/nanopore/metaquastOutZymoNanopore.tar.gz
scp -i /home/daniel/.ssh/funmic2023 -P 30500 -r ubuntu@129.70.51.6:$file . 

### zymoNano binning ###

## step 1 make the index of the read assembly

## bowtie is really not handling these reads well. 
## can we use minimap?

conda deactivate 

conda activate alignmentTools 

indexWD="/vol/danBot/binning/zymoMC/nanopore/nanoporeIndex"
cont="/vol/danBot/assemblies/zymoMC/flyeNanopore/assembly.fasta"
reads="/vol/danBot/datasets/zymoMC/nanopore/ERR7287988_shortened.fastq"

cd $indexWD

minimap2 -d zymoNanoAssembly.mmi $cont # make index
minimap2 -t 8 -ax map-ont zymoNanoAssembly.mmi --split-prefix mmsplit $reads | samtools view -F 3584 -b --threads 18 > nanoReadsAligned2Contigs.bam ## 11:09 to 11:58, ~50 mins!

## do we need this?: yes
samtools sort -l 1 \
    -@10 \
    -o zymoNanoReads2ContigsSorted.bam \
    -O BAM \
    nanoReadsAligned2Contigs.bam
## 12:03 to 12:25??ish


samtools index $bam ## do we need this? yes. takes time, maybe put this above
## 13:55ish

## not even sure we can use the for the metabat and concoct pipelines

### metabat zymoNano ###

conda deactivate

conda activate metabat2

runMetaBat.sh

cd /vol/danBot/binning/zymoMC/nanopore/metabat2/
cont="/vol/danBot/assemblies/zymoMC/flyeNanopore/assembly.fasta"
## but this probably is what will really work:
bam="/vol/danBot/binning/zymoMC/nanopore/nanoporeIndex/zymoNanoReads2ContigsSorted.bam"

\time -v runMetaBat.sh $cont $bam ## started 12:38, finished two minutes later, didn't even use a gig of RAM

### concoct zymoNano ###

conda deactivate

conda activate concoct

cd /vol/danBot/binning/zymoMC/nanopore/concoct

cont="/vol/danBot/assemblies/zymoMC/flyeNanopore/assembly.fasta"
outputDir="/vol/danBot/binning/zymoMC/nanopore/concoct"
bam="/vol/danBot/binning/zymoMC/nanopore/nanoporeIndex/zymoNanoReads2ContigsSorted.bam"

#samtools index $bam ## do we need this? yes. takes time, maybe put this above

cd /vol/danBot/binning/zymoMC/nanopore/concoct

cut_up_fasta.py $cont -c 10000 -o 0 --merge_last -b concoctContigs_10K.bed > concoctContigs_10K.fa
concoct_coverage_table.py concoctContigs_10K.bed $bam > coverage_table.tsv
## 14:16 UTC, Finished 14:55, 40 mins!!

## the meat of it is here:
concoct \
  --composition_file concoctContigs_10K.fa \
  --coverage_file coverage_table.tsv \
  -t 25 \
  -b $outputDir

## get illumina reads through the binning and refining steps
## get nanopore polished?
## then bin/refine

cd $outputDir
merge_cutup_clustering.py clustering_gt1000.csv > clustering_merged.csv
mkdir fasta_bins
extract_fasta_bins.py $cont clustering_merged.csv --output_path fasta_bins/

## 15 bins. weird.

### vamb zymoNano ###

conda deactivate

conda activate vamb 

cd /vol/danBot/binning/zymoMC/nanopore/vamb

## they run a cataloging program on their assemblies. Not sure if I need to do this but...

cd /vol/danBot/binning/zymoMC/nanopore/vamb
reads="/vol/danBot/datasets/zymoMC/nanopore/ERR7287988_shortened.fastq"
cont="/vol/danBot/assemblies/zymoMC/flyeNanopore/assembly.fasta"
outdir="/vol/danBot/binning/zymoMC/nanopore/vamb/vambOut"
rm -r vambOut

concatenate.py zymoNanoCatalogue.fna.gz $cont

conda deactivate
conda activate alignmentTools

minimap2 -d zymoNanoCatalogue.mmi zymoNanoCatalogue.fna.gz # make index, quick

## align reads to metagenome
minimap2 -t 8 -N 5 -ax map-ont zymoNanoCatalogue.mmi --split-prefix mmsplit $reads | samtools view -F 3584 -b --threads 10 > zymoNanoVamb_ReadsAligned2Contigs.bam &
## takes forever. 47 mins

## run the binner:
conda deactivate 

conda activate vamb

## had to really drop the batch size down to far below what they recommend:
vamb --outdir vambOut --fasta zymoNanoCatalogue.fna.gz --bamfiles zymoNanoVamb_ReadsAligned2Contigs.bam -t 8 -o C --minfasta 200000 &

ls /vol/danBot/binning/zymoMC/nanopore/vamb/vambOut/bins | wc -l  ## 

### zymoNano das_tool ###

conda deactivate
conda activate das_tool

## okay, first the pain in the ass of generating abundance tables

## the script to start us off is here:
ls /vol/danBot/refining/Fasta_to_Contig2Bin.sh

cd /vol/danBot/refining/zymoMC/nanopore

## our bins are here:
concoctBins=/vol/danBot/binning/zymoMC/nanopore/concoct/fasta_bins
metabatBins=/vol/danBot/binning/zymoMC/nanopore/metabat2/assembly.fasta.metabat-bins-20230224_114049
vambBins=/vol/danBot/binning/zymoMC/nanopore/vamb/vambOut/bins

ls $concoctBins
ls $metabatBins
ls $vambBins

## concoct:
/vol/danBot/refining/Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $concoctBins \
    > concoct.contigs2bin.tsv

## that seemed to work more smoothly than with the concoct table from the illumina

## metabat
/vol/danBot/refining/Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $metabatBins \
    > metabat.contigs2bin.tsv
## why is that so short? I guess the longer reads...

## vamb
/vol/danBot/refining/Fasta_to_Contig2Bin.sh \
    -e fna \
    -i $vambBins \
    > vamb.contigs2bin.tsv

## we need to cut the first three letters or so...
cut --complement -c 1-3 vamb.contigs2bin.tsv > vamb.contigs2bin_edited.tsv


DAS_Tool  -i concoct.contigs2bin.tsv,metabat.contigs2bin.tsv,vamb.contigs2bin_edited.tsv \
    -l concoct,metabat,vamb \
    -c $cont \
    -t 25 \
    --write_bins \
    --write_bin_evals \
    -o DASTool_zymoNano

## 8 bins, that's encouraging. Run checkM on them and see how they look on monday
## 16:09

## put them here:
ls /vol/danBot/refining/zymoMC/illumina/DASToolRun1_DASTool_bins

## when we get a chance, rerun the metaphlan

## meh, metaphlan just ain't working with nanopore reads
## shelve it for now, look for an easy alternative when there is 
## time.

##### sludge data through pipeline #####

### look at sludge illumina files ###

## let's get the sludge data through the above pipeline
## start by catching up the illumina data from this

## do we understand this data?
## 

wc -l IL-202001-1.fastq && wc -l IL-202001-2.fastq  ## both have exactly 94183808 lines, weird

head -n 1 IL-202001-1.fastq
head -n 1 IL-202001-2.fastq

head -n 4 IL-202001-1.fastq
head -n 4 IL-202001-2.fastq

## it looks like these were done in the same run? but different flow cell?

## are the reads and qualities the same?

clear
echo "#############################################################"
tail -n 4 IL-202001-1.fastq
echo ""
echo "#############################################################"
echo ""
tail -n 4 IL-202001-2.fastq
echo "#############################################################"

## nope, these are different reads. Not sure what they did here...

cd /vol/danBot/datasets/sludge/illumina


head IL-202001-1.fastq

##  first read in IL-202001-1.fastq 
readAddress=":1:1102:17610:1802_."
grep -n $readAddress IL-202001-1.fastq &
1:@M00878:365:000000000-CVP5H:1:1102:17610:1802_1
47091905:@M00878:365:000000000-CVP5H:1:1102:17610:1802_2
## both are present in the first file

##  last read in IL-202001-1.fastq 
readAddress="1:2119:11004:25088_."
47091901:@M00878:374:000000000-J34NL:1:2119:11004:25088_1
94183805:@M00878:374:000000000-J34NL:1:2119:11004:25088_2
grep -n $readAddress IL-202001-1.fastq 
## again, both are present in the first file
## not interleaved, but stacked.

## for IL-202001-1.fastq, looks like the forward reads 
## end at 47091901 (+3) and the reverse reads begin at 47091905

## and for IL-202001-2.fastq:
wc -l IL-202001-2.fastq ## 94183808

head IL-202001-2.fastq ## first read 1:1102:18835:1811_1
tail IL-202001-2.fastq ## last read 1:2119:13532:25096_2

readAddress="1:1102:18835:1811_."
grep -n $readAddress IL-202001-2.fastq &
1:@M00878:365:000000000-CVP5H:1:1102:18835:1811_1
47091905:@M00878:365:000000000-CVP5H:1:1102:18835:1811_2

readAddress="1:2119:13532:25096_."
grep -n $readAddress IL-202001-2.fastq &
47091901:@M00878:374:000000000-J34NL:1:2119:13532:25096_1
94183805:@M00878:374:000000000-J34NL:1:2119:13532:25096_2

## so it looks like the forward read sets end with the read at line 47091901 (+3), 
## just like the other illumina file

## and it looks like the reverse read sets start with the read at line 47091905


## so some reassembly is in order ... break up each file in R1 and R2:

## the last line of our R1 reads in IL1 should be


mkdir /vol/danBot/datasets/sludge/illumina/splitIL1/
mkdir /vol/danBot/datasets/sludge/illumina/splitIL2/

cd /vol/danBot/datasets/sludge/illumina/splitIL1/

split ../IL-202001-1.fastq -l 47091904

mv xaa IL-202001-1_R1.fastq
mv xab IL-202001-1_R2.fastq

## sanity checks

wc -l * 

head -n 4 IL-202001-1_R1.fastq
head -n 4 IL-202001-1_R2.fastq

tail -n 4 IL-202001-1_R1.fastq
tail -n 4 IL-202001-1_R2.fastq

## looks right

#mkdir /vol/danBot/datasets/sludge/illumina/splitIL2/
cd /vol/danBot/datasets/sludge/illumina/splitIL2/

## the last line of our R1 reads in IL2 should be
split ../IL-202001-2.fastq -l 47091904 &

## rename
mv xaa IL-202001-2_R1.fastq
mv xab IL-202001-2_R2.fastq

## we don't actually know that these are R1, R2, etc...
## should be right, because they were from the same 
## run. 
## see if the readqc makes sense:
## I guess we should run quality checks on these separately

ls /vol/danBot/datasets/sludge/illumina/splitIL1/

conda activate readQC

cd /vol/danBot/datasets/sludge/sludgeQC/illumina

#file=/vol/danBot/datasets/sludge/illumina/splitIL1/IL-202001-1_R1.fastq
#file=/vol/danBot/datasets/sludge/illumina/splitIL1/IL-202001-1_R2.fastq
#file=/vol/danBot/datasets/sludge/illumina/splitIL2/IL-202001-2_R1.fastq
file=/vol/danBot/datasets/sludge/illumina/splitIL2/IL-202001-2_R2.fastq
filename=$(basename $file)
fastqc -t 5 -o . \
  $file &> fastqc_$filename.log &

#  -o "/vol/danBot/datasets/sludge/sludgeQC/illumina/"$filename \

## wrap them and get them local:
cd ..

tar -czvf sludgeIlluminaFastqc.tar.gz illumina/ 

file=/vol/danBot/datasets/sludge/sludgeQC/sludgeIlluminaFastqc.tar.gz
scp -i /home/daniel/.ssh/funmic2023 -P 30500 -r ubuntu@129.70.51.6:$file . 

tar -xvf sludgeIlluminaFastqc.tar.gz

### concatenate these ###


#mkdir /vol/danBot/datasets/sludge/illumina/recombinedSludgeIllumina 

cd /vol/danBot/datasets/sludge/illumina/recombinedSludgeIllumina 

## R1 reads:

cat /vol/danBot/datasets/sludge/illumina/splitIL1/IL-202001-1_R1.fastq \
    /vol/danBot/datasets/sludge/illumina/splitIL2/IL-202001-2_R1.fastq > recombinedSludgeIllumina_R1.fastq && \
cat /vol/danBot/datasets/sludge/illumina/splitIL1/IL-202001-1_R2.fastq \
    /vol/danBot/datasets/sludge/illumina/splitIL2/IL-202001-2_R2.fastq > recombinedSludgeIllumina_R2.fastq &

## or multitask
cat /vol/danBot/datasets/sludge/illumina/splitIL1/IL-202001-1_R1.fastq \
    /vol/danBot/datasets/sludge/illumina/splitIL2/IL-202001-2_R1.fastq > recombinedSludgeIllumina_R1.fastq &

cat /vol/danBot/datasets/sludge/illumina/splitIL1/IL-202001-1_R2.fastq \
    /vol/danBot/datasets/sludge/illumina/splitIL2/IL-202001-2_R2.fastq > recombinedSludgeIllumina_R2.fastq &

    recombinedSludgeIllumina_R1.fastq
    recombinedSludgeIllumina_R2.fastq

## put a copy lower...going to clean this up
mv recombinedSludgeIllumina_R1.fastq /vol/danBot/datasets/sludge/illumina/sludgeIllumina_R1.fastq
mv recombinedSludgeIllumina_R2.fastq /vol/danBot/datasets/sludge/illumina/sludgeIllumina_R2.fastq

## sanity check

wc -l * 

clear
head -n 4 recombinedSludgeIllumina_R1.fastq
head -n 4 recombinedSludgeIllumina_R2.fastq

clear
tail -n 4 recombinedSludgeIllumina_R1.fastq
tail -n 4 recombinedSludgeIllumina_R2.fastq


## each read should be represented once in each file:

readAddress="1:1102:18835:1811_."
grep -n $readAddress recombinedSludgeIllumina_R1.fastq
grep -n $readAddress recombinedSludgeIllumina_R2.fastq

readAddress="1:2119:13532:25096_."
grep -n $readAddress recombinedSludgeIllumina_R1.fastq
grep -n $readAddress recombinedSludgeIllumina_R2.fastq

## just to be sure, look at these with readqc

cd /vol/danBot/datasets/sludge/sludgeQC/illumina

conda deactivate
conda activate readQC

fastqc -t 2 -o . /vol/danBot/datasets/sludge/illumina/recombinedSludgeIllumina/* &> fastqc_$filename.log &

## great, looks good. Can we use these for the assembly?

### assembly of sludge illumina data ###

conda deactivate 
conda activate megahit 

cd /vol/danBot/assemblies/sludge
fast1=/vol/danBot/datasets/sludge/illumina/recombinedSludgeIllumina/recombinedSludgeIllumina_R1.fastq
fast2=/vol/danBot/datasets/sludge/illumina/recombinedSludgeIllumina/recombinedSludgeIllumina_R2.fastq
outdir=/vol/danBot/assemblies/sludge/illumina
\time -v nohup megahit -1 $fast1 \
          -2 $fast2 \
          -t 25 \
          -o $outdir &> sludgeIlluMegahit.log &


cat /vol/danBot/assemblies/sludge/sludgeIlluMegahit.log

## taking a couple hours (3+ hours)

## while we wait...

## we can run metaquast on the assembly on the sludgeNano data?

### sludge metaquast ###

## nanopore ##

mkdir -p /vol/danBot/communityProfiles/sludge/nanopore/metaquast

cd /vol/danBot/communityProfiles/sludge/nanopore/metaquast

conda deactivate 

conda activate metaquast

sludgeNanoAssembly=/vol/danBot/assemblies/sludge/nanopore/assembly.fasta
sludgeNanoporeMetaquastOut=/vol/danBot/communityProfiles/sludge/nanopore/metaquast/

nohup metaquast -t 2 \
          -o $sludgeNanoporeMetaquastOut \
          $sludgeNanoAssembly

## get it local:
tar -czvf sludgeNanoMetaquast.tar.gz metaquast/

file=/vol/danBot/communityProfiles/sludge/nanopore/sludgeNanoMetaquast.tar.gz
scp -i /home/daniel/.ssh/funmic2023 -P 30500 -r ubuntu@129.70.51.6:$file . 

tar -xvf sludgeNanoMetaquast.tar.gz

## illumina ##

cd /vol/danBot/communityProfiles/sludge/illumina/metaquast

sludgeIlluAssembly=/vol/danBot/assemblies/sludge/illumina/final.contigs.fa
sludgeIlluMetaquastOut=/vol/danBot/communityProfiles/sludge/illumina/metaquast/
metaquast -t 20 \
          -o $sludgeIlluMetaquastOut \
          $sludgeIlluAssembly &> metaquastSludgeIllu.log &

## get it local:

tar -czvf sludgeIlluMetaquast.tar.gz metaquast/
cd /home/daniel/Documents/teaching/functionalMicrobiomes/readQC/sludge/illumina
file=/vol/danBot/communityProfiles/sludge/illumina/sludgeIlluMetaquast.tar.gz
scp -i /home/daniel/.ssh/funmic2023 -P 30500 -r ubuntu@129.70.51.6:$file . 
tar -xvf sludgeIlluMetaquast.tar.gz

cd /home/daniel/Documents/teaching/functionalMicrobiomes/readQC/sludge/illumina/metaquast

firefox report.html

### sludge illumina metaphlan ###

conda deactivate 

conda activate metaphlan

cd /vol/danBot/communityProfiles/sludge/illumina/metaphlan

metaphlanMarkerDB=/vol/danBot/metaphlanDB
seq1=/vol/danBot/datasets/sludge/illumina/recombinedSludgeIllumina/recombinedSludgeIllumina_R1.fastq
seq2=/vol/danBot/datasets/sludge/illumina/recombinedSludgeIllumina/recombinedSludgeIllumina_R2.fastq
\time -v nohup metaphlan $seq1,$seq2 \
    --bowtie2db $metaphlanMarkerDB \
    --bowtie2out metaphlanBowtie2.sludgeIllu.bz2 \
    --nproc 20 \
    --input_type fastq \
    -o MetaPhlanProfiled_sludgeIllu.txt &> metaphlanSludgeIllu.log &

## try with lots of cores. if memory dump, lower. Start with 20
## started 15:37 UTC, monday feb27
## quick, worked fine

mphlanprofile=MetaPhlanProfiled_sludgeIllu.txt
grep -v "t__" $mphlanprofile | grep "s__" | sed 's/^.*s__//g' | cut -f1,3 > sludgeIlluMetaphlanAbundances.tsv

grep -E "(s__)|(^ID)" $mphlanprofile | grep -v "t__" | sed 's/^.*s__//g' > merged_abundance_table_species.txt

### sludge binning ###

## illumina ##

## indexing ##
conda deactivate 
conda activate alignmentTools 
indexWD="/vol/danBot/binning/sludge/minimapIndices/"
cont="/vol/danBot/assemblies/sludge/illumina/final.contigs.fa"
reads1="/vol/danBot/datasets/sludge/illumina/recombinedSludgeIllumina/recombinedSludgeIllumina_R1.fastq"
reads2="/vol/danBot/datasets/sludge/illumina/recombinedSludgeIllumina/recombinedSludgeIllumina_R2.fastq"
cd $indexWD

minimap2 -d sludgeIlluAssembly.mmi $cont # make index

minimap2 -t 8 -ax sr sludgeIlluAssembly.mmi --split-prefix mmsplit $reads1 $reads2 | samtools view -F 3584 -b --threads 15 > sludgeIlluAligned2Contigs.bam &
## 9:43 local, took maybe 20 min? not sure. quick-ish

## do we need this?: yes
samtools sort -l 1 \
    -@15 \
    -o sludgeIlluReads2ContigsSorted.bam \
    -O BAM \
    sludgeIlluAligned2Contigs.bam
## quick

## oops
#mv zymoNanoReads2ContigsSorted.bam sludgeIlluReads2ContigsSorted.bam

bam=sludgeIlluReads2ContigsSorted.bam
samtools index -@ 15 $bam ## do we need this? yes. with lots of cores is very quick

## sludge illu metabat ##

conda deactivate

conda activate metabat2

cd /vol/danBot/binning/sludge/illumina/metabat

runMetaBat.sh

cont="/vol/danBot/assemblies/sludge/illumina/final.contigs.fa"
bam="/vol/danBot/binning/sludge/illumina/minimapIndices/sludgeIlluReads2ContigsSorted.bam"
\time -v runMetaBat.sh $cont $bam ## started 12:38, finished two minutes later, didn't even use a gig of RAM

## holy crap, 138 bins...

## sludge illu concoct ##

conda deactivate

conda activate concoct

cd /vol/danBot/binning/sludge/illumina/concoct

cont="/vol/danBot/assemblies/sludge/illumina/final.contigs.fa"
outputDir="/vol/danBot/binning/sludge/illumina/concoct"
sortedBam="/vol/danBot/binning/sludge/illumina/minimapIndices/sludgeIlluReads2ContigsSorted.bam"

cut_up_fasta.py $cont -c 10000 -o 0 --merge_last -b concoctContigs_10K.bed > concoctContigs_10K.fa
concoct_coverage_table.py concoctContigs_10K.bed $sortedBam > coverage_table.tsv

## this takes maybe ten min?, started ~12:00
## the meat of it is here:

concoct \
  --composition_file concoctContigs_10K.fa \
  --coverage_file coverage_table.tsv \
  -t 25 \
  -b $outputDir


## get illumina reads through the binning and refining steps
## get nanopore polished?
## then bin/refine

cd $outputDir
merge_cutup_clustering.py clustering_gt1000.csv > clustering_merged.csv
mkdir fasta_bins

extract_fasta_bins.py $cont clustering_merged.csv --output_path fasta_bins/
## 227. Jeezus

## let's rename these
cd fasta_bins

for i in *; do 
mv $i "concoct_"$i
done

## this is here:
cd /vol/danBot/binning/sludge/illumina/concoct/fasta_bins


## sludge illu vamb ##

conda deactivate

conda activate vamb 

cd /vol/danBot/binning/sludge/illumina/vamb

## they run a cataloging program on their assemblies. Not sure if I need to do this but...

cd /vol/danBot/binning/sludge/illumina/vamb

reads1="/vol/danBot/datasets/sludge/illumina/recombinedSludgeIllumina/recombinedSludgeIllumina_R1.fastq"
reads2="/vol/danBot/datasets/sludge/illumina/recombinedSludgeIllumina/recombinedSludgeIllumina_R2.fastq"
cont="/vol/danBot/assemblies/sludge/illumina/final.contigs.fa"
outdir="/vol/danBot/binning/zymoMC/nanopore/vamb/vambOut"

concatenate.py sludgeIlluCatalogue.fna.gz $cont

conda deactivate

conda activate alignmentTools

minimap2 -d sludgeIlluCatalogue.mmi sludgeIlluCatalogue.fna.gz # make index, quick

## align reads to metagenome
minimap2 -t 12 -N 5 -ax sr sludgeIlluCatalogue.mmi --split-prefix mmsplit $reads1 $reads2 | samtools view -F 3584 -b --threads 13 > sludgeIlluVamb_ReadsAligned2Contigs.bam &
## ~ten minutes with above settings

conda deactivate

conda activate vamb 

## 

############# refining ###########

## try dastools on our illumina assembly

## das tools needs a contig-bin map of alignments
## one for each of the binning softwares we used

conda activate das_tool

Fasta_to_Contigs2Bin.sh

## odd, that file is not found in the conda installation
## can't find it anywhere in the anaconda environment directories, at least

## can we grab it out of the github repo?

cd /vol/danBot/refining/zymoMC

wget https://raw.githubusercontent.com/cmks/DAS_Tool/master/src/Fasta_to_Contig2Bin.sh
chmod 777 Fasta_to_Contig2Bin.sh

## our bins are here:
#concoctBins=/vol/danBot/binning/zymoMC/illumina/concoct/fasta_bins_renamed/ 
concoctBins=/vol/danBot/binning/zymoMC/illumina/concoct/fasta_bins/ 
metabatBins=/vol/danBot/binning/zymoMC/illumina/metabat2/final.contigs.fa.metabat-bins-20230222_091706/
vambBins=/vol/danBot/binning/zymoMC/illumina/vamb/vambOut/bins/

ls $concoctBins
ls $metabatBins
ls $vambBins

## concoct:
./Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $concoctBins \
    > concoct.contigs2bin.tsv

## I don't think that quite worked...
## let's keep just the first and last columns of that

paste <(cut -d " " -f 1 concoct.contigs2bin.tsv) <(cut -f 2 concoct.contigs2bin.tsv) > concoct.contigs2bin_edited.tsv

## move to the directory:
#mv concoct.contigs2bin.tsv $concoctBins

## metabat
./Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $metabatBins \
    > metabat.contigs2bin.tsv
#mv metabat.contigs2bin.tsv $metabatBins

## vamb
./Fasta_to_Contig2Bin.sh \
    -e fna \
    -i $vambBins \
    > vamb.contigs2bin.tsv
#mv vamb.contigs2bin.tsv $vambBins

## we need to cut the first three letters or so...
cut --complement -c 1-3 vamb.contigs2bin.tsv > vamb.contigs2bin_edited.tsv

cd /vol/danBot/refining/zymoMC

conda deactivate 

conda activate das_tool

## typos, probably

## adapted for conda

conda activate das_tool

DAS_Tool  -i concoct.contigs2bin_edited.tsv,metabat.contigs2bin.tsv,vamb.contigs2bin_edited.tsv \
    -l concoct,metabat,vamb \
    -c $cont \
    -t 25 \
    --write_bins \
    --write_bin_evals \
    -o DASToolRun1

## put them here:
ls /vol/danBot/refining/zymoMC/illumina/DASToolRun1_DASTool_bins

## and we're down to 5 mags. Check them out with checkM
## but first, we have to catch up the nanopore data...


### checkM ###

conda activate checkm

cd /vol/danBot/refining/zymoMC/illumina

## let's run checkm on our mags:

## first step - find the right branch of tol:
zymoIllBins=/vol/danBot/refining/zymoMC/illumina/DASToolRun1_DASTool_bins

checkm lineage_wf -t 22 -x fa $zymoIllBins checkMout

checkMout="/vol/danBot/refining/zymoMC/illumina/checkMout"

## this also generated the markers we need to do some quality analyses.
## we can look at this with checkm's qa function:

zymoIlluminaMarkers=/vol/danBot/refining/zymoMC/illumina/checkMout/lineage.ms

checkm qa $zymoIlluminaMarkers $checkMout > zymoIllCheckMout.txt

## great, get the nanopore data back on track, then try phylophlan on our MAGs.


### metaphlan ###

## let's look at our community using metaphlan, as a contrast to the metaquast
## tool, whihc is dependent on 16s recoovery, I think.

conda deactivate

conda activate metaphlan

cd /vol/danBot/communityProfiles/zymoMC/illumina/metaphlan

metaphlanMarkerDB=/vol/danBot/metaphlanDB
seqs1=/vol/danBot/datasets/zymoMC/illumina/ERR7255689_1.fastq
seqs2=/vol/danBot/datasets/zymoMC/illumina/ERR7255689_2.fastq

## run metaphlan. Save the bowtie alignment in case we want to do it again..

metaphlan $seqs1,$seqs2 \
    --bowtie2db $metaphlanMarkerDB \
    --bowtie2out metaphlanBowtie2.ZymoIllu.bz2 \
    --nproc 20 \
    --input_type fastq \
    -o MetaPhlanProfiled_zymoIllu.txt 

## is there any way to quickly? we have this old script:


mphlanprofile=MetaPhlanProfiled_zymoIllu.txt 

head $mphlanprofile

#grep "s__" $mphlanprofile | sed  's/\([0-9]*[0-9]\.[0-9]*\).*$/\1/g' | sed 's/^.*s__//g' | cut -f 1,3
## too complicated but its neat, so save it... 

## this works better. 
mphlanprofile=MetaPhlanProfiled_zymoIllu.txt 
grep -v "t__" $mphlanprofile | grep "s__" | sed 's/^.*s__//g' | cut -f1,3 > zymoIlluMetaphlanAbundances.tsv

### visualize metaphlan results with graphlan ###

## if there is time, see visualizations for this? they use GraPhlan
## here:
https://github.com/biobakery/biobakery/wiki/metaphlan2#create-a-cladogram-with-graphlan

conda deactivate

conda activate graphlan

cd /vol/danBot/communityProfiles/zymoMC/illumina/metaphlan



## they edit their abundance table in a similar way:

mphlanprofile=MetaPhlanProfiled_zymoIllu.txt 

grep -E "(s__)|(^ID)" $mphlanprofile | grep -v "t__" | sed 's/^.*s__//g' > merged_abundance_table_species.txt

head merged_abundance_table_species.txt

export2graphlan.py -i merged_abundance_table_species.txt \
  --tree merged_abundance.tree.txt \
  --annotation merged_abundance.annot.txt \
  --most_abundant 100 \
  --abundance_threshold 1 \
  --least_biomarkers 10 \
  --annotations 5,6 \
  --external_annotations 7 \
  --min_clade_size 1 

## but it's not installing. Wait to try this out, I think it is a dead end, and that I probably
## need to build my pipeline for tree visualization, don't have time for this.

### phylophlan ###

cd /vol/danBot/assignTax/zymoMC/illumina

conda activate phylophlan

MAGs=/vol/danBot/refining/zymoMC/illumina/DASToolRun1_DASTool_bins

## the newest database as implied by name is: CMG2122
## but that database sucks, I think is some project or tutorial specific thing.

phylophlan_metagenomic

\time -v phylophlan_metagenomic \
    -i $MAGs \
    -e fa \
    -d SGB.Jul20 \
    -o phylophlanOut_ZymoIllu \
    --nproc 2 \
    -n 1 \
    --verbose &> phylophlan23.log &

## and works like a charm. all five mags go right where they are supposed to.

## this takes a long time, just to download the databases..
## so, maybe add the database for the class ahead of time. 

## add processors if not running an assembly

## looks like they have a vis script:

## if the nanopore assembly doesn't give more mags, what do we do?

## well, need to start on the sludge data...we need to know that 
## they will have at least a few MAGs to work with.

## best to get the other nanopore assembly going tonight, so we can 
## run it through the pipeline quickly and make sure it is a viable 
## dataset

## so get download going when we have a core or two. 
## if the R10 data isn't enough, we'll have to try the R9 data

## or sick them on the mbarc data?

## looks like we have the right data. So set up the code for another flye assembly.
## also metabat?

## we have to understand the miseq data from the sludge - is it any good?
## or is it totally dependent on the hiseq reads?

## then we can run all three through the same pipeline?

#### run MC nanopore through the above pipeline ###

## start over with with zymoNano assembly:

## run metaquast and metaphlan on the nanopore data. 
## can metaphlan handle nanopore data?
## I think it is largely based on bowtie, which isn't 
## optimized for nanopore, but let's see

conda deactivate 

conda activate metaphlan

cd /vol/danBot/communityProfiles/zymoMC/nanopore/metaphlan

metaphlanMarkerDB=/vol/danBot/metaphlanDB
#seqs=/vol/danBot/datasets/zymoMC/nanopore/ERR7287988_shortened.fastq ## too big, memory dump
seqs=/vol/danBot/datasets/zymoMC/nanopore/ERR7287988_forZymoNanoMetaphlan.fastq ## smaller, 27 gig


## started ~13:42 UTC
## lots of cores memory dumps, even with 5 cores. try 1
\time -v nohup metaphlan $seqs \
    --bowtie2db $metaphlanMarkerDB \
    --bowtie2out metaphlanBowtie2.ZymoNano.bz2 \
    --nproc 1 \
    --input_type fastq \
    -o MetaPhlanProfiled_zymoIllu.txt  &> metaphlanZymoNano.log &

## runs really slow. But can't speed up because of memory issues
## use nohup next time

### zymoNano metaquast ###

## as with metaphlan, not sure how metaquast will behave with nanopore reads
## should be fine, this uses the assembly, not the reads. 
## but who knows. Try it out.

cd /vol/danBot/communityProfiles/zymoMC/nanopore/metaquast

conda deactivate 

conda activate metaquast

illuminaNanoAssembly=/vol/danBot/assemblies/zymoMC/flyeNanopore/assembly.fasta
illuminaZymoMetaquastOut=/vol/danBot/communityProfiles/zymoMC/nanopore/metaquast
## not much memory usage, but the downloads from ncbi take a while
metaquast -t 13 \
          -o $illuminaZymoMetaquastOut \
          $illuminaNanoAssembly &> nanoporeMetaquast.log &

## zip it up 

cd /vol/danBot/communityProfiles/zymoMC/nanopore
tar -czvf metaquastOutZymoNanopore.tar.gz metaquast/

## and get it local:
cd /home/daniel/Documents/teaching/functionalMicrobiomes/readQC/zymoQC/nanoporeQC/nano10.4/metaquast
file=/vol/danBot/communityProfiles/zymoMC/nanopore/metaquastOutZymoNanopore.tar.gz
scp -i /home/daniel/.ssh/funmic2023 -P 30500 -r ubuntu@129.70.51.6:$file . 

### zymoNano binning ###

## step 1 make the index of the read assembly

## bowtie is really not handling these reads well. 
## can we use minimap?

conda deactivate 

conda activate alignmentTools 

indexWD="/vol/danBot/binning/zymoMC/nanopore/nanoporeIndex"
cont="/vol/danBot/assemblies/zymoMC/flyeNanopore/assembly.fasta"
reads="/vol/danBot/datasets/zymoMC/nanopore/ERR7287988_shortened.fastq"

cd $indexWD

minimap2 -d zymoNanoAssembly.mmi $cont # make index
minimap2 -t 8 -ax map-ont zymoNanoAssembly.mmi --split-prefix mmsplit $reads | samtools view -F 3584 -b --threads 18 > nanoReadsAligned2Contigs.bam ## 11:09 to 11:58, ~50 mins!

## do we need this?: yes
samtools sort -l 1 \
    -@10 \
    -o zymoNanoReads2ContigsSorted.bam \
    -O BAM \
    nanoReadsAligned2Contigs.bam
## 12:03 to 12:25??ish


samtools index $bam ## do we need this? yes. takes time, maybe put this above
## 13:55ish

## not even sure we can use the for the metabat and concoct pipelines

### metabat zymoNano ###

conda deactivate

conda activate metabat2

runMetaBat.sh

cd /vol/danBot/binning/zymoMC/nanopore/metabat2/
cont="/vol/danBot/assemblies/zymoMC/flyeNanopore/assembly.fasta"
## but this probably is what will really work:
bam="/vol/danBot/binning/zymoMC/nanopore/nanoporeIndex/zymoNanoReads2ContigsSorted.bam"

\time -v runMetaBat.sh $cont $bam ## started 12:38, finished two minutes later, didn't even use a gig of RAM

### concoct zymoNano ###

conda deactivate

conda activate concoct

cd /vol/danBot/binning/zymoMC/nanopore/concoct

cont="/vol/danBot/assemblies/zymoMC/flyeNanopore/assembly.fasta"
outputDir="/vol/danBot/binning/zymoMC/nanopore/concoct"
bam="/vol/danBot/binning/zymoMC/nanopore/nanoporeIndex/zymoNanoReads2ContigsSorted.bam"

#samtools index $bam ## do we need this? yes. takes time, maybe put this above

cd /vol/danBot/binning/zymoMC/nanopore/concoct

cut_up_fasta.py $cont -c 10000 -o 0 --merge_last -b concoctContigs_10K.bed > concoctContigs_10K.fa
concoct_coverage_table.py concoctContigs_10K.bed $bam > coverage_table.tsv
## 14:16 UTC, Finished 14:55, 40 mins!!

## the meat of it is here:
concoct \
  --composition_file concoctContigs_10K.fa \
  --coverage_file coverage_table.tsv \
  -t 25 \
  -b $outputDir

## get illumina reads through the binning and refining steps
## get nanopore polished?
## then bin/refine

cd $outputDir
merge_cutup_clustering.py clustering_gt1000.csv > clustering_merged.csv
mkdir fasta_bins
extract_fasta_bins.py $cont clustering_merged.csv --output_path fasta_bins/

## 15 bins. weird.

### vamb zymoNano ###

conda deactivate

conda activate vamb 

cd /vol/danBot/binning/zymoMC/nanopore/vamb

## they run a cataloging program on their assemblies. Not sure if I need to do this but...

cd /vol/danBot/binning/zymoMC/nanopore/vamb
reads="/vol/danBot/datasets/zymoMC/nanopore/ERR7287988_shortened.fastq"
cont="/vol/danBot/assemblies/zymoMC/flyeNanopore/assembly.fasta"
outdir="/vol/danBot/binning/zymoMC/nanopore/vamb/vambOut"
rm -r vambOut

concatenate.py zymoNanoCatalogue.fna.gz $cont

conda deactivate
conda activate alignmentTools

minimap2 -d zymoNanoCatalogue.mmi zymoNanoCatalogue.fna.gz # make index, quick

## align reads to metagenome
minimap2 -t 8 -N 5 -ax map-ont zymoNanoCatalogue.mmi --split-prefix mmsplit $reads | samtools view -F 3584 -b --threads 10 > zymoNanoVamb_ReadsAligned2Contigs.bam &
## takes forever. 47 mins

## run the binner:
conda deactivate 

conda activate vamb

## had to really drop the batch size down to far below what they recommend:
vamb --outdir vambOut --fasta zymoNanoCatalogue.fna.gz --bamfiles zymoNanoVamb_ReadsAligned2Contigs.bam -t 8 -o C --minfasta 200000 &

ls /vol/danBot/binning/zymoMC/nanopore/vamb/vambOut/bins | wc -l  ## 

### zymoNano das_tool ###

conda deactivate
conda activate das_tool

## okay, first the pain in the ass of generating abundance tables

## the script to start us off is here:
ls /vol/danBot/refining/Fasta_to_Contig2Bin.sh

cd /vol/danBot/refining/zymoMC/nanopore

## our bins are here:
concoctBins=/vol/danBot/binning/zymoMC/nanopore/concoct/fasta_bins
metabatBins=/vol/danBot/binning/zymoMC/nanopore/metabat2/assembly.fasta.metabat-bins-20230224_114049
vambBins=/vol/danBot/binning/zymoMC/nanopore/vamb/vambOut/bins

ls $concoctBins
ls $metabatBins
ls $vambBins

## concoct:
/vol/danBot/refining/Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $concoctBins \
    > concoct.contigs2bin.tsv

## that seemed to work more smoothly than with the concoct table from the illumina

## metabat
/vol/danBot/refining/Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $metabatBins \
    > metabat.contigs2bin.tsv
## why is that so short? I guess the longer reads...

## vamb
/vol/danBot/refining/Fasta_to_Contig2Bin.sh \
    -e fna \
    -i $vambBins \
    > vamb.contigs2bin.tsv

## we need to cut the first three letters or so...
cut --complement -c 1-3 vamb.contigs2bin.tsv > vamb.contigs2bin_edited.tsv


DAS_Tool  -i concoct.contigs2bin.tsv,metabat.contigs2bin.tsv,vamb.contigs2bin_edited.tsv \
    -l concoct,metabat,vamb \
    -c $cont \
    -t 25 \
    --write_bins \
    --write_bin_evals \
    -o DASTool_zymoNano

## 8 bins, that's encouraging. Run checkM on them and see how they look on monday
## 16:09

## put them here:
ls /vol/danBot/refining/zymoMC/illumina/DASToolRun1_DASTool_bins

## when we get a chance, rerun the metaphlan

## meh, metaphlan just ain't working with nanopore reads
## shelve it for now, look for an easy alternative when there is 
## time.

##### sludge data through pipeline #####

### look at sludge illumina files ###

## let's get the sludge data through the above pipeline
## start by catching up the illumina data from this

## do we understand this data?
## 

wc -l IL-202001-1.fastq && wc -l IL-202001-2.fastq  ## both have exactly 94183808 lines, weird

head -n 1 IL-202001-1.fastq
head -n 1 IL-202001-2.fastq

head -n 4 IL-202001-1.fastq
head -n 4 IL-202001-2.fastq

## it looks like these were done in the same run? but different flow cell?

## are the reads and qualities the same?

clear
echo "#############################################################"
tail -n 4 IL-202001-1.fastq
echo ""
echo "#############################################################"
echo ""
tail -n 4 IL-202001-2.fastq
echo "#############################################################"

## nope, these are different reads. Not sure what they did here...

cd /vol/danBot/datasets/sludge/illumina


head IL-202001-1.fastq

##  first read in IL-202001-1.fastq 
readAddress=":1:1102:17610:1802_."
grep -n $readAddress IL-202001-1.fastq &
1:@M00878:365:000000000-CVP5H:1:1102:17610:1802_1
47091905:@M00878:365:000000000-CVP5H:1:1102:17610:1802_2
## both are present in the first file

##  last read in IL-202001-1.fastq 
readAddress="1:2119:11004:25088_."
47091901:@M00878:374:000000000-J34NL:1:2119:11004:25088_1
94183805:@M00878:374:000000000-J34NL:1:2119:11004:25088_2
grep -n $readAddress IL-202001-1.fastq 
## again, both are present in the first file
## not interleaved, but stacked.

## for IL-202001-1.fastq, looks like the forward reads 
## end at 47091901 (+3) and the reverse reads begin at 47091905

## and for IL-202001-2.fastq:
wc -l IL-202001-2.fastq ## 94183808

head IL-202001-2.fastq ## first read 1:1102:18835:1811_1
tail IL-202001-2.fastq ## last read 1:2119:13532:25096_2

readAddress="1:1102:18835:1811_."
grep -n $readAddress IL-202001-2.fastq &
1:@M00878:365:000000000-CVP5H:1:1102:18835:1811_1
47091905:@M00878:365:000000000-CVP5H:1:1102:18835:1811_2

readAddress="1:2119:13532:25096_."
grep -n $readAddress IL-202001-2.fastq &
47091901:@M00878:374:000000000-J34NL:1:2119:13532:25096_1
94183805:@M00878:374:000000000-J34NL:1:2119:13532:25096_2

## so it looks like the forward read sets end with the read at line 47091901 (+3), 
## just like the other illumina file

## and it looks like the reverse read sets start with the read at line 47091905



## so some reassembly is in order ... break up each file in R1 and R2:

## the last line of our R1 reads in IL1 should be
cd /vol/danBot/datasets/sludge/illumina/splitIL1/
split ../IL-202001-1.fastq -l 47091904

mv xaa IL-202001-1_R1.fastq
mv xab IL-202001-1_R2.fastq

## sanity checks
wc -l * 

head -n 4 IL-202001-1_R1.fastq
head -n 4 IL-202001-1_R2.fastq
tail -n 4 IL-202001-1_R1.fastq
tail -n 4 IL-202001-1_R2.fastq
## looks right

#mkdir /vol/danBot/datasets/sludge/illumina/splitIL2/
cd /vol/danBot/datasets/sludge/illumina/splitIL2/

## the last line of our R1 reads in IL2 should be
split ../IL-202001-2.fastq -l 47091904 &

## rename
mv xaa IL-202001-2_R1.fastq
mv xab IL-202001-2_R2.fastq

## we don't actually know that these are R1, R2, etc...
## should be right, because they were from the same 
## run. 
## see if the readqc makes sense:
## I guess we should run quality checks on these separately

ls /vol/danBot/datasets/sludge/illumina/splitIL1/

conda activate readQC

cd /vol/danBot/datasets/sludge/sludgeQC/illumina

#file=/vol/danBot/datasets/sludge/illumina/splitIL1/IL-202001-1_R1.fastq
#file=/vol/danBot/datasets/sludge/illumina/splitIL1/IL-202001-1_R2.fastq
#file=/vol/danBot/datasets/sludge/illumina/splitIL2/IL-202001-2_R1.fastq
file=/vol/danBot/datasets/sludge/illumina/splitIL2/IL-202001-2_R2.fastq
filename=$(basename $file)
fastqc -t 5 -o . \
  $file &> fastqc_$filename.log &

#  -o "/vol/danBot/datasets/sludge/sludgeQC/illumina/"$filename \

## wrap them and get them local:
cd ..

tar -czvf sludgeIlluminaFastqc.tar.gz illumina/ 

file=/vol/danBot/datasets/sludge/sludgeQC/sludgeIlluminaFastqc.tar.gz
scp -i /home/daniel/.ssh/funmic2023 -P 30500 -r ubuntu@129.70.51.6:$file . 

tar -xvf sludgeIlluminaFastqc.tar.gz

### concatenate these ###


#mkdir /vol/danBot/datasets/sludge/illumina/recombinedSludgeIllumina 

cd /vol/danBot/datasets/sludge/illumina/recombinedSludgeIllumina 

## R1 reads:

cat /vol/danBot/datasets/sludge/illumina/splitIL1/IL-202001-1_R1.fastq \
    /vol/danBot/datasets/sludge/illumina/splitIL2/IL-202001-2_R1.fastq > recombinedSludgeIllumina_R1.fastq && \
cat /vol/danBot/datasets/sludge/illumina/splitIL1/IL-202001-1_R2.fastq \
    /vol/danBot/datasets/sludge/illumina/splitIL2/IL-202001-2_R2.fastq > recombinedSludgeIllumina_R2.fastq &

## sanity check

wc -l * 

clear
head -n 4 recombinedSludgeIllumina_R1.fastq
head -n 4 recombinedSludgeIllumina_R2.fastq

clear
tail -n 4 recombinedSludgeIllumina_R1.fastq
tail -n 4 recombinedSludgeIllumina_R2.fastq


## each read should be represented once in each file:

readAddress="1:1102:18835:1811_."
grep -n $readAddress recombinedSludgeIllumina_R1.fastq
grep -n $readAddress recombinedSludgeIllumina_R2.fastq

readAddress="1:2119:13532:25096_."
grep -n $readAddress recombinedSludgeIllumina_R1.fastq
grep -n $readAddress recombinedSludgeIllumina_R2.fastq

## just to be sure, look at these with readqc

cd /vol/danBot/datasets/sludge/sludgeQC/illumina

conda deactivate
conda activate readQC

fastqc -t 2 -o . /vol/danBot/datasets/sludge/illumina/recombinedSludgeIllumina/* &> fastqc_$filename.log &

## great, looks good. Can we use these for the assembly?

### assembly of sludge illumina data ###

conda deactivate 
conda activate megahit 

cd /vol/danBot/assemblies/sludge
fast1=/vol/danBot/datasets/sludge/illumina/recombinedSludgeIllumina/recombinedSludgeIllumina_R1.fastq
fast2=/vol/danBot/datasets/sludge/illumina/recombinedSludgeIllumina/recombinedSludgeIllumina_R2.fastq
outdir=/vol/danBot/assemblies/sludge/illumina
\time -v nohup megahit -1 $fast1 \
          -2 $fast2 \
          -t 25 \
          -o $outdir &> sludgeIlluMegahit.log &


cat /vol/danBot/assemblies/sludge/sludgeIlluMegahit.log

## taking a couple hours (3+ hours)

## while we wait...

## we can run metaquast on the assembly on the sludgeNano data?

### sludge metaquast ###

## nanopore ##

mkdir -p /vol/danBot/communityProfiles/sludge/nanopore/metaquast

cd /vol/danBot/communityProfiles/sludge/nanopore/metaquast

conda deactivate 

conda activate metaquast

sludgeNanoAssembly=/vol/danBot/assemblies/sludge/nanopore/assembly.fasta
sludgeNanoporeMetaquastOut=/vol/danBot/communityProfiles/sludge/nanopore/metaquast/

nohup metaquast -t 2 \
          -o $sludgeNanoporeMetaquastOut \
          $sludgeNanoAssembly

## get it local:
tar -czvf sludgeNanoMetaquast.tar.gz metaquast/

file=/vol/danBot/communityProfiles/sludge/nanopore/sludgeNanoMetaquast.tar.gz
scp -i /home/daniel/.ssh/funmic2023 -P 30500 -r ubuntu@129.70.51.6:$file . 

tar -xvf sludgeNanoMetaquast.tar.gz

## illumina ##

cd /vol/danBot/communityProfiles/sludge/illumina/metaquast

sludgeIlluAssembly=/vol/danBot/assemblies/sludge/illumina/final.contigs.fa
sludgeIlluMetaquastOut=/vol/danBot/communityProfiles/sludge/illumina/metaquast/
metaquast -t 20 \
          -o $sludgeIlluMetaquastOut \
          $sludgeIlluAssembly &> metaquastSludgeIllu.log &

## get it local:

tar -czvf sludgeIlluMetaquast.tar.gz metaquast/
cd /home/daniel/Documents/teaching/functionalMicrobiomes/readQC/sludge/illumina
file=/vol/danBot/communityProfiles/sludge/illumina/sludgeIlluMetaquast.tar.gz
scp -i /home/daniel/.ssh/funmic2023 -P 30500 -r ubuntu@129.70.51.6:$file . 
tar -xvf sludgeIlluMetaquast.tar.gz

cd /home/daniel/Documents/teaching/functionalMicrobiomes/readQC/sludge/illumina/metaquast

firefox report.html

### sludge illumina metaphlan ###

conda deactivate 

conda activate metaphlan

cd /vol/danBot/communityProfiles/sludge/illumina/metaphlan

metaphlanMarkerDB=/vol/danBot/metaphlanDB
seq1=/vol/danBot/datasets/sludge/illumina/recombinedSludgeIllumina/recombinedSludgeIllumina_R1.fastq
seq2=/vol/danBot/datasets/sludge/illumina/recombinedSludgeIllumina/recombinedSludgeIllumina_R2.fastq
\time -v nohup metaphlan $seq1,$seq2 \
    --bowtie2db $metaphlanMarkerDB \
    --bowtie2out metaphlanBowtie2.sludgeIllu.bz2 \
    --nproc 20 \
    --input_type fastq \
    -o MetaPhlanProfiled_sludgeIllu.txt &> metaphlanSludgeIllu.log &

## visualize ##

conda deactivate

conda activate graphlan

cd /vol/danBot/communityProfiles/sludge/illumina/metaphlan

mphlanprofile=MetaPhlanProfiled_sludgeIllu.txt
grep -v "t__" $mphlanprofile | grep "s__" | sed 's/^.*s__//g' | cut -f1,3 > sludgeIlluMetaphlanAbundances.tsv

## so try full suggested settings:
export2graphlan.py -i $mphlanprofile \
  --skip_rows 1,2 \
  --tree merged_abundance.tree.txt \
  --annotation merged_abundance.annot.txt \
  --most_abundant 100 \
  --abundance_threshold 1 \
  --least_biomarkers 10 \
  --annotations 3,4,5,6 \
  --external_annotations 7


graphlan_annotate.py --annot merged_abundance.annot.txt merged_abundance.tree.txt merged_abundance.xml
graphlan.py --dpi 300 merged_abundance.xml merged_abundance.png --external_legends

## get it local

mkdir sludgeIlluGraphlanOut

mv *.png sludgeIlluGraphlanOut/

cp sludgeIlluMetaphlanAbundances.tsv sludgeIlluGraphlanOut/  ## put a copy of our relative abundances in there


tar -czvf sludgeIlluGraphlanOut.tar.gz sludgeIlluGraphlanOut/

file=/vol/danBot/communityProfiles/sludge/illumina/metaphlan/sludgeIlluGraphlanOut.tar.gz
scp -i /home/daniel/.ssh/funmic2023 -P 30500 -r ubuntu@129.70.51.6:$file .

tar -xzf sludgeIlluGraphlanOut.tar.gz 

### sludge binning ###

## illumina ##

## indexing ##
conda deactivate 
conda activate alignmentTools 
indexWD="/vol/danBot/binning/sludge/minimapIndices/"
cont="/vol/danBot/assemblies/sludge/illumina/final.contigs.fa"
reads1="/vol/danBot/datasets/sludge/illumina/recombinedSludgeIllumina/recombinedSludgeIllumina_R1.fastq"
reads2="/vol/danBot/datasets/sludge/illumina/recombinedSludgeIllumina/recombinedSludgeIllumina_R2.fastq"
cd $indexWD

minimap2 -d sludgeIlluAssembly.mmi $cont # make index

minimap2 -t 8 -ax sr sludgeIlluAssembly.mmi --split-prefix mmsplit $reads1 $reads2 | samtools view -F 3584 -b --threads 15 > sludgeIlluAligned2Contigs.bam &
## 9:43 local, took maybe 20 min? not sure. quick-ish

## do we need this?: yes
samtools sort -l 1 \
    -@15 \
    -o sludgeIlluReads2ContigsSorted.bam \
    -O BAM \
    sludgeIlluAligned2Contigs.bam
## quick

## oops
#mv zymoNanoReads2ContigsSorted.bam sludgeIlluReads2ContigsSorted.bam

bam=sludgeIlluReads2ContigsSorted.bam
samtools index -@ 15 $bam ## do we need this? yes. with lots of cores is very quick

## sludge illu metabat ##

conda deactivate

conda activate metabat2

cd /vol/danBot/binning/sludge/illumina/metabat

runMetaBat.sh

cont="/vol/danBot/assemblies/sludge/illumina/final.contigs.fa"
bam="/vol/danBot/binning/sludge/illumina/minimapIndices/sludgeIlluReads2ContigsSorted.bam"

\time -v runMetaBat.sh $cont $bam 
## 104 bins


## sludge illu concoct ##

## oh god, mixed up my scripts, this is done above
## around 2400 or so.


### sludge illu refining ###

## das tool time

## bins are here:


conda activate das_tool

cd /vol/danBot/refining/sludge/illumina

/vol/danBot/refining/Fasta_to_Contig2Bin.sh

concoctBins=/vol/danBot/binning/sludge/illumina/concoct/fasta_bins/
metabatBins=/vol/danBot/binning/sludge/illumina/metabat/final.contigs.fa.metabat-bins-20230228_095818/
vambBins=/vol/danBot/binning/sludge/illumina/vamb/vambOut/bins/

ls $concoctBins
ls $metabatBins
ls $vambBins

## concoct:
/vol/danBot/refining/Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $concoctBins \
    > concoct.contigs2bin.tsv

paste <(cut -d " " -f 1 concoct.contigs2bin.tsv) <(cut -f 2 concoct.contigs2bin.tsv) > concoct.contigs2bin_edited.tsv

## metabat
/vol/danBot/refining/Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $metabatBins \
    > metabat.contigs2bin.tsv


## vamb
/vol/danBot/refining/Fasta_to_Contig2Bin.sh \
    -e fna \
    -i $vambBins \
    > vamb.contigs2bin.tsv

## get rid of a few 
cut --complement -c 1-3 vamb.contigs2bin.tsv > vamb.contigs2bin_edited.tsv

conda activate das_tool

cont="/vol/danBot/assemblies/sludge/illumina/final.contigs.fa"
DAS_Tool  -i concoct.contigs2bin_edited.tsv,metabat.contigs2bin.tsv,vamb.contigs2bin_edited.tsv \
    -l concoct,metabat,vamb \
    -c $cont \
    -t 25 \
    --write_bins \
    --write_bin_evals \
    -o DASToolRun1 &

## these are stored here:

cd /vol/danBot/refining/sludge/illumina/DASToolRun1_DASTool_bins

### sludge illu checkM ###


conda deactivate

conda activate checkm

cd /vol/danBot/refining/sludge/illumina/DASToolRun1_DASTool_bins

## let's run checkm on our mags:

## first step - find the right branch of tol:
sludgeNanoBins=/vol/danBot/refining/sludge/illumina/DASToolRun1_DASTool_bins

checkm lineage_wf -t 2 -x fa $sludgeNanoBins checkMout &

checkMout="/vol/danBot/refining/zymoMC/illumina/checkMout"

## this also generated the markers we need to do some quality analyses.
## we can look at this with checkm's qa function:

## not sure why I called this nano, should be illu

zymoIlluminaMarkers=/vol/danBot/refining/zymoMC/illumina/checkMout/lineage.ms

checkm qa $zymoIlluminaMarkers $checkMout > zymoIllCheckMout.txt

## great, get the nanopore data back on track, then try phylophlan on our MAGs.

### sludge illu phylophlan ###



##### run sludge nanopore through pipeline #####

## start over. 
## can't run metaphlan on nanopore data
## already ran metaquast on it, it was a disaster
## so, time for binning. 

### bin sludge nano ###

## make index ##

conda deactivate 

conda activate alignmentTools 

indexWD="/vol/danBot/binning/sludge/nanopore/minimapIndices"
cont="/vol/danBot/assemblies/sludge/nanopore/assembly.fasta"
reads="/vol/danBot/datasets/sludge/nanopore/ERR7014844.fastq"
cd $indexWD

minimap2 -d sludgeNanoAssembly.mmi $cont # make index
minimap2 -t 12 -ax map-ont sludgeNanoAssembly.mmi --split-prefix mmsplit $reads | samtools view -F 3584 -b --threads 15 > sludgeNanoAligned2Contigs.bam 
## 14:55 local, took maybe 20 min?

cd $indexWD

## do we need this?: yes
samtools sort -l 1 \
    -@15 \
    -o sludgeNanoReads2ContigsSorted.bam \
    -O BAM \
    sludgeNanoAligned2Contigs.bam
## quick

bam=sludgeNanoReads2ContigsSorted.bam
samtools index -@ 15 $bam ## do we need this? yes. with lots of cores is very quick

## sludge nanopore concoct ##

conda activate concoct

cont="/vol/danBot/assemblies/sludge/nanopore/assembly.fasta"
outputDir="/vol/danBot/binning/sludge/nanopore/concoct/"
bam="/vol/danBot/binning/sludge/nanopore/minimapIndices/sludgeNanoReads2ContigsSorted.bam"

cut_up_fasta.py $cont -c 10000 -o 0 --merge_last -b concoctContigs_10K.bed > concoctContigs_10K.fa
concoct_coverage_table.py concoctContigs_10K.bed $bam > coverage_table.tsv
concoct \
  --composition_file concoctContigs_10K.fa \
  --coverage_file coverage_table.tsv \
  -t 25 \
  -b $outputDir

cd $outputDir
merge_cutup_clustering.py clustering_gt1000.csv > clustering_merged.csv
mkdir fasta_bins
extract_fasta_bins.py $cont clustering_merged.csv --output_path fasta_bins/

## clean up the names...
cd /vol/danBot/binning/sludge/nanopore/concoct/fasta_bins

for i in *; do
mv $i "concoct_"$i
done

## 179 bins

## sludge nanopore metabat  ##

cd /vol/danBot/binning/sludge/nanopore/metabat

conda deactivate

conda activate metabat2

cd /vol/danBot/binning/sludge/nanopore/metabat

cont="/vol/danBot/assemblies/sludge/nanopore/assembly.fasta"
bam="/vol/danBot/binning/sludge/nanopore/minimapIndices/sludgeNanoReads2ContigsSorted.bam"
runMetaBat.sh $cont $bam 

### vamb sludge nanopore ###


conda deactivate

conda activate vamb 

cd /vol/danBot/binning/sludge/nanopore/vamb


## they run a cataloging program on their assemblies. Not sure if I need to do this but...

cd /vol/danBot/binning/sludge/nanopore/vamb

reads="/vol/danBot/datasets/sludge/nanopore/ERR7014844.fastq"
cont="/vol/danBot/assemblies/sludge/nanopore/assembly.fasta"
outdir="/vol/danBot/binning/sludge/nanopore/vamb/vambOut"

concatenate.py sludgeNanoCatalogue.fna.gz $cont

conda deactivate

conda activate alignmentTools

minimap2 -d sludgeNanoCatalogue.mmi sludgeNanoCatalogue.fna.gz # make index, quick

## align reads to metagenome
minimap2 -t 10 -N 5 -ax map-ont sludgeNanoCatalogue.mmi --split-prefix mmsplit $reads | samtools view -F 3584 -b --threads 10 > sludgeNanoVamb_ReadsAligned2Contigs.bam &
## 

## run the binner:
conda deactivate 

conda activate vamb

## had to really drop the batch size down to far below what they recommend:
vamb --outdir vambOut --fasta sludgeNanoCatalogue.fna.gz --bamfiles sludgeNanoVamb_ReadsAligned2Contigs.bam -o C --minfasta 200000 

## not run

ls /vol/danBot/binning/sludge/nanopore/vamb/vambOut/bins 

ls /vol/danBot/binning/sludge/nanopore/vamb/vambOut/bins | wc -l  ## 166 bins

### sludge nano refining ###

conda deactivate

conda activate das_tool


cd /vol/danBot/refining/sludge/nanopore
concoctBins=/vol/danBot/binning/sludge/nanopore/concoct/fasta_bins/
metabatBins=/vol/danBot/binning/sludge/nanopore/metabat/assembly.fasta.metabat-bins-20230228_150348/
vambBins=/vol/danBot/binning/sludge/nanopore/vamb/vambOut/bins/

ls $concoctBins
ls $metabatBins
ls $vambBins


## concoct:
/vol/danBot/refining/Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $concoctBins \
    > concoct.contigs2bin.tsv

#paste <(cut -d " " -f 1 concoct.contigs2bin.tsv) <(cut -f 2 concoct.contigs2bin.tsv) > concoct.contigs2bin_edited.tsv
## don't need it, I guess. Weird.

## metabat
/vol/danBot/refining/Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $metabatBins \
    > metabat.contigs2bin.tsv


## vamb
/vol/danBot/refining/Fasta_to_Contig2Bin.sh \
    -e fna \
    -i $vambBins \
    > vamb.contigs2bin.tsv

less vamb.contigs2bin.tsv 

## get rid of a few characters
cut --complement -c 1-3 vamb.contigs2bin.tsv > vamb.contigs2bin_edited.tsv
less vamb.contigs2bin_edited.tsv

cont="/vol/danBot/assemblies/sludge/nanopore/assembly.fasta"

DAS_Tool  -i concoct.contigs2bin.tsv,metabat.contigs2bin.tsv,vamb.contigs2bin_edited.tsv \
    -l concoct,metabat,vamb \
    -c $cont \
    -t 25 \
    --write_bins \
    --write_bin_evals \
    -o sludgeNano_refinedBins

### checkm sludge nano ###

conda deactivate

conda activate checkm

cd /vol/danBot/refining/sludge/nanopore/sludgeNano_refinedBins_DASTool_bins

## let's run checkm on our mags:

## first step - find the right branch of tol:
sludgeNanoBins=/vol/danBot/refining/sludge/nanopore/sludgeNano_refinedBins_DASTool_bins
checkm lineage_wf -t 26 -x fa $sludgeNanoBins checkMout &

checkMout="/vol/danBot/refining/sludge/nanopore/sludgeNano_refinedBins_DASTool_bins/checkMout"

## this also generated the markers we need to do some quality analyses.
## we can look at this with checkm's qa function:

zymoIlluminaMarkers=/vol/danBot/refining/zymoMC/illumina/checkMout/lineage.ms

/vol/danBot/refining/sludge/nanopore/sludgeNano_refinedBins_DASTool_bins

checkm qa $zymoIlluminaMarkers $checkMout > zymoIllCheckMout.txt


########### barcoding ############

## let's get the metabarcoding together
## this will be a mishmash of the amplicons
## created two years ago (and analyzed last year)
## and milan's new data

## get last year's sip data onto denbi:

## our local file:
sipData=/home/daniel/Documents/teaching/functionalMicrobiomes/barcode2023/16S_amps_SIP_2022.zip
## put it here:
barcodeDir=/vol/danBot/datasets/metabarcoding/
scp -i /home/daniel/.ssh/funmic2023 -P 30500 -r $sipData ubuntu@129.70.51.6:$barcodeDir

## also get the new milan data from alfons:
file=/home/daniel/Documents/teaching/functionalMicrobiomes/barcode2023/Oemik-Exp014.zip
barcodeDir=/vol/danBot/datasets/metabarcoding/
scp -i /home/daniel/.ssh/funmic2023 -P 30500 -r $file ubuntu@129.70.51.6:$barcodeDir

## and his metadata
file=/home/daniel/Documents/teaching/functionalMicrobiomes/barcode2023/mapping.tsv
barcodeDir=/vol/danBot/datasets/metabarcoding/
scp -i /home/daniel/.ssh/funmic2023 -P 30500 -r $file ubuntu@129.70.51.6:$barcodeDir

## 

## today: 
## try to get graphlan installed 
## get mags out of sludge
## phylophlan

########### barcoding ############

## let's get the metabarcoding together
## this will be a mishmash of the amplicons
## created two years ago (and analyzed last year)
## and milan's new data

## get last year's sip data onto denbi:

## our local file:
sipData=/home/daniel/Documents/teaching/functionalMicrobiomes/barcode2023/16S_amps_SIP_2022.zip
## put it here:
barcodeDir=/vol/danBot/datasets/metabarcoding/
scp -i /home/daniel/.ssh/funmic2023 -P 30500 -r $sipData ubuntu@129.70.51.6:$barcodeDir

## also get the new milan data from alfons:
file=/home/daniel/Documents/teaching/functionalMicrobiomes/barcode2023/Oemik-Exp014.zip
barcodeDir=/vol/danBot/datasets/metabarcoding/
scp -i /home/daniel/.ssh/funmic2023 -P 30500 -r $file ubuntu@129.70.51.6:$barcodeDir

## to integrate Milan's data, I think we need to put it into the manifest, and metadata

## first, I guess we need to do some quality checks

#### fastqc both sip SIP data ####

## we should look at the quality of both datasets, Milan's and the old data 

cd /vol/danBot/datasets/metabarcoding/milanReads
cd /vol/danBot/datasets/metabarcoding/old16S

## we need the following

milanReadDir="/vol/danBot/datasets/metabarcoding/milanReads/"
mv Oemik-001254* $milanReadDir
mv Oemik-001255* $milanReadDir
mv Oemik-001256* $milanReadDir
mv Oemik-001257* $milanReadDir
mv Oemik-001258* $milanReadDir
mv Oemik-001259* $milanReadDir
mv Oemik-001260* $milanReadDir
mv Oemik-001261* $milanReadDir
mv Oemik-001262* $milanReadDir


## today: 

## clean up the computer, down to data and anaconda packages

## okay, computer volume seems ready

#### testing new volumes ####

## we need to test our volumes, see if everything runs well. 

## we have three working instances

## let's try to set up Milan's computer. If the volumes aren't up and 
## running by then, include this in an email to de.NBI.

## access milan via:

## from danBot
ssh -p 30284 -i /home/ubuntu/.ssh/danbotFuncomp ubuntu@129.70.51.6 ## put this, plus all the others, into our .bashrc on both personal and denvi

## from personal
ssh -p 30284 -i /home/daniel/.ssh/ubuntu_e ubuntu@129.70.51.6

## clean out the private key from danBot (oops). 
#rm .ssh/danbotFuncomp
#cat .ssh/danbotFuncomp.pub >> .ssh/authorized_keys

## tried changing permissions on danBot
#chmod 600 danbotFuncom*
#chmod 700 ~/.ssh
## oh well. Work it out later.
## do need this, though, for large file transfers to catch up students

## put his key on there:

vim ~/.ssh/authorized_keys

## mount volume
lsblk
blkid

sudo mkdir /vol/milanBot
sudo mount -U "c1feef14-014f-4cb0-9ee0-4db329308eab" /vol/milanBot

## but this causes problems with the anaconda installation. It's looking in /vol/danBot,
## if we mount in the same spot, does anaconda come back?

sudo umount /vol/milanBot


## yup. I think we are stuck with this...sucks



## try the above with another...

## schneiderBot, which belongs to Julia


## from personal
ssh -p 30381  -i /home/daniel/.ssh/ubuntu_e ubuntu@129.70.51.6 

## from danBot
ssh -p 30381 -i /home/ubuntu/.ssh/danbotFuncomp ubuntu@129.70.51.6 

## mount drive
sudo mount -U "c1feef14-014f-4cb0-9ee0-4db329308eab" /vol/danBot


## restart, see if anaconda 

### now hausmannBot, which belongs to Maria

## personal
ssh -p 30355  -i /home/daniel/.ssh/ubuntu_e ubuntu@129.70.51.6

## from danBot
ssh -p 30355 -i /home/ubuntu/.ssh/danbotFuncomp ubuntu@129.70.51.6 

sudo mount -U "c1feef14-014f-4cb0-9ee0-4db329308eab" /vol/danBot

## remove: bkup bashrc, pub/private key, bkup assemblies

rm /vol/danBot/bashrc.bk

### scharrerBot, which belongs to Anja ###

## personal
ssh -p 30452  -i /home/daniel/.ssh/ubuntu_e ubuntu@129.70.51.6

## from danBot
ssh -p 30452 -i /home/ubuntu/.ssh/danbotFuncomp ubuntu@129.70.51.6 

## so ...

## on local
echo 'alias anja="ssh -p 30452  -i /home/daniel/.ssh/ubuntu_e ubuntu@129.70.51.6"' >> ~/.bashrc

## on danbot
echo 'alias anja="ssh -p 30452 -i /home/ubuntu/.ssh/danbotFuncomp ubuntu@129.70.51.6"' >> ~/.bashrc

vim ~/.ssh/authorized_keys ## put 

sudo mount -U "c1feef14-014f-4cb0-9ee0-4db329308eab" /vol/danBot

rm -f /home/ubuntu/*fa*
rm -rf /home/ubuntu/backups/
rm -f /home/ubuntu/.ssh/danbotFuncomp
rm -f /home/ubuntu/.ssh/danbotFuncomp.pub
rm /vol/danBot/bashrc.bk

### merzBot, which belongs to Susanne ###

## personal
ssh -p 30461  -i /home/daniel/.ssh/ubuntu_e ubuntu@129.70.51.6

## from danBot
ssh -p 30461 -i /home/ubuntu/.ssh/danbotFuncomp ubuntu@129.70.51.6 

## on local
echo 'alias susanne="ssh -p 30461  -i /home/daniel/.ssh/ubuntu_e ubuntu@129.70.51.6"' >> ~/.bashrc

## on danbot
echo 'alias susanne="ssh -p 30461 -i /home/ubuntu/.ssh/danbotFuncomp ubuntu@129.70.51.6"' >> ~/.bashrc

vim ~/.ssh/authorized_keys 

rm -f /home/ubuntu/*fa*
rm -rf /home/ubuntu/backups/
rm -f /home/ubuntu/.ssh/danbotFuncomp
rm -f /home/ubuntu/.ssh/danbotFuncomp.pub
rm /vol/danBot/bashrc.bk

### arnoldBot, which belongs to Rebekka  ###
ssh -p 30372  -i /home/daniel/.ssh/ubuntu_e ubuntu@129.70.51.6

## from danBot
ssh -p 30372 -i /home/ubuntu/.ssh/danbotFuncomp ubuntu@129.70.51.6 

## on local
echo 'alias rebekka="ssh -p 30372  -i /home/daniel/.ssh/ubuntu_e ubuntu@129.70.51.6"' >> ~/.bashrc

## on danbot
echo 'alias rebekka="ssh -p 30372 -i /home/ubuntu/.ssh/danbotFuncomp ubuntu@129.70.51.6"' >> ~/.bashrc

## looks like the last computer they promised me isn't really available. 
## so we can't do our installations via the image of the original computer.
## so we need to rebuild our conda environment from scratch, I think.

## clean the old anaconda off the volume they created for us
## start with a fresh Instance and volume and try to rep

## all the datasets are there, so we should try to rebuild the anaconda environments

## here is our list. 

alignmentTools
checkm        
concoct       
das_tool      
flye          
graphlan      
maxbin2       
medaka        
megahit       
metabat2      
metaphlan     
metaquast     
nanoplot      
phylophlan    
qiime2        
readQC        
vamb          
vsearch       




das_tool      

## check the wgets for a das_tool, I think there is an extra script
## check the das_tool installation generally, because I think we probably
## deleted the github repo for das_tool. 

## check das_tools, all installs ## 

## on danBot, check with sample data:

conda activate das_tool

mkdir /vol/danBot/sample_data
cd /vol/danBot/sample_data

wget https://raw.githubusercontent.com/cmks/DAS_Tool/master/sample_data/sample.human.gut_concoct_contigs2bin.tsv
wget https://github.com/cmks/DAS_Tool/raw/master/sample_data/sample.human.gut_maxbin2_contigs2bin.tsv
wget https://github.com/cmks/DAS_Tool/raw/master/sample_data/sample.human.gut_metabat_contigs2bin.tsv
wget https://github.com/cmks/DAS_Tool/raw/master/sample_data/sample.human.gut_tetraESOM_contigs2bin.tsv
wget https://github.com/cmks/DAS_Tool/raw/master/sample_data/sample.human.gut_contigs.fa

cd /vol/danBot/
DAS_Tool  -i sample_data/sample.human.gut_concoct_contigs2bin.tsv,\
sample_data/sample.human.gut_maxbin2_contigs2bin.tsv,\
sample_data/sample.human.gut_metabat_contigs2bin.tsv,\
sample_data/sample.human.gut_tetraESOM_contigs2bin.tsv \
  -l concoct,maxbin,metabat,tetraESOM \
  -c sample_data/sample.human.gut_contigs.fa \
  --write_bins \
  -t 25 \
  -o sample_output/DASToolRun1  

rm -r sample_data/
rm -r sample_output/

## works on danBot, arnoldBot, scharrerBot
## seems okay

### qiime time ###

## we need to check out qiime pipeline

### let's aggregate the ssh commands for a slide ###

Milan   "ssh -p 30284  -i /path/to/your/privateKey ubuntu@129.70.51.6"
Julia   "ssh -p 30381  -i /path/to/your/privateKey ubuntu@129.70.51.6"
Maria   "ssh -p 30355  -i /path/to/your/privateKey ubuntu@129.70.51.6"
Anja    "ssh -p 30452  -i /path/to/your/privateKey ubuntu@129.70.51.6"
Susanne "ssh -p 30461  -i /path/to/your/privateKey ubuntu@129.70.51.6"
Rebekka "ssh -p 30372  -i /path/to/your/privateKey ubuntu@129.70.51.6"


ssh -p 30500 -i /home/daniel/.ssh/funmic2023.pub ubuntu@129.70.51.6

ssh -p 30381 -i /root/.ssh/id_rsa ubuntu@129.70.51.6

## or try it with a shell variable:

key=/home/daniel/.ssh/funmic2023.pub
ssh -p 30500 -i $key ubuntu@129.70.51.6


## to get the latest version of the script:

wget https://raw.githubusercontent.com/danchurch/FunctionalMicrobiomePractical2022/main/funmic2023/funBASHterminalScript.txt


## get Julia's key on there:


### qiime time ###

## we need to check out qiime pipeline

## some emergency file transfers
file=/home/ubuntu/backups/zymoNanoAssembly.fasta
scp -i /home/ubuntu/.ssh/danbotFuncomp -P 30381 $file ubuntu@129.70.51.6:/vol/danBot/

scp -i /home/ubuntu/.ssh/danbotFuncomp -P 30284 MetaPhlanProfiled_zymoIllu.txt ubuntu@129.70.51.6:/vol/danBot/

scp -i /home/ubuntu/.ssh/danbotFuncomp -P  30284 zymoIlluAssembly.fasta ubuntu@129.70.51.6:/vol/danBot/




### we need to add the refining script somewhere they can use without problems


julia
anja
maria
rebekka
susanne

milan

cd /home/ubuntu/.local/bin
wget https://raw.githubusercontent.com/cmks/DAS_Tool/master/src/Fasta_to_Contig2Bin.sh
chmod 777 Fasta_to_Contig2Bin.sh

cd /vol
Fasta_to_Contig2Bin.sh

## okay, think that worked


## share working metaquast files:


## file is here:


#julia
file="/vol/danBot/zymoMetaquast/illumina"
scp -r -i /home/ubuntu/.ssh/danbotFuncomp -P  30381 $file ubuntu@129.70.51.6:/vol/danBot/
#anja
scp -r -i /home/ubuntu/.ssh/danbotFuncomp -P  30452  $file ubuntu@129.70.51.6:/vol/danBot/
# maria
scp -r -i /home/ubuntu/.ssh/danbotFuncomp -P  30355  $file ubuntu@129.70.51.6:/vol/danBot/
# susanne
scp -r -i /home/ubuntu/.ssh/danbotFuncomp -P 30461  $file ubuntu@129.70.51.6:/vol/danBot/

# maria nanopore metaquast
file="/vol/danBot/zymoMetaquast/nanopore"
scp -r -i /home/ubuntu/.ssh/danbotFuncomp -P 30355  $file ubuntu@129.70.51.6:/vol/danBot/



mv illumina/ zymoMetaquastIllumina/




#### run nanopore binning for the class ####

# mkdir /vol/danBot/ZymoBinning/nanopore

## we don't have time to run the full binning and refining for both illumina and nanopore

## let's repeat here:

## first step is align our raw reads back to the metagenome assembly

## make our directories
mkdir /vol/danBot/zymoBinning/nanopore
mkdir /vol/danBot/zymoBinning/nanopore/readAlignmentsForBinning

cd /vol/danBot/zymoBinning/nanopore/readAlignmentsForBinning

## conda environment
conda deactivate
conda activate alignmentTools

## variables
nanoporeAssembly="/vol/danBot/assemblies/zymoMC/nanopore/assembly.fasta"
reads="/vol/danBot/datasets/zymoMC/nanopore/ERR7287988_shortened.fastq"

## for our nanopore alignments we use minimap
minimap2 -d zymoMCnanoAssembly.mmi $nanoporeAssembly # make index

## make the alignments, takes ~1/2 hour
minimap2 -t 25 -ax map-ont zymoMCnanoAssembly.mmi --split-prefix mmsplit $reads | samtools view -F 3584 -b --threads 25 > zymoMCnanoAligned2Contigs.bam


## sort and index the alignment. nanopore takes much more time than the illumina reads
samtools sort -l 1 \
    -@25 \
    -o zymoMCnanoAligned2ContigsSorted.bam \
    -O BAM \
    zymoMCnanoAligned2Contigs.bam

## index it
samtools index -@ 25 zymoMCnanoAligned2ContigsSorted.bam 


## now to binning. Start with metabat

conda deactivate
conda activate metabat2

## make our output directory
mkdir /vol/danBot/zymoBinning/nanopore/metabat
cd /vol/danBot/zymoBinning/nanopore/metabat

## define our variables
nanoporeAssembly="/vol/danBot/assemblies/zymoMC/nanopore/assembly.fasta"
bam="/vol/danBot/zymoBinning/nanopore/readAlignmentsForBinning/zymoMCnanoAligned2ContigsSorted.bam"

## run the program
runMetaBat.sh $illuminaAssembly $bam

## concoct ##


## activate our concoct environment
conda deactivate
conda activate concoct

## make our output directory
mkdir /vol/danBot/zymoBinning/nanopore/concoct
cd /vol/danBot/zymoBinning/nanopore/concoct

## define our variables
nanoporeAssembly="/vol/danBot/assemblies/zymoMC/nanopore/assembly.fasta"
readAlignments="/vol/danBot/zymoBinning/nanopore/readAlignmentsForBinning/zymoMCnanoAligned2ContigsSorted.bam"
outdir="/vol/danBot/zymoBinning/nanopore/concoct"

cut_up_fasta.py $nanoporeAssembly -c 10000 -o 0 --merge_last -b concoctContigs_10K.bed > concoctContigs_10K.fa

concoct_coverage_table.py concoctContigs_10K.bed $readAlignments > coverage_table.tsv 



ls $readAlignments

concoct \
  --composition_file concoctContigs_10K.fa \
  --coverage_file coverage_table.tsv \
  -t 25 \
  -b $outdir

merge_cutup_clustering.py clustering_gt1000.csv > clustering_merged.csv

mkdir fasta_bins

extract_fasta_bins.py $nanoporeAssembly clustering_merged.csv --output_path fasta_bins/

## concoct gives numerical numbers for names, which upsets some software downstream
## rename, with some BASH terminal magic:

cd fasta_bins
for i in *; do
mv $i "concoct_$i"
done

## not run ##

### binning with zymoMC illumina reads with VAMB ###

## activate conda environment
conda deactivate
conda activate vamb

## make our output directory
mkdir /vol/danBot/zymoBinning/nanopore/vamb
cd /vol/danBot/zymoBinning/nanopore/vamb

## define our variables
nanoporeAssembly="/vol/danBot/assemblies/zymoMC/nanopore/assembly.fasta"
reads="/vol/danBot/datasets/zymoMC/nanopore/ERR7287988_shortened.fastq"

concatenate.py nanocatalogue.fna.gz $nanoporeAssembly

conda deactivate
conda activate alignmentTools

## make an index with minimap
minimap2 -d nanocatalogue.mmi nanocatalogue.fna.gz

## map our reads to metagenome assembly with minimap and samtools
## This takes some time, maybe 20 minutes

minimap2 -t 25 -N 5 -ax sr nanocatalogue.mmi --split-prefix mmsplit $reads | samtools view -F 3584 -b --threads 25 > nanoReadsAligned2Contigs.bam 

conda deactivate
conda activate vamb

vamb --outdir vambOut --fasta zymoNanoCatalogue.fna.gz --bamfiles zymoNanoVamb_ReadsAligned2Contigs.bam -t 8 -o C --minfasta 200000 &

### nanopore zymomc bin refining  ###

#### bin refining ####

## now we have three sets of bins
## we can select from among these
## and refine them with DAS tools:

## activate conda
conda deactivate
conda activate das_tool

## make our output directory:
mkdir -p /vol/danBot/zymoRefineBins/nanopore
cd /vol/danBot/zymoRefineBins/nanopore

## define variables
metabatBins=/vol/danBot/zymoBinning/nanopore/metabat/_________ ## this will be different for y
ou!!!
concoctBins=/vol/danBot/zymoBinning/nanopore/concoct/fasta_bins/
vambBins=/vol/danBot/zymoBinning/nanopore/vamb/vambOut/bins/
nanoporeAssembly="/vol/danBot/assemblies/zymoMC/nanopore/assembly.fasta"

## for each binner, we need a table to tell das_tools
## which contig belogs to which bin

## metabat
Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $metabatBins \
    > metabat.contigs2bin.tsv

head metabat.contigs2bin.tsv

## concoct:
Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $concoctBins \
    > concoct.contigs2bin.tsv

head concoct.contigs2bin.tsv ## that looks weird
## sometimes we have to clean this concoct table up with some BASH terminal magic
paste <(cut -d " " -f 1 concoct.contigs2bin.tsv) <(cut -f 2 concoct.contigs2bin.tsv) > concoct.contigs2bin_edited.tsv

## vamb
Fasta_to_Contig2Bin.sh \
    -e fna \
    -i $vambBins \
    > vamb.contigs2bin.tsv

head vamb.contigs2bin.tsv ## also looks weird

## we need to cut the first three letters out of or so...
cut --complement -c 1-3 vamb.contigs2bin.tsv > vamb.contigs2bin_edited.tsv

## with these we can run DAS tool to refine bins:

DAS_Tool  -i concoct.contigs2bin_edited.tsv,metabat.contigs2bin.tsv,vamb.contigs2bin_edited.tsv \
    -l concoct,metabat,vamb \
    -c $illuminaAssembly \
    -t 25 \
    --write_bins \
    -o zymoMCnanoBinsRefined


#mv zymoMVilluminaBinsRefined zymoMCilluminaBinsRefined

